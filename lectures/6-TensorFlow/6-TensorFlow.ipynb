{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** These lectures are for TensorFlow 1 (tensorflow 1.15.2 was used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Learning is an exciting branch of machine learning that uses data, lots of data, to teach computers how to do things only humans were capable of before\n",
    "- Deep Learning has emerged as a central tool to solve perception problems in recent years\n",
    "- it's the state of the art on everything having to do with computer vision and speech recognition\n",
    "- but there is more; icreasingly, people are finding that Deep Learning is a much better tool to solve problems, like discovering new medicines, understanding natural language, understanding documents, and, for example, ranking them for search, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Problems - Big and Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- many companies today have made deep learning a central part of their mission learning toolkit\n",
    "  - Facebook, Baidu, Microsoft and Google, are all using deep learning in their products and pushing the research forward\n",
    "  - it's easy to understand why, deep learning shines wherever there is lots of data and complex problems to solve\n",
    "  - and all these companies are facing lots of complicated problems, like understanding what's in an image to help you find it, or translating a document into another language that you can speak\n",
    "- this class will explore a continuum of complexity from very simple models to very large ones that you will still be able to train in minutes on your personal computer to very elaborate tasks like predicting the meaning of words or classifying images\n",
    "- one of the nice things about deep learning is it's really a family of techniques that adapts to all sorts of data and all sorts of problems - all using a common infrastructure and a common langauge to describe things\n",
    "- a lot of the important work on neural networks happen in the 80s and in the 90s, but back then computers were slow and data sets very tiny\n",
    "- the researchers didn't really find many applications in the real world\n",
    "- as a result, in the first decade of the 21st century, neural networks have completely disappeared from the world of machine learning\n",
    "  - working on neural networks was definitely fringe\n",
    "- it's only in the last few years, first in speech recognition around 2009, and then in computer vision around 2012, that neural networks\n",
    "made a big comeback\n",
    "  - what changed? lots of data, and cheap and fast GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, Tensor World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import tensorflow as tf\\n\\n# Create TensorFlow object called tensor\\nhello_constant = tf.constant(\\\"Hello World!\\\")\\n\\nwith tf.Session() as sess:\\n    # Run the tf.constant operation in the session\\n    output = sess.run(hello_constant)\\n    print(output)\";\n",
       "                var nbb_formatted_code = \"import tensorflow as tf\\n\\n# Create TensorFlow object called tensor\\nhello_constant = tf.constant(\\\"Hello World!\\\")\\n\\nwith tf.Session() as sess:\\n    # Run the tf.constant operation in the session\\n    output = sess.run(hello_constant)\\n    print(output)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant(\"Hello World!\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in TensorFlow, data isn’t stored as integers, floats, or strings\n",
    "- these values are encapsulated in an object called a **tensor**\n",
    "- in the case of `hello_constant = tf.constant('Hello World!')`, `hello_constant` is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:\n",
    "\n",
    "`A = tf.constant(1234)` // A is a 0-dimensional int32 tensor\n",
    "\n",
    "`B = tf.constant([123,456,789])` // B is a 1-dimensional int32 tensor\n",
    "\n",
    "`C = tf.constant([ [123,456,789], [222,333,444] ])` // C is a 2-dimensional int32 tensor\n",
    "\n",
    "- `tf.constant()` is one of many TensorFlow operations you will use in this lesson\n",
    "  - the tensor returned by `tf.constant()` is called a **constant tensor**, because the value of the tensor never changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process\n",
    "- https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9\n",
    "- let’s take the TensorFlow code you ran and turn that into a graph:\n",
    "\n",
    "<img src=\"resources/tf_session.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- a \"TensorFlow Session\", as shown above, is an environment for running a graph\n",
    "- the session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "- what if you want to use a non-constant?\n",
    "- this is where `tf.placeholder()` and `feed_dict` come into place\n",
    "\n",
    "\n",
    "- sadly you can’t just set `x` to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters\n",
    "- you need `tf.placeholder()`\n",
    "  - it returns a tensor that gets its value from data passed to the `tf.session.run()` function, allowing you to set the input right before the session runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session’s feed_dict\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "```\n",
    "- use the `feed_dict` parameter in `tf.session.run()` to set the placeholder tensor\n",
    "- the above example shows the tensor `x` being set to the string `\"Hello, world\"`\n",
    "- it's also possible to set more than one tensor using `feed_dict` as shown below\n",
    "\n",
    "```python\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "```\n",
    "\n",
    "**Note:** If the data passed to the `feed_dict` doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error `“ValueError: invalid literal for...”`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Math\n",
    "- https://www.tensorflow.org/api_docs/python/tf/math\n",
    "\n",
    "### Addition\n",
    "```python\n",
    "x = tf.add(5, 2)  # 7\n",
    "```\n",
    "- the `tf.add()` function does exactly what you expect it to do; it takes in two numbers, two tensors, or one of each, and returns their sum as a tensor\n",
    "\n",
    "### Subtraction\n",
    "```python\n",
    "x = tf.subtract(10, 4) # 6\n",
    "```\n",
    "\n",
    "### Multiplication\n",
    "```python\n",
    "y = tf.multiply(2, 5)  # 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting types\n",
    "- it may be necessary to convert between types to make certain operators work together\n",
    "- for example, if you tried the following, it would fail with an exception:\n",
    "```python\n",
    "tf.subtract(tf.constant(2.0),tf.constant(1))  # Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: \n",
    "```\n",
    "- that's because the constant `1` is an integer but the constant `2.0` is a floating point value and `subtract` expects them to match\n",
    "\n",
    "- in cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type\n",
    "- in this case, converting the `2.0` to an integer before subtracting, like so, will give the correct result:\n",
    "```python\n",
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this entire course, I'm going to focus on the problem of classification\n",
    "- classification is the task of taking an input, like this letter, and giving it a label that says this is a B\n",
    "- the typical setting is that you have a lot of examples, called the training sets, that have already been sorted in this is an A, this is a B, and so on\n",
    "  - now if you get a completely new example, your goal is going to be to figure out which of those classes it belongs to\n",
    "- there is a lot more to machine learning then just classification, but classification, or marginally prediction, is the central building block of machine learning\n",
    "- once you know how to classify things, it's very easy, for example, to learn how to detect them or to rank them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Your Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a logistic classifier is what’s called the linear classifier, $WX + b = y$\n",
    "  - it takes the input (X), for example, the pixels in an image, and applies a linear function to them to generate its predictions (y)\n",
    "  - a linear function is just a giant matrix multiplier\n",
    "    - it takes all the inputs as a big vector that will denote $X$ and multiplies them with a matrix to generate its predictions, one per output class\n",
    "  - throughout we'll denote the inputs by $X$, the weights by $W$ and the bias term by $b$\n",
    "  - the weights of that matrix and the bias is where the machine learning comes in\n",
    "  - we're going to train that model; that means we're going to try to find the values for the weights and bias which are good at performing those predictions\n",
    "  \n",
    "<img src=\"resources/linear_classification_scores.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- how are we going to use those scores to perform the classification?\n",
    "- each image that we have as an input can have one and only one possible label\n",
    "  - so we're going to turn those scores into probabilities\n",
    "  - we're going to want the probability of the correct class to be very close to $1$\n",
    "  - and the probability for every other class to be close to $0$\n",
    "- the way to turn scores into probabilities is to use a softmax function $S(y_i) = \\dfrac{e^{y_i}}{\\sum_{j} e^{y_j}}$ which I'll denote here by $S$\n",
    "- this is what it looks like\n",
    "\n",
    "<img src=\"resources/linear_classification_softmax_workfow.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- but beyond the formula what's important to know about it is that it can take any kind of scores and turn them into proper probabilities\n",
    "- proper probabilities sum to $1$ and they will be large when the scores are large and small, when the scores are comparatively smaller\n",
    "- scores, in the context of logistic regression, are often also called logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let’s derive the function `y = Wx + b`\n",
    "  - we want to translate our input, `x`, to labels, `y`\n",
    "- for example, imagine we want to classify images as digits\n",
    "- `x` would be our list of pixel values, and `y` would be the logits, one for each digit\n",
    "- let's take a look at `y = Wx`, where the weights, `W`, determine the influence of `x` at predicting each `y`\n",
    "\n",
    "<img src=\"resources/tensorflow_linear_function_1.jpg\" style=\"width: 50%;\"/>\n",
    "\n",
    "- `y = Wx` allows us to segment the data into their respective labels using a line\n",
    "- however, this line has to pass through the origin, because whenever `x` equals $0$, then `y` is also going to equal $0$\n",
    "- we want the ability to shift the line away from the origin to fit more complex data\n",
    "  - the simplest solution is to add a number to the function, which we call “bias”\n",
    "  \n",
    "<img src=\"resources/tensorflow_linear_function_2.jpg\" style=\"width: 50%;\"/>\n",
    "\n",
    "- our new function becomes `Wx + b`, allowing us to create predictions on linearly separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we've been using the `y = Wx + b` function for our linear function\n",
    "- but there's another function that does the same thing, `y = xW + b`\n",
    "- these functions do the same thing and are interchangeable, except for the dimensions of the matrices involved\n",
    "- to shift from one function to the other, you simply have to swap the row and column dimensions of each matrix\n",
    "  - this is called transposition\n",
    "\n",
    "- for rest of this lesson, we actually use `xW + b`, because this is what TensorFlow uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Bias in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the goal of training a neural network is to modify weights and biases to best predict the labels\n",
    "- in order to use weights and bias, you'll need a Tensor that can be modified - this leaves out `tf.placeholder()` and `tf.constant()`, since those Tensors can't be modified\n",
    "- this is where `tf.Variable` class comes in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Variable()\n",
    "\n",
    "```python\n",
    "x = tf.Variable(5)\n",
    "```\n",
    "\n",
    "- the `tf.Variable` class creates a tensor with an initial value that can be modified, much like a normal Python variable\n",
    "- this tensor stores its state in the session, so you must initialize the state of the tensor manually\n",
    "- you'll use the `tf.global_variables_initializer()` function to initialize the state of all the Variable tensors\n",
    "\n",
    "\n",
    "#### Initialization\n",
    "```python\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "```\n",
    "\n",
    "\n",
    "- the `tf.global_variables_initializer()` call returns an operation that will initialize all TensorFlow variables from the graph\n",
    "- you call the operation using a session to initialize all the variables as shown above\n",
    "- using the `tf.Variable` class allows us to change the weights and bias, but an initial value needs to be chosen\n",
    "\n",
    "\n",
    "- initializing the weights with random numbers from a normal distribution is good practice\n",
    "- randomizing the weights helps the model from becoming stuck in the same place every time you train it\n",
    "- you'll learn more about this in the next lesson, when you study gradient descent\n",
    "\n",
    "\n",
    "- similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights\n",
    "- you'll use the `tf.truncated_normal()` function to generate random numbers from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.truncated_normal()\n",
    "\n",
    "```python\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "```\n",
    "- the `tf.truncated_normal()` function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean\n",
    "- since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias\n",
    "- let's use the simplest solution, setting the bias to $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.zeros()\n",
    "\n",
    "```python\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "```\n",
    "- the `tf.zeros()` function returns a tensor with all zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow\n",
    "- since `xW in xW + b` is matrix multiplication, you have to use the `tf.matmul()` function instead of `tf.multiply()`\n",
    "  - don't forget that order matters in matrix multiplication, so `tf.matmul(a,b)` is not the same as `tf.matmul(b,a)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from test import *\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can’t train a neural network on a single sample\n",
    "- let’s apply `n` samples of `x` to the function `y = Wx + b`, which becomes `Y = WX + B`\n",
    "\n",
    "<img src=\"resources/linear_update.jpg\" style=\"width: 50%;\"/>\n",
    "\n",
    "- for every sample of `X` (`X1`, `X2`, `X3`), we get logits for label 1 (`Y1`) and label 2 (`Y2`)\n",
    "- in order to add the bias to the product of `WX`, we had to turn `b` into a matrix of the same shape\n",
    "  - this is a bit unnecessary, since the bias is only two numbers\n",
    "  - it should really be a vector\n",
    "\n",
    "\n",
    "- we can take advantage of an operation called broadcasting used in TensorFlow and Numpy\n",
    "- this operation allows arrays of different dimension to be added or multiplied with each other\n",
    "- for example:\n",
    "```python\n",
    "import numpy as np\n",
    "t = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "u = np.array([1, 2, 3])\n",
    "print(t + u)\n",
    "```\n",
    "\n",
    "- the code above will print...\n",
    "\n",
    "```python\n",
    "[[ 2  4  6]\n",
    " [ 5  7  9]\n",
    " [ 8 10 12]\n",
    " [11 13 15]]\n",
    "```\n",
    "\n",
    "- this is because `u` is the same dimension as the last dimension in `t`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the next step is to assign a probability to each label, which you can then use to classify the data\n",
    "- use the softmax function to turn your logits into probabilities\n",
    "- we can do this by using the formula above, which uses the input of $y$ values and the mathematical constant $e$ which is approximately equal to $2.718$\n",
    "- by taking $e$ to the power of any real value we always get back a positive value, this then helps us scale when having negative $y$ values\n",
    "- the summation symbol on the bottom of the divisor indicates that we add together all the $e^\\text{(input y value)}$ elements in order to get our calculated probability outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for the next quiz, you'll implement a `softmax(x)` function that takes in `x`, a one or two dimensional array of logits\n",
    "  - in the one dimensional case, the array is just a single set of logits\n",
    "  - in the two dimensional case, each column in the array is a set of logits\n",
    "- the `softmax(x)` function should return a NumPy array of the same shape as `x`\n",
    "\n",
    "\n",
    "- for example, given a one-dimensional array:\n",
    "```python\n",
    "# logits is a one-dimensional array with 3 elements\n",
    "logits = [1.0, 2.0, 3.0]\n",
    "# softmax will return a one-dimensional array with 3 elements\n",
    "print softmax(logits)\n",
    "```\n",
    "\n",
    "```python\n",
    "[ 0.09003057  0.24472847  0.66524096]\n",
    "```\n",
    "\n",
    "\n",
    "- given a two-dimensional array where each column represents a set of logits:\n",
    "```python\n",
    "# logits is a two-dimensional array\n",
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "# softmax will return a two-dimensional array with the same shape\n",
    "print softmax(logits)\n",
    "```\n",
    "\n",
    "```python\n",
    "[\n",
    "  [ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
    "  [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
    "  [ 0.66524096  0.97962921  0.86681333  0.33333333]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Implement the softmax function, which is specified by the formula above in the lectures.\\n# The probabilities for each column must sum to 1\\n\\nimport numpy as np\\n\\n\\ndef softmax(x):\\n    \\\"\\\"\\\"Compute softmax values for each sets of scores in x.\\\"\\\"\\\"\\n    # TODO: Compute and return softmax(x)\\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\\n\\n\\nlogits = [3.0, 1.0, 0.2]\\nprint(softmax(logits))\";\n",
       "                var nbb_formatted_code = \"# Implement the softmax function, which is specified by the formula above in the lectures.\\n# The probabilities for each column must sum to 1\\n\\nimport numpy as np\\n\\n\\ndef softmax(x):\\n    \\\"\\\"\\\"Compute softmax values for each sets of scores in x.\\\"\\\"\\\"\\n    # TODO: Compute and return softmax(x)\\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\\n\\n\\nlogits = [3.0, 1.0, 0.2]\\nprint(softmax(logits))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement the softmax function, which is specified by the formula above in the lectures.\n",
    "# The probabilities for each column must sum to 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: TensorFlow Softmax Workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that you've built a softmax function from scratch, let's see how softmax is done in TensorFlow\n",
    "\n",
    "```python\n",
    "x = tf.nn.softmax([2.0, 1.0, 0.2])\n",
    "```\n",
    "\n",
    "- easy as that! `tf.nn.softmax()` implements the softmax function for you\n",
    "  - it takes in logits and returns softmax activations\n",
    "\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        pass\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What happens to the softmax probabilities when you multiply the logits by 10?\n",
    "<br/>\n",
    "**A:** Probabilities get closer to 0.0 or 1.0\n",
    "\n",
    "\n",
    "**Q:** What happens to the softmax probabilities when you divide the logits by 10?\n",
    "<br/>\n",
    "**A:** Probabilities get close to the uniform distribution. Since all the scores decrease in magnitude, the resulting softmax probabilities will be closer to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we need a way to represent our labels mathematically\n",
    "- we just said, let's have the probabilities for the correct class be close to $1$ and the probability for all the others be close to $0$\n",
    "- we can write down exactly that\n",
    "- each label will be represented by a vector, that is as long as there are classes and it has the value $1.0$ for the correct class and $0$ everywhere else\n",
    "- this is often called one-hot encoding\n",
    "\n",
    "<img src=\"resources/one_hot_encoding.png\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-hot encoding works very well from most problems until you get into situations where you have tens of thousands, or even millions of classes\n",
    "- in that case, your vector becomes really, really large and has mostly zeros everywhere and that becomes very inefficient\n",
    "- you'll see later how we can deal with these problems using embeddings\n",
    "- what's nice about this approach is that we can now measure how well we're doing by simply comparing two vectors\n",
    "  - one that comes out of your classifiers and contains the probabilities of your classes and the one-hot encoded vector that corresponds to your labels\n",
    "- the natural way to measure the distance between those two probability vectors is called the **Cross Entropy**\n",
    "\n",
    "<img src=\"resources/cross_entropy.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- I'll denote it by $D$ here for distance\n",
    "- math, it looks like this: $D(S,L) = -\\sum_{i} L_i log(S_i)$\n",
    "- be careful, the cross entropy is not symmetric; $D(S,L) \\neq D(L,S)$ and you have a nasty log in there so you have to make sure that your labels and your distributions are in the right place\n",
    "- your labels, because they're one-hot encoded, will have a lot of zeroes in them and you don't want to take the log of zeroes\n",
    "- for your distribution, the softmax will always guarantee that you have a little bit of probability going everywhere, so you never really take a log of zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's recap, because we have a lot of pieces already\n",
    "- we have an input, it's going to be turned into logits using a linear model, which is basically your matrix multiply and a bias\n",
    "- we're then going to feed the logits, which are scores, into a softmax to turn them into probabilities\n",
    "- then we're going to compare those probabilities to the one-hot encoded labels using the cross entropy function\n",
    "\n",
    "<img src=\"resources/multinomial_logistic_classification.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- this entire setting is often called **multinomial logistic classification**\n",
    "  - $D(S(WX+b),L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we have all the pieces of our puzzle\n",
    "- the question is how we're going to find those weights $w$ and those biases $b$ that will get our classifier to do what we want it to do\n",
    "- that is, have a low distance for the correct class but have a high distance for the incorrect class\n",
    "- one thing you can do is measure that distance averaged over the entire training sets for all the inputs and all the labels that you have available\n",
    "  - that's called the training loss\n",
    "  - $£ = \\dfrac{1}{N} \\sum_{i} D(S(wx_i + b), L_i)$\n",
    "    - this loss, which is the **average cross-entropy** over your entire training set, is one humongous function\n",
    "    - every example in your training set gets multiplied by this one big matrix $w$, and then they get all added up in one big sum\n",
    "    - we want all the distances to be small, which would mean we're doing a good job at classifying every example in the training data\n",
    "    - so we want the loss to be small\n",
    "    - the loss is a function of the weights and the biases\n",
    "    - so we are simply going to try and minimize that function\n",
    "\n",
    "\n",
    "- imagine that the loss is a function of two weights, weight one and weight two, just for the sake of argument\n",
    "- it's going to be a function which will be large in some areas, and small in others\n",
    "- we're going to try the weights which cause this loss to be the smallest\n",
    "- we've just turned the machine learning problem into one of numerical optimization\n",
    "- there's lots of ways to solve a numerical optimization problem\n",
    "- the simplest way is one you've probably encountered before, **gradient descent**\n",
    "  - take the derivative of your loss, with respect to your parameters, and follow that derivative by taking a step backwards and repeat until you get to the bottom\n",
    "  - gradient descent is relatively simple, especially when you have powerful numerical tools that compute the derivatives for you\n",
    "  - remember, I'm showing you the derivative for a function of just two parameters here, but for a typical problem it could be a function of thousands, millions or even billions of parameters\n",
    "\n",
    "<img src=\"resources/gradient_descent_circle_graph.png\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Aspects of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the coming lectures, I'll talk about these tools that compute the derivatives for you, and a lot about what's good and bad about grading descent\n",
    "- for the moment, though, we'll assume that I give you the optimizer as a black box that you can simply use\n",
    "- there are two last practical things that stand in your way of training your first model\n",
    "  - first is how do you fill image pixels to this classifier and then where do you initialize the optimization? let's look into this\n",
    "\n",
    "\n",
    "- this is where we have to talk a bit about numerical stability\n",
    "- when you do numerical computations, you always have to worry a bit about calculating values that are too large or too small\n",
    "- in particular, adding very small values to a very large value can introduce a lot of errors\n",
    "  - try this in Python: take the value 1,000,000,000 and then add to it the value 10 to the minus 6 1,000,000 times, then subtract 1,000,000,000 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"a = 1000000000\\nfor i in range(1000000):\\n    a = a + 1e-6\\nprint(a - 1000000000)\";\n",
       "                var nbb_formatted_code = \"a = 1000000000\\nfor i in range(1000000):\\n    a = a + 1e-6\\nprint(a - 1000000000)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Inputs and Initial Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the example in the quiz, the math says the result should be $1.0$, but, the code says $0.95$; that's a big difference\n",
    "  - go ahead, replace the one billion with just one, and you'll see that the error becomes very tiny\n",
    "\n",
    "\n",
    "- we're going to want the values involved in the calculation of this big lost function that we care about, to never get too big or too small\n",
    "- one good guiding principle is that we always want our variables to have:\n",
    "  - $0$ mean; $\\mu(X_i) = 0$\n",
    "  - equal variance $\\sigma (X_i) = \\sigma (X_j)$ whenever possible\n",
    "\n",
    "\n",
    "- on top of the numerical issues, there are also a really good mathematical reasons to keep values you compute roughly around a mean of zero and equal variance when you're doing optimization\n",
    "- badly conditioned problem means that the optimizer has to do a lot of searching to go and find a good solution\n",
    "- well conditioned problem makes it a lot easier for the optimizer to do its job\n",
    "\n",
    "<img src=\"resources/well_vs_bad_conditioned_problem.png\" style=\"width: 80%;\"/>\n",
    "\n",
    "- if you're dealing with images it's simple\n",
    "- you can take the pixel values of your image, they are usually between $0$ and $255$, and simply subtract $128$ and divide by $128$\n",
    "  - it doesn't change the content of your image, but it makes it much easier for the optimization to proceed numerically\n",
    "\n",
    "\n",
    "- you also want your weights and biases to be initialized at a good enough starting point for  the gradient descent to proceed\n",
    "- there are lots of fancy schemes to find good initialization values, but we're going to focus on a simple general method\n",
    "  - draw the weights randomly from a Gaussian distribution with mean $0$ and standard deviation $\\sigma$\n",
    "  - the sigma value determines the order of magnitude of your outputs at the initial point of your optimization\n",
    "  - because of the softmax on top of it, the order of magnitude also determines the peakiness of your initial probability distribution\n",
    "    - a large sigma will mean that your distribution will have large peaks, it's going to be very opinionated\n",
    "    - a small sigma means that your distribution is very uncertain about things\n",
    "    - it's usually better to begin with an uncertain distribution, and let the optimization become more confident as the training progress\n",
    "      - so, use a small sigma to begin with\n",
    "\n",
    "\n",
    "- now we actually have everything we need to actually train this classifier\n",
    "- we've got our training data ($x_i$ and $b$), which is normalized to have zero mean and unit variance\n",
    "- we multiply it by a large matrix ($W$), which is initialized with random weights\n",
    "- we apply the softmax ($S$) then the cross-entropy loss ($D$) and we calculate the average of this loss over the entire training data ($\\dfrac{1}{N})$\n",
    "- then our magical optimization package:\n",
    "  - computes the derivative of this loss with respect to:\n",
    "    - the weights: $w \\leftarrow w - \\alpha \\Delta £$\n",
    "    - the biases: $b \\leftarrow b - \\alpha \\Delta_b £$\n",
    "  - takes a step back in the direction opposite to that derivative\n",
    "  - then, we start all over again; we repeat the process (loop) until we reach minimum of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that you have trained your first model, there is something very important I want to discuss\n",
    "- you might have seen in the assignment that we had a training set, as well as a validation set, and a test set\n",
    "- it has to do with measuring how well you're doing without accidentally shooting yourself in the foot, and it is a lot more subtle then you might initially think\n",
    "- it's also very important because as we will discover later, once you know how to measure your performance on a problem, you've already solved half of it\n",
    "\n",
    "\n",
    "- let me explain why measuring performance is subtle\n",
    "- let's go back to our classification task\n",
    "- you've got a whole lot of images with labels\n",
    "- you could say, okay, I'm going to run my classifier on those images, and see how many I got right\n",
    "  - that's my error measure\n",
    "- and then you go out and use your classifier on new images; images that you've never seen in the past, and you measure how many you get right, and your performance gets worse\n",
    "  - the classifier doesn't do as well\n",
    "\n",
    "\n",
    "- so what happened?\n",
    "- well, imagine I construct a classifier that simply compares the new image to any of the other images that I've already seen in my training set, and just returns the label\n",
    "- by the measure we defined earlier, it's a great classifier; it would get 100% accuracy on the training set but as soon as it sees a new image, it's lost; it has no idea what to do\n",
    "  - it's not a great classifier\n",
    "- the problem is that your classifier has memorized the training set, and it fails to generalize to new examples\n",
    "- it's not just a theoretical problem\n",
    "- every classifier that you will build will tend to try and memorize the training set\n",
    "- and it will usually do that very, very well\n",
    "- your job though, is to help it generalize to new data instead\n",
    "\n",
    "\n",
    "- so, how do we measure generalization instead of measuring how well the classifier memorized the data?\n",
    "- the simplest way is to take a small subset of the training set, not use it in training, and measure the error on that test data\n",
    "- problem solved, now your classifier cannot cheat because it never sees the test data, so it can't memorize it\n",
    "- but there is still a problem, because training a classifier is usually a process of trial and error\n",
    "  - you try a classifier, you measure its performance and then you try another one and you measure again\n",
    "  - and another, and another, you tweak the model, you explore the parameters, you measure, and finally, you have what you think is the perfect classifier\n",
    "  - and then after all this care you've taken to separate your test data from your training data and only measuring your performance on the test data, now you deploy your system in a real production environment\n",
    "  - and you get more data and you score your performance on that new data and it doesn't do nearly as well\n",
    "\n",
    "\n",
    "- what happened is that your classifier has seen your test data, indirectly, through your own eyes\n",
    "- every time you made a decision about which classifier to use, which parameter to tune, you actually give information to your classifier about the test set\n",
    "- just a tiny bit, but it adds up\n",
    "- so over time, as you run many, and many experiments, your test data bleeds into your training data\n",
    "\n",
    "\n",
    "- there are many ways to deal with this; I'll give you the simplest one\n",
    "- take another chunk of you training set and hide it under a rock\n",
    "- never look at it until you have made your final decision\n",
    "- you can use your validation set to measure your actual error, and maybe the validation set will bleed into the training sets\n",
    "- but that's okay because you'll always have this test set that you can rely on to actually measure your real performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition: Overfitting -> Dataset Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm not going to talk about cross-validation here, but if you've never encountered it in your curriculum, I'd strongly recommend that you learn about it\n",
    "- I am spending time on this because it's essential to deep learning in particular\n",
    "- deep learning has many knobs that you can tweak, and you will be tempted to tweak them over and over\n",
    "- you have to be very careful about overfitting on your test set\n",
    "- use the validation set\n",
    "  - how big does your validation and test sets need to be? it depends\n",
    "  - the bigger your validation set the more precise your numbers will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Test Set Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imagine that your validation set has just six examples with an accuracy of $66%$\n",
    "- now you tweak your model and your performance goes from $66%$ to $83%$\n",
    "  - is this something you can trust? no, of course\n",
    "  - this is only a change of label for a single example\n",
    "  - it could just be noise\n",
    "- the bigger your test set, the less noisy the accuracy measurement will be\n",
    "- here is a useful rule of thumb, and if you're a statistician, feel free to cover your ears right now\n",
    "  - a change that affects 30 examples in your validation sets one way or another, is usually statistically significant and typically can be trusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Validation Set Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's do some back of the envelope calculations\n",
    "- imagine you have 3000 examples in your validation set, and assume you trust my hand-wavy rule of 30\n",
    "- which level of accuracy improvement can you trust to not be in the noise?\n",
    "  - a difference from 80% to 81%? YES\n",
    "  - a difference from 80% to 80.5%? NO\n",
    "  - a difference of 80% to 80.1%? NO\n",
    "\n",
    "\n",
    "- if your accuracy changes from 80% to 80.1%, that's only at most 3 examples changing their labels\n",
    "  - it's 0.1 times 3000 divided by 100\n",
    "  - that's very few; it could just be noise and it definitely doesn't meet my rule of thumb of 30 examples minimum\n",
    "- same thing going from 80% to 80.5%\n",
    "  - at worst, only 15 examples are changing then\n",
    "- when you get an improvement of 1% going from 80% to 81%, that's now a more robust 30 examples that are going from incorrect to correct\n",
    "  - that's a stronger signal that whatever you're doing is indeed improving your accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is why for most classification tasks people tend to hold back more than 30,000 examples for validation\n",
    "- this makes accuracy figures significant to the first decimal place and gives you enough resolution to see small improvements\n",
    "- $> 30,000 \\text{ examples} \\rightarrow \\text{ changes } > 0.1\\% \\text{ in accuracy}$\n",
    "- if your classes are not well balanced; for example, if some important classes are very rare, this heuristic is no longer good\n",
    "- bad news, you're only going to need much more data\n",
    "- now, holding back even 30,000 examples can be a lot of data if you have a small training set\n",
    "- **cross-validation**, which I've mentioned before, is one possible way to mitigate the issue\n",
    "  - but cross-validation can be a slow process, so getting more data is often the right solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training logistic regression using gradient descent is great\n",
    "- for one thing, you're directly optimizing the error measure that you care about\n",
    "  - that's always a great idea\n",
    "  - that's why in practice, a lot of machine learning research is about designing the right loss function to optimize\n",
    "- but as you might experienced if you've run the model in the assignments, it's got problems\n",
    "- the biggest one is that it's very difficult to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the problem with scaling gradient descent is simple; you need to compute these gradients $- \\alpha \\Delta £(w_1,w_2)$\n",
    "- here's another rule of thumb\n",
    "  - if computing your loss $£ = \\sum_{i} D_i$ takes $n$ floating point operations, computing its gradient takes about three times that compute\n",
    "  - as we saw earlier, this loss function is huge\n",
    "    - it depends on every single element in your training set\n",
    "      - that can be a lot of compute if your data set is big, and we want to be able to train on lots of data because in practice, on real problems, you'll always get more gains, the more data you use\n",
    "    - because gradient descent is iterative, you have to do that for many steps\n",
    "    - that means going through your data tens or hundreds of times; that's not good\n",
    "    \n",
    "<img src=\"resources/stochastic_gradient_descent_1.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "\n",
    "- so instead, we're going to cheat\n",
    "- instead of computing the loss, we're going to compute an estimate of it, a very bad estimate; a terrible estimate, in fact\n",
    "  - that estimate is going to be so bad, you might wonder why it works at all\n",
    "  - you would be right because we're going to also have to spend some time making it less terrible\n",
    "- the estimate we're going to use is simply computing the average loss for a very small random fraction of the training data\n",
    "  - think between $1$ and $1000$ training samples each time\n",
    "  - I say random because it's very important\n",
    "    - if the way you pick your samples isn't random enough, it no longer works at all\n",
    "\n",
    "\n",
    "- so we're going to take a very small sliver of the training data, compute the loss for that sample, compute the derivative for that sample, and pretend that that derivative is the right direction to use to do gradient descent\n",
    "  - it is not at all the right direction; in fact, at times, it might increase the real loss, not reduce it\n",
    "  - but we're going to compensate by doing this many, many times, taking very, very small steps each time\n",
    "  - so each step is a lot cheaper to compute, but we pay a price\n",
    "  - we have to take many more smaller steps instead of one large step\n",
    "  - on balance though, we win by a lot\n",
    "  - in fact, as you'll see in the assignments, doing this is vastly more efficient than doing gradient decent\n",
    "\n",
    "<img src=\"resources/stochastic_gradient_descent_2.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- this technique is called **stochastic gradient descent** and is at the core of deep learning\n",
    "  - that's because stochastic gradient descent scales well with both data and model size, and we want both big data and big models\n",
    "  - stochastic gradient descent, *SGD* for short, is nice and scalable\n",
    "  - but because it's fundamentally a pretty bad optimizer that happens to be the only one that's fast enough, it comes with a lot of issues in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum and Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you've already seen some of these tricks\n",
    "  - I asked you to make your inputs zero mean and equal variance earlier\n",
    "    - it's very important for SGD\n",
    "  - I also told you to initialize with random weights that have relatively small variance, same thing\n",
    "- I'm going to talk about a few more of those important tricks and that should cover all you really need to worry about to implement SGD\n",
    "\n",
    "<img src=\"resources/momentum.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- the first one is momentum\n",
    "- remember that at each step, we're taking a very small step in a random direction\n",
    "- but on aggregate, those steps take us towards the minimum of the loss\n",
    "- we can take advantage of the knowledge that we've accumulated from previous steps about where we should be headed\n",
    "  - a cheap way to do that is to keep a running average of the gradients $M \\leftarrow 0.9M + \\Delta£$ and to use that running average $M(w_1, w_2)$ instead of the direction of the current batch of the data\n",
    "- this momentum technique works very well and often leads to better convergence\n",
    "\n",
    "\n",
    "- the second one is learning rate decay\n",
    "- remember, when replacing gradient descent with SGD, I said that we were going to take smaller, noisier steps towards our objective\n",
    "  - how small should that step be? that's a whole area of research, as well\n",
    "  - one the thing that's always the case, however is that it's beneficial to make that step smaller and smaller as you train\n",
    "  - some like to apply an exponential decay to their learning rate\n",
    "  - some like to make it smaller every time the loss reaches a plateau\n",
    "  - there are lots of ways to go about it, but lowering it over time is the key thing to remember\n",
    "\n",
    "<img src=\"resources/learning_rate_decay.png\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Hyperspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning rate training can be very strange\n",
    "- for example, you might think that using a higher learning rate means that you learn more or that you learn faster, that's just not true\n",
    "- in fact, you can often take a model, lower the learning rate and get to a better model faster\n",
    "\n",
    "\n",
    "- it gets even worse\n",
    "- you might be tempted to look at the curve that shows the loss over time to see how quickly you learn\n",
    "- here the higher learning rate starts faster but then it plateaus when the lower learning rate keeps on going and gets better\n",
    "\n",
    "<img src=\"resources/learning_rate_tuning.png\" style=\"width: 50%;\"/>\n",
    "\n",
    "- it is a very familiar picture for anyone who's trained neural networks\n",
    "- never trust how quickly you learn, it has often little to do with how well you train\n",
    "- this is where *SGD* gets its reputation for being *black magic*\n",
    "  - you have many, many hyperparameters that you could play with\n",
    "    - initial learning rate, learning rate decay, momentum, batch size, weight initialization\n",
    "      - you have to get them right\n",
    "    - in practice, it's not that bad but if you have to remember just one thing is that **when things don't work, always try to lower your learning rate first**\n",
    "- there are lots of good solutions for small models\n",
    "- but sadly, none that's completely satisfactory, so far, for the very large models that we really care about\n",
    "- I mentioned one approach called **ADAGRAD** that makes things a little bit easier.\n",
    "  - *ADAGRAD* is a modification of *SGD* which implicitly does momentum and learning rate decay for you\n",
    "  - using *ADAGRAD*, often makes learning less sensitive to hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mini-batching is a technique for training on subsets of the dataset instead of all the data at one time\n",
    "- this provides the ability to train a model, even if a computer lacks the memory to store the entire dataset\n",
    "- mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples\n",
    "  - however, this is a small price to pay in order to be able to run the model at all\n",
    "\n",
    "\n",
    "- it's also quite useful combined with SGD\n",
    "- he idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches\n",
    "- for each mini-batch, you train the network weights with gradient descent\n",
    "- since these batches are random, you're performing SGD with each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's look at the MNIST dataset with weights and a bias to see if your machine can handle it\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "```\n",
    "\n",
    "\n",
    "**Question 1:** Calculate the memory size of `train_features`, `train_labels`, `weights`, and `bias` in bytes. Ignore memory for overhead, just calculate the memory required for the stored data.\n",
    "\n",
    "You may have to look up how much memory a float32 requires. - It's 4 bytes\n",
    "\n",
    "`train_features` Shape: (55000, 784) Type: float32 - It needs 172480000 bytes\n",
    "<br/>\n",
    "`train_labels` Shape: (55000, 10) Type: float32 - It needs 2200000 bytes\n",
    "<br/>\n",
    "`weights` Shape: (784, 10) Type: float32 - It needs 31360 bytes\n",
    "<br/>\n",
    "`bias` Shape: (10,) Type: float32 - It needs 40 bytes\n",
    "\n",
    "\n",
    "- the total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory\n",
    "  - you could train this whole dataset on most CPUs and GPUs\n",
    "  - but larger datasets that you'll use in the future measured in gigabytes or more\n",
    "- it's possible to purchase more memory, but it's expensive\n",
    "  - a Titan X GPU with 12 GB of memory costs over \\\\$1,000\n",
    "\n",
    "\n",
    "- instead, in order to run large models on your machine, you'll learn how to use mini-batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Mini-batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in order to use mini-batching, you must first divide your data into batches\n",
    "- unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size\n",
    "  - for example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples\n",
    "  - since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples ($7\\cdot128 + 1\\cdot104 = 1000$)\n",
    "  - in that case, the size of the batches would vary, so you need to take advantage of TensorFlow's `tf.placeholder()` function to receive the varying batch sizes\n",
    "\n",
    "\n",
    "- continuing the example, if each sample had `n_input = 784` features and `n_classes = 10` possible labels, the dimensions for `features` would be `[None, n_input]` and `labels` would be `[None, n_classes]`\n",
    "- what does `None` do here?\n",
    "  - the `None` dimension is a placeholder for the batch size\n",
    "  - at runtime, TensorFlow will accept any batch size greater than $0$\n",
    "\n",
    "\n",
    "- going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples\n",
    "\n",
    "\n",
    "**Question 2:** Use the parameters below, how many batches are there, and what is the last batch size? Batch_size is 128.\n",
    "\n",
    "`features` is (50000, 400)\n",
    "<br/>\n",
    "`labels` is (50000, 10)\n",
    "\n",
    "\n",
    "There's 391 batches (50000 / 128 = 390,625).\n",
    "<br/>\n",
    "The last batch size is 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Mini-batch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `batches` function to batch `features` and `labels`. The function should return each batch with a maximum size of `batch_size`. To help you with the quiz, look at the following example output of a working `batches` function.\n",
    "\n",
    "```python\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "```\n",
    "\n",
    "The example_batches variable would be the following:\n",
    "\n",
    "```python\n",
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import math\\n\\n\\ndef batches(batch_size, features, labels):\\n    \\\"\\\"\\\"\\n    Create batches of features and labels\\n    :param batch_size: The batch size\\n    :param features: List of features\\n    :param labels: List of labels\\n    :return: Batches of (Features, Labels)\\n    \\\"\\\"\\\"\\n    assert len(features) == len(labels)\\n    # TODO: Implement batching\\n    output_batches = []\\n\\n    sample_size = len(features)\\n    for start_i in range(0, sample_size, batch_size):\\n        end_i = start_i + batch_size\\n        batch = [features[start_i:end_i], labels[start_i:end_i]]\\n        output_batches.append(batch)\\n\\n    return output_batches\";\n",
       "                var nbb_formatted_code = \"import math\\n\\n\\ndef batches(batch_size, features, labels):\\n    \\\"\\\"\\\"\\n    Create batches of features and labels\\n    :param batch_size: The batch size\\n    :param features: List of features\\n    :param labels: List of labels\\n    :return: Batches of (Features, Labels)\\n    \\\"\\\"\\\"\\n    assert len(features) == len(labels)\\n    # TODO: Implement batching\\n    output_batches = []\\n\\n    sample_size = len(features)\\n    for start_i in range(0, sample_size, batch_size):\\n        end_i = start_i + batch_size\\n        batch = [features[start_i:end_i], labels[start_i:end_i]]\\n        output_batches.append(batch)\\n\\n    return output_batches\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "\n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"from pprint import pprint\\n\\n# 4 Samples of features\\nexample_features = [\\n    [\\\"F11\\\", \\\"F12\\\", \\\"F13\\\", \\\"F14\\\"],\\n    [\\\"F21\\\", \\\"F22\\\", \\\"F23\\\", \\\"F24\\\"],\\n    [\\\"F31\\\", \\\"F32\\\", \\\"F33\\\", \\\"F34\\\"],\\n    [\\\"F41\\\", \\\"F42\\\", \\\"F43\\\", \\\"F44\\\"],\\n]\\n# 4 Samples of labels\\nexample_labels = [[\\\"L11\\\", \\\"L12\\\"], [\\\"L21\\\", \\\"L22\\\"], [\\\"L31\\\", \\\"L32\\\"], [\\\"L41\\\", \\\"L42\\\"]]\\n\\n# PPrint prints data structures like 2d arrays, so they are easier to read\\npprint(batches(3, example_features, example_labels))\";\n",
       "                var nbb_formatted_code = \"from pprint import pprint\\n\\n# 4 Samples of features\\nexample_features = [\\n    [\\\"F11\\\", \\\"F12\\\", \\\"F13\\\", \\\"F14\\\"],\\n    [\\\"F21\\\", \\\"F22\\\", \\\"F23\\\", \\\"F24\\\"],\\n    [\\\"F31\\\", \\\"F32\\\", \\\"F33\\\", \\\"F34\\\"],\\n    [\\\"F41\\\", \\\"F42\\\", \\\"F43\\\", \\\"F44\\\"],\\n]\\n# 4 Samples of labels\\nexample_labels = [[\\\"L11\\\", \\\"L12\\\"], [\\\"L21\\\", \\\"L22\\\"], [\\\"L31\\\", \\\"L32\\\"], [\\\"L41\\\", \\\"L42\\\"]]\\n\\n# PPrint prints data structures like 2d arrays, so they are easier to read\\npprint(batches(3, example_features, example_labels))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    [\"F11\", \"F12\", \"F13\", \"F14\"],\n",
    "    [\"F21\", \"F22\", \"F23\", \"F24\"],\n",
    "    [\"F31\", \"F32\", \"F33\", \"F34\"],\n",
    "    [\"F41\", \"F42\", \"F43\", \"F44\"],\n",
    "]\n",
    "# 4 Samples of labels\n",
    "example_labels = [[\"L11\", \"L12\"], [\"L21\", \"L22\"], [\"L31\", \"L32\"], [\"L41\", \"L42\"]]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Mini-batch 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    # for batch_features, batch_labels in ______\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```\n",
    "\n",
    "The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. You'll go over this subject in the next section where we talk about \"epochs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an epoch is a single forward and backward pass of the whole dataset\n",
    "- this is used to increase the accuracy of the model without requiring more data\n",
    "- this section will cover epochs in TensorFlow and how to choose the right number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the following TensorFlow code trains a model using 10 epochs\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- running the code will output the following:\n",
    "\n",
    "```python\n",
    "Epoch: 0    - Cost: 11.0     Valid Accuracy: 0.204\n",
    "Epoch: 1    - Cost: 9.95     Valid Accuracy: 0.229\n",
    "Epoch: 2    - Cost: 9.18     Valid Accuracy: 0.246\n",
    "Epoch: 3    - Cost: 8.59     Valid Accuracy: 0.264\n",
    "Epoch: 4    - Cost: 8.13     Valid Accuracy: 0.283\n",
    "Epoch: 5    - Cost: 7.77     Valid Accuracy: 0.301\n",
    "Epoch: 6    - Cost: 7.47     Valid Accuracy: 0.316\n",
    "Epoch: 7    - Cost: 7.2      Valid Accuracy: 0.328\n",
    "Epoch: 8    - Cost: 6.96     Valid Accuracy: 0.342\n",
    "Epoch: 9    - Cost: 6.73     Valid Accuracy: 0.36 \n",
    "Test Accuracy: 0.3801000118255615\n",
    "```\n",
    "\n",
    "- each epoch attempts to move to a lower cost, leading to better accuracy\n",
    "- this model continues to improve accuracy up to Epoch 9. Let's increase the number of epochs to 100\n",
    "\n",
    "```python\n",
    "...\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.86\n",
    "Epoch: 80   - Cost: 0.11     Valid Accuracy: 0.869\n",
    "Epoch: 81   - Cost: 0.109    Valid Accuracy: 0.869\n",
    "....\n",
    "Epoch: 85   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 86   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 87   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 88   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 89   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 90   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 91   - Cost: 0.104    Valid Accuracy: 0.869\n",
    "Epoch: 92   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 93   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 94   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 95   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 96   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 97   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 98   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Epoch: 99   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.8696000006198883\n",
    "```\n",
    "\n",
    "- from looking at the output above, you can see the model doesn't increase the validation accuracy after epoch 80\n",
    "- let's see what happens when we increase the learning rate\n",
    "\n",
    "`learn_rate = 0.1`\n",
    "\n",
    "```python\n",
    "Epoch: 76   - Cost: 0.214    Valid Accuracy: 0.752\n",
    "Epoch: 77   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "Epoch: 78   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "...\n",
    "Epoch: 85   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 86   - Cost: 0.209    Valid Accuracy: 0.756\n",
    "Epoch: 87   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 88   - Cost: 0.208    Valid Accuracy: 0.756\n",
    "Epoch: 89   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 90   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 91   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 92   - Cost: 0.204    Valid Accuracy: 0.756\n",
    "Epoch: 93   - Cost: 0.206    Valid Accuracy: 0.756\n",
    "Epoch: 94   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 95   - Cost: 0.2974   Valid Accuracy: 0.756\n",
    "Epoch: 96   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 97   - Cost: 0.2996   Valid Accuracy: 0.756\n",
    "Epoch: 98   - Cost: 0.203    Valid Accuracy: 0.756\n",
    "Epoch: 99   - Cost: 0.2987   Valid Accuracy: 0.756\n",
    "Test Accuracy: 0.7556000053882599\n",
    "```\n",
    "\n",
    "- looks like the learning rate was increased too much\n",
    "- the final accuracy was lower, and it stopped improving earlier\n",
    "- let's stick with the previous learning rate, but change the number of epochs to 80\n",
    "\n",
    "```python\n",
    "Epoch: 65   - Cost: 0.122    Valid Accuracy: 0.868\n",
    "Epoch: 66   - Cost: 0.121    Valid Accuracy: 0.868\n",
    "Epoch: 67   - Cost: 0.12     Valid Accuracy: 0.868\n",
    "Epoch: 68   - Cost: 0.119    Valid Accuracy: 0.868\n",
    "Epoch: 69   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 70   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 71   - Cost: 0.117    Valid Accuracy: 0.868\n",
    "Epoch: 72   - Cost: 0.116    Valid Accuracy: 0.868\n",
    "Epoch: 73   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 74   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 75   - Cost: 0.114    Valid Accuracy: 0.868\n",
    "Epoch: 76   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 77   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 78   - Cost: 0.112    Valid Accuracy: 0.868\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.868\n",
    "Epoch: 80   - Cost: 0.111    Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.86909999418258667\n",
    "```\n",
    "\n",
    "- the accuracy only reached 0.86, but that could be because the learning rate was too high\n",
    "- lowering the learning rate would require more epochs, but could ultimately achieve better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
