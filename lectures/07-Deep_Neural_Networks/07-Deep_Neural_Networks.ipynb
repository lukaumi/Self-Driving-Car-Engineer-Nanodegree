{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the last lesson we've trained a simple logistic classifier on images\n",
    "- now, we're going to take this classifier and turn it into a deep network\n",
    "  - and it's going to be just a few lines of code, so make sure you understand well what was going on in the previous model\n",
    "- in the second part you are going to take a small peak into how our optimizer does all the hard work for you computing gradients for arbitrary functions\n",
    "- and then we are going to look together at the important topic of regularization, which will enable us to train much, much larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the simple model that you've trained so far is nice, but it's also relatively limited\n",
    "- here is a question for you\n",
    "  - how many train parameters did it actually have?\n",
    "  - as a reminder, each input was a 28 by 28 image, and the output was 10 classes\n",
    "\n",
    "<img src=\"resources/model_question_number_of_parameters.png\" style=\"width: 70%;\">\n",
    "\n",
    "- the matrix, $W$, here, takes, as an input, the entire image, so $28 \\times 28$ pixels\n",
    "- the output is of size $10$ (letters A - J), so the other dimension of the matrix is $10$\n",
    "- the biases are just $1 \\times 10$\n",
    "- so the total number of parameters are $28 \\times 28 \\times 10 + 10 = 7,850$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that's the case in general\n",
    "- if you have $N$ inputs, and $K$ outputs, you have $(N+1)K$ parameters to use, not one more\n",
    "\n",
    "<img src=\"resources/number_of_parameters.png\" style=\"width: 70%;\">\n",
    "\n",
    "- the thing is, you might want to use many, many more parameters in practice\n",
    "- also, it's linear\n",
    "  - this means that the kind of interactions that you're capable of representing with that model is somewhat limited\n",
    "    - for example, if two inputs interact in an additive way ($Y = X_1 + X_2$), your model can represent them well as a matrix multiply\n",
    "    - but if two inputs interact in the way that the outcome depends on the product ($Y = X_1 \\times X_2$) of the two for example, you won't be able to model that efficiently with a linear model\n",
    "  - linear operations are really nice though\n",
    "    - big matrix multiplies are exactly what GPUs were designed for; they're relatively cheap and very, very fast\n",
    "  - numerically, linear operations are very stable: $Y = WX \\rightarrow \\Delta Y \\sim |W| \\Delta X$\n",
    "    - we can show mathematically that small changes in the input ($\\Delta X$) can never yield big changes in the output ($\\Delta Y$)\n",
    "  - the derivates are very nice too\n",
    "    - the derivative of a linear function ($Y = WX$) is constant\n",
    "    - $\\dfrac {dY}{dX} = W^T$ &nbsp;&nbsp;,&nbsp;&nbsp; $\\dfrac {dY}{dW} = X^T$\n",
    "    - you can't get more stable numerically than a constant\n",
    "\n",
    "\n",
    "- we would like to keep our parameters inside big linear functions, but we would also want the entire model to be nonlinear\n",
    "- we can't just keep multiplying our inputs by linear functions, because that's just equivalent to one big linear function\n",
    "- so, we're going to have to introduce non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectified Linear Units (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let me introduce you to the lazy engineer's favorite non-linear function: **the rectified linear units**, or **ReLU** for short\n",
    "- ReLUs are literally the simplest non-linear functions you can think of\n",
    "- they're linear if $x > 0$, and they're the $0$ everywhere else\n",
    "\n",
    "<img src=\"resources/RELU_function.png\" style=\"width: 50%;\">\n",
    "\n",
    "- ReLUs have nice derivatives, as well\n",
    "  - when $x < 0$ the ReLU is $0$ so the derivative is $0$ as well\n",
    "  - when $x > 0$ the ReLU is equal to $x$ so the derivative is equal to $1$\n",
    "  - the derivative of the segment to the left would be zero, as the value remains constant ($x=0$), and to the right would be a constant ($=1$) since it grows linearly ($y=x$)\n",
    "\n",
    "<img src=\"resources/RELU_function_derivative.png\" style=\"width: 30%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network of ReLUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- because we're lazy engineers, we're going to take something that works - our logistic classifier and do the minimal amount of change to make it nonlinear\n",
    "- we're going to construct our new function in the simplest way that we can think of\n",
    "- instead of having a single matrix multiply as our classifier, we're going to insert a ReLU right in the middle\n",
    "- we now have two matrices, one going from the inputs to the ReLUs and another one connecting the ReLUs to the classifier\n",
    "- we've solved two of our problems\n",
    "  - our function is now nonlinear, thanks to the ReLU in the middle\n",
    "  - and we now have a new knob that we can tune this number $H$ which corresponds to the number of ReLU units that we have in the classifier; we can make it as big as we want\n",
    "\n",
    "<img src=\"resources/network_of_RELUs.png\" style=\"width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: TensorFlow ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this lesson, you'll learn how to build multilayer neural networks with TensorFlow\n",
    "- adding a hidden layer to a network allows it to model more complex functions\n",
    "- also, using a non-linear activation function on the hidden layer lets it model non-linear functions\n",
    "\n",
    "<img src=\"resources/two_layer_neural_network_relu.png\" style=\"width: 70%;\">\n",
    "\n",
    "- depicted above is a \"2-layer\" neural network:\n",
    "  - the first layer effectively consists of the set of weights and biases applied to $X$ and passed through ReLUs\n",
    "    - the output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer\n",
    "  - the second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a rectified linear unit (ReLU) is type of [activation function](https://en.wikipedia.org/wiki/Activation_function) that is defined as `f(x) = max(0, x)`\n",
    "  - the function returns $0$ if `x` is negative, otherwise it returns `x`\n",
    "- TensorFlow provides the ReLU function as `tf.nn.relu()`\n",
    "\n",
    "```python\n",
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "```\n",
    "\n",
    "- the above code applies the `tf.nn.relu()` function to the hidden_layer, effectively turning off any negative weights and acting like an on/off switch\n",
    "- adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function\n",
    "- this nonlinearity allows the network to solve more complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# In this quiz, you'll use TensorFlow's ReLU function to turn the linear model below into a nonlinear model.\\n\\nimport tensorflow as tf\\n\\noutput = None\\nhidden_layer_weights = [\\n    [0.1, 0.2, 0.4],\\n    [0.4, 0.6, 0.6],\\n    [0.5, 0.9, 0.1],\\n    [0.8, 0.2, 0.8],\\n]\\nout_weights = [[0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]\\n\\n# Weights and biases\\nweights = [tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]\\nbiases = [tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]\\n\\n# Input\\nfeatures = tf.Variable(\\n    [[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]]\\n)\";\n",
       "                var nbb_formatted_code = \"# In this quiz, you'll use TensorFlow's ReLU function to turn the linear model below into a nonlinear model.\\n\\nimport tensorflow as tf\\n\\noutput = None\\nhidden_layer_weights = [\\n    [0.1, 0.2, 0.4],\\n    [0.4, 0.6, 0.6],\\n    [0.5, 0.9, 0.1],\\n    [0.8, 0.2, 0.8],\\n]\\nout_weights = [[0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]\\n\\n# Weights and biases\\nweights = [tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]\\nbiases = [tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]\\n\\n# Input\\nfeatures = tf.Variable(\\n    [[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this quiz, you'll use TensorFlow's ReLU function to turn the linear model below into a nonlinear model.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8],\n",
    "]\n",
    "out_weights = [[0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]\n",
    "biases = [tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable(\n",
    "    [[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1099997  8.44     ]\n",
      " [ 0.         0.       ]\n",
      " [24.01      38.24     ]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# TODO: Create Model\\nhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\\nhidden_layer = tf.nn.relu(hidden_layer)\\nlogits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\\n\\n# TODO: save and print session results on a variable named \\\"output\\\"\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    output = sess.run(logits)\\n    print(output)\";\n",
       "                var nbb_formatted_code = \"# TODO: Create Model\\nhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\\nhidden_layer = tf.nn.relu(hidden_layer)\\nlogits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\\n\\n# TODO: save and print session results on a variable named \\\"output\\\"\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    output = sess.run(logits)\\n    print(output)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: save and print session results on a variable named \"output\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(logits)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yes, I could talk about neural networks as metaphors for the brain\n",
    "- it's nice and it might even be true, but it comes with a lot of baggage and it can sometimes lead you astray\n",
    "- so I'm not going to talk about it at all in this course\n",
    "- no need to be a wizard neural scientist\n",
    "- neural networks naturally make sense if you're simply a lazy engineer with a big GPU who just wants machine learning to work better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as long as you know how to write the derivatives of your individual functions, there is a simple graphical way to combine them together and compute the derivative for the whole function\n",
    "- there is a way to write this chain rule that is very efficient computationally, with lots of data reuse and that looks like a very simple data pipeline\n",
    "\n",
    "<img src=\"resources/chain_rule.png\" style=\"width: 80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imagine your network is a stack of simple operations; like linear transforms, ReLUs, whatever you want\n",
    "  - some have parameters, like the matrix transforms, some don't, like the ReLUs\n",
    "- when you apply your data to some input $x$, you have data flowing through the stack up to your predictions $y$\n",
    "- to compute the derivatives, you create another graph that looks like this:\n",
    "\n",
    "<img src=\"resources/backprop.png\" style=\"width: 80%;\">\n",
    "\n",
    "- the data in that new graph flows backwards through the network, gets combined using the chain rule that we saw before, and produces gradients\n",
    "  - that graph can be derived completely automatically from the individual operations in your network\n",
    "  - so, most deep learning frameworks will just do it for you\n",
    "  - this is called back propagation and it's a very powerful concept\n",
    "    - it makes computing derivatives of complex function very efficient, as long as the function is made up of simple blocks with simple derivatives\n",
    "\n",
    "\n",
    "- running the model up to the predictions is often called the *forward prop*\n",
    "- and the model that goes backwards is called the *back prop*\n",
    "\n",
    "\n",
    "- to recap, to run stochastic gradient descent for every single little batch of your data in your training set, you're going to run the forward prop and then the back prop\n",
    "  - that will give you gradient for each of your weights in your model\n",
    "- then you're going to apply those gradients with learning rates to your original weights and update them\n",
    "- you're going to repeat that all over again many, many times\n",
    "- this is how your entire model gets optimized\n",
    "\n",
    "\n",
    "- keep in mind this diagram\n",
    "- in particular, each block of the back prop often takes about twice the memory that's needed for the forward prop and twice to compute\n",
    "  - that's important when you want to size your model and fit in memory for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network in TensorFlow\n",
    "\n",
    "- in the following walkthrough, we'll step through TensorFlow code written to classify the letters in the MNIST database\n",
    "- if you would like to run the network on your computer, the file is provided [here](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61a3a_multilayer-perceptron/multilayer-perceptron.zip)\n",
    "- you can find this and many more examples of TensorFlow at Aymeric Damien's GitHub [repository](https://github.com/aymericdamien/TensorFlow-Examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow MNIST\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "```\n",
    "\n",
    "- you'll use the MNIST dataset provided by TensorFlow, which batches and One-Hot encodes the data for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parameters\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "```\n",
    "\n",
    "- the focus here is on the architecture of multilayer neural networks, not parameter tuning, so here we'll just give you the learning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer Parameters\n",
    "\n",
    "```python\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "```\n",
    "\n",
    "- the variable `n_hidden_layer` determines the size of the hidden layer in the neural network\n",
    "  - this is also known as the width of a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "\n",
    "```python\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "```\n",
    "\n",
    "- deep neural networks use multiple layers with each layer requiring it's own weight and bias\n",
    "  - the `'hidden_layer'` weight and bias is for the hidden layer\n",
    "  - the `'out'` weight and bias is for the output layer\n",
    "  - if the neural network were deeper, there would be weights and biases for each additional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "```python\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "```\n",
    "\n",
    "- the MNIST data is made up of 28px by 28px images with a single [channel](https://en.wikipedia.org/wiki/Channel_(digital_image%29)\n",
    "- the `tf.reshape()` function above reshapes the 28px by 28px matrices in `x` into row vectors of 784px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "<img src=\"resources/multi_layer_perceptron.png\" style=\"width: 70%;\">\n",
    "\n",
    "```python\n",
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n",
    "```\n",
    "\n",
    "- you've seen the linear function `tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])` before, also known as `xw + b`\n",
    "- combining linear functions together using a ReLU will give you a two layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "```python\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "```\n",
    "\n",
    "- this is the same optimization technique used in the Intro to TensorFLow lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "```python\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "```\n",
    "\n",
    "- the MNIST library in TensorFlow provides the ability to receive the dataset in batches\n",
    "- calling the `mnist.train.next_batch()` function returns a subset of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Neural Network\n",
    "\n",
    "<img src=\"resources/deeper_neural_network.png\" style=\"width: 70%;\">\n",
    "\n",
    "- that's it!\n",
    "- going from one layer to two is easy\n",
    "- adding more layers to the network allows you to solve more complicated problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Learning Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so now you have a small neural network; it's not particularly deep, just 2 layers\n",
    "- you can make it bigger, more complex by increasing the size of that hidden layer in the middle, but it turns out that increasing this $H$ is not particularly efficient in general\n",
    "- you need to make it very, very big, and then it gets really hard to train\n",
    "- this is where the central idea of deep learning comes into play\n",
    "- instead you can also add more layers and make your model deeper\n",
    "\n",
    "<img src=\"resources/adding_more_layers.png\" style=\"width: 70%;\">\n",
    "\n",
    "- there are lots of good reasons to do that\n",
    "- one is parameter efficiency\n",
    "  - you can typically get much more performance with pure parameters by going deeper, rather than wider\n",
    "\n",
    "<img src=\"resources/deeper_vs_wider.png\" style=\"width: 70%;\">\n",
    "  \n",
    "- another one is that a lot of natural phenomena, that you might be interested in, tend to have a hierarchical structure which deep models naturally capture\n",
    "  - if you poke at a model for images, for example, and visualize what the model learns you'll often find very simple things at the lowest layers, like lines or edges\n",
    "  - once you move up, you tend to see more complicated things like geometric shapes\n",
    "  - go further up, and you start seeing things like objects, faces\n",
    "  - this is very powerful, because the model structure matches the kind of abstractions that you might expect to see in your data, and as a result the model has an easier time learning them\n",
    "\n",
    "<img src=\"resources/hierarchical_structure.png\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Restore TensorFlow Models\n",
    "\n",
    "- training a model can take hours\n",
    "- but once you close your TensorFlow session, you lose all the trained weights and biases\n",
    "- if you were to reuse the model in the future, you would have to train it all over again!\n",
    "- fortunately, TensorFlow gives you the ability to save your progress using a class called `tf.train.Saver`\n",
    "  - this class provides the functionality to save any `tf.Variable` to your file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Variables\n",
    "\n",
    "- let's start with a simple example of saving `weights` and `bias` Tensors\n",
    "- for the first example you'll just save two variables\n",
    "- later examples will save all the weights in a practical model\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# The file path to save the data\n",
    "save_file = \"./model.ckpt\"\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print(\"Weights:\")\n",
    "    print(sess.run(weights))\n",
    "    print(\"Bias:\")\n",
    "    print(sess.run(bias))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "```\n",
    "\n",
    "- the Tensors `weights` and `bias` are set to random values using the `tf.truncated_normal()` function\n",
    "- the values are then saved to the` save_file` location, \"model.ckpt\", using the `tf.train.Saver.save()` function (the \".ckpt\" extension stands for \"checkpoint\")\n",
    "\n",
    "\n",
    "- if you're using TensorFlow 0.11.0RC1 or newer, a file called \"model.ckpt.meta\" will also be created\n",
    "  - this file contains the TensorFlow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Variables\n",
    "\n",
    "```python\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print(\"Weight:\")\n",
    "    print(sess.run(weights))\n",
    "    print(\"Bias:\")\n",
    "    print(sess.run(bias))\n",
    "```\n",
    "\n",
    "- you'll notice you still need to create the weights and bias Tensors in Python\n",
    "- the `tf.train.Saver.restore()` function loads the saved data into weights and bias\n",
    "  - since `tf.train.Saver.restore()` sets all the TensorFlow Variables, you don't need to call `tf.global_variables_initializer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Trained Model\n",
    "\n",
    "- first start with a model:\n",
    "\n",
    "```python\n",
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's train that model, then save the weights:\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(\n",
    "                optimizer,\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    features: mnist.validation.images,\n",
    "                    labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
    "                epoch,\n",
    "                valid_accuracy))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model\n",
    "\n",
    "- let's load the weights and bias from memory, then check the test accuracy\n",
    "\n",
    "```python\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "- sometimes you might want to adjust, or \"finetune\" a model that you have already trained and saved\n",
    "-  however, loading saved Variables directly into a modified model can generate errors\n",
    "- let's go over how to avoid these problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Error\n",
    "\n",
    "- TensorFlow uses a string identifier for Tensors and Operations called `name`\n",
    "- if a name is not given, TensorFlow will create one automatically\n",
    "- TensorFlow will give the first node the name `<Type>`, and then give the name `<Type>_<number>` for the subsequent nodes\n",
    "- let's see how this can affect loading a model with a different order of weights and bias:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - ERROR\n",
    "    saver.restore(sess, save_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the code above prints out the following:\n",
    "\n",
    "```python\n",
    "Save Weights: Variable:0\n",
    "```\n",
    "```python\n",
    "Save Bias: Variable_1:0\n",
    "```\n",
    "```python\n",
    "Load Weights: Variable_1:0\n",
    "\n",
    "```\n",
    "```python\n",
    "Load Bias: Variable:0\n",
    "```\n",
    "```python\n",
    "...\n",
    "```\n",
    "```python\n",
    "InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match.\n",
    "```\n",
    "```python\n",
    "...\n",
    "```\n",
    "\n",
    "- you'll notice that the `name` properties for `weights` and `bias` are different than when you saved the model\n",
    "  - this is why the code produces the \"Assign requires shapes of both tensors to match\" error\n",
    "  - the code `saver.restore(sess, save_file)` is trying to load weight data into bias and bias data into weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- instead of letting TensorFlow set the name property, let's set it manually:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')\n",
    "```\n",
    "\n",
    "- the code above prints out the following:\n",
    "\n",
    "```python\n",
    "Save Weights: weights_0:0\n",
    "```\n",
    "```python\n",
    "Save Bias: bias_0:0\n",
    "```\n",
    "```python\n",
    "Load Weights: weights_0:0\n",
    "\n",
    "```\n",
    "```python\n",
    "Load Bias: bias_0:0\n",
    "```\n",
    "```python\n",
    "Loaded Weights and Bias successfully.\n",
    "```\n",
    "\n",
    "- that worked! The Tensor names match and the data loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why did we not figure out earlier that deep models were effective?\n",
    "  - many reasons, but mostly because deep models only really shine if you have enough data to train them\n",
    "- it's only in recent years that large enough data sets have made their way to the academic world\n",
    "\n",
    "\n",
    "- we know better today how to train very, very big models using better regularization techniques\n",
    "- there is a general issue when you're doing numerical optimization which I call the *skinny jeans problem*\n",
    "  - skinny jeans look great, they fit perfectly, but they're really, really hard to get into\n",
    "  - so most people end up wearing jeans that are just a bit too big\n",
    "  - it's exactly the same with deep networks\n",
    "    - the network that's just the right size for your data is very, very hard to optimize\n",
    "    - so in practice, we always try networks that are way too big for our data and then we try our best to prevent them from overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the first way we prevent over fitting, is by looking at the performance on our validation set and stopping to train, as soon as we stop improving\n",
    "  - it's called early termination, and it's still the best way to prevent your network from over optimizing on the training set\n",
    "\n",
    "<img src=\"resources/early_termination.png\" style=\"width: 70%;\">\n",
    "\n",
    "- another way is to apply regularization\n",
    "  - regularizing means applying artificial constraints on your network, that implicitly reduce the number of free parameters while not making it more difficult to optimize\n",
    "  - in the skinny jeans analogy, think stretch pants; they fit just as well, but because they're flexible, they don't make things harder to fit in\n",
    "\n",
    "\n",
    "- the stretch pants of deep learning are called **L2 Regularization**\n",
    "  - the idea is to add another term ($\\beta \\dfrac{1}{2} ||w||_2^2 $) to the loss ($£$), which penalizes large weights\n",
    "  - it's typically achieved by adding the L2 norm of your weights to the loss, multiplied by a small constant\n",
    "\n",
    "<img src=\"resources/L2_regularization.png\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the nice thing about L2 Regularization is that it’s very, very simple\n",
    "  - because you just add it your loss, the structure of your network doesn’t have to change\n",
    "  - you can even compute its derivative by hand\n",
    "  - remember that the L2 norm stands for the sum of the squares of the individual elements in a vector\n",
    "\n",
    "\n",
    "- the formula for the derivative of L2 norm of a vector:\n",
    "\n",
    "<img src=\"resources/L2_norm_derivative_of_vector.png\" style=\"width: 30%;\">\n",
    "\n",
    "- the derivative of $\\dfrac{1}{2}x^2$ in one dimension, is simply $x$\n",
    "  - so when you take that derivative for each of the components of your vector, you get the same components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there's another important technique for regularization that only emerged relatively recently and works amazingly well\n",
    "  - it's called **Dropout**\n",
    "- dropout works like this:\n",
    "  - imagine that you have one layer that connects to another layer\n",
    "  - the values that go from one layer to the next are often called *activations*\n",
    "  - now take those activations and randomly, for every example you train your network on, set half of them to $0$\n",
    "    - completely and randomly, you basically take half of the data that's flowing through your network, and just destroy it and then randomly do the process again\n",
    "\n",
    "\n",
    "- so what happens with dropout?\n",
    "  - your network can never rely on any given activation to be present, because they might be squashed at any given moment\n",
    "  - so it is forced to learn a redundant representation for everything to make sure that at least some of the information remains\n",
    "  - it's like a game of whack-a-mole\n",
    "    - one activations get smashed, but there is always one or more that do the same job and that don't get killed, so everything remains fine at the end\n",
    "\n",
    "\n",
    "- forcing your network to learn redundant representations might sound very inefficient\n",
    "- in practice, it makes things more robust, and prevents over fitting\n",
    "- it also makes your network act as if taking the consensus over an ensemble of networks, which is always a good way to improve performance\n",
    "\n",
    "\n",
    "- dropout is one of the most important techniques to emerge in the last few years\n",
    "- if dropout doesn't work for you, you should probably be using a bigger network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when you evaluate the network that's been trained with drop out, you obviously no longer want this randomness, you want something deterministic\n",
    "- you're going to want to take the consensus over these redundant models\n",
    "  - you get the consensus opinion by averaging the activations\n",
    "  - you want $y_e$ to be the average of all the yts that you got during training ($y_e \\sim E(y_t)$)\n",
    "\n",
    "\n",
    "- here's a trick to make sure this expectation holds\n",
    "- during training, not only do you use zero outs to the activations that you drop out, but you also scale the remaining activations by a factor of $2$\n",
    "\n",
    "<img src=\"resources/consensus opinion_1.png\" style=\"width: 70%;\">\n",
    "\n",
    "- this way, when it comes time to average them during evaluation, you just remove these dropouts and scaling operations from your neural net\n",
    "- the result is an average of these activations that is properly scaled\n",
    "\n",
    "<img src=\"resources/consensus opinion_2.png\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/dropout-node.jpeg\" style=\"width: 50%;\">\n",
    "\n",
    "- dropout is a regularization technique for reducing overfitting\n",
    "- the technique temporarily drops units ([artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)) from the network, along with all of those units' incoming and outgoing connections\n",
    "\n",
    "\n",
    "- TensorFlow provides the `tf.nn.dropout()` function, which you can use to implement dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "```\n",
    "\n",
    "- the code above illustrates how to apply dropout to a neural network\n",
    "- the `tf.nn.dropout()` function takes in two parameters:\n",
    "  - `hidden_layer`: the tensor to which you would like to apply dropout\n",
    "  - `keep_prob`: the probability of keeping (i.e. not dropping) any given unit\n",
    "\n",
    "\n",
    "- `keep_prob` allows you to adjust the number of units to drop\n",
    "- in order to compensate for dropped units, `tf.nn.dropout()` multiplies all units that are kept (i.e. not dropped) by `1/keep_prob`\n",
    "\n",
    "\n",
    "- during training, a good starting value for `keep_prob` is $0.5$\n",
    "- during testing, use a `keep_prob` value of $1.0$ to keep all units and maximize the power of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Dropout Quiz 1\n",
    "\n",
    "- take a look at the code snippet below; do you see what's wrong?\n",
    "- there's nothing wrong with the syntax, however the test accuracy is extremely low\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "...\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i in range(batches):\n",
    "            ....\n",
    "\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                keep_prob: 0.5})\n",
    "\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "        features: test_features,\n",
    "        labels: test_labels,\n",
    "        keep_prob: 0.5})\n",
    "```\n",
    "\n",
    "- `keep_prob` should be set to $1.0$ when evaluating validation accuracy\n",
    "  - you should only drop units while training the model\n",
    "  - during validation or testing, you should keep all of the units to maximize accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Dropout Quiz 2\n",
    "\n",
    "- this quiz will be starting with the code from the ReLU Quiz and applying a dropout layer\n",
    "  - build a model with a ReLU layer and dropout layer using the `keep_prob` placeholder to pass in a probability of $0.5$\n",
    "  - print the logits from the model\n",
    "\n",
    "\n",
    "**Note:** Output will be different every time the code is run. This is caused by dropout randomizing the units it drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import tensorflow as tf\\nfrom test import *\\n\\ntf.set_random_seed(123456)\\n\\n\\nhidden_layer_weights = [\\n    [0.1, 0.2, 0.4],\\n    [0.4, 0.6, 0.6],\\n    [0.5, 0.9, 0.1],\\n    [0.8, 0.2, 0.8],\\n]\\nout_weights = [[0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]\\n\\n\\n# set random seed\\ntf.set_random_seed(123456)\\n\\n\\n# Weights and biases\\nweights = [tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]\\nbiases = [tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]\\n\\n\\n# Input\\nfeatures = tf.Variable(\\n    [[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]]\\n)\";\n",
       "                var nbb_formatted_code = \"import tensorflow as tf\\nfrom test import *\\n\\ntf.set_random_seed(123456)\\n\\n\\nhidden_layer_weights = [\\n    [0.1, 0.2, 0.4],\\n    [0.4, 0.6, 0.6],\\n    [0.5, 0.9, 0.1],\\n    [0.8, 0.2, 0.8],\\n]\\nout_weights = [[0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]\\n\\n\\n# set random seed\\ntf.set_random_seed(123456)\\n\\n\\n# Weights and biases\\nweights = [tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]\\nbiases = [tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]\\n\\n\\n# Input\\nfeatures = tf.Variable(\\n    [[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from test import *\n",
    "\n",
    "tf.set_random_seed(123456)\n",
    "\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8],\n",
    "]\n",
    "out_weights = [[0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]\n",
    "\n",
    "\n",
    "# set random seed\n",
    "tf.set_random_seed(123456)\n",
    "\n",
    "\n",
    "# Weights and biases\n",
    "weights = [tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]\n",
    "biases = [tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]\n",
    "\n",
    "\n",
    "# Input\n",
    "features = tf.Variable(\n",
    "    [[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f02b38efc5c5>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[[ 8.46      9.400001]\n",
      " [ 0.        0.      ]\n",
      " [14.280001 33.100002]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# TODO: Create Model with Dropout\\nkeep_prob = tf.placeholder(tf.float32)\\nhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\\nhidden_layer = tf.nn.relu(hidden_layer)\\nhidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\\n\\nlogits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\\n\\n\\n# TODO: save and print session results as variable named \\\"output\\\"\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    output = sess.run(logits, feed_dict={keep_prob: 0.5})\\n    print(output)\";\n",
       "                var nbb_formatted_code = \"# TODO: Create Model with Dropout\\nkeep_prob = tf.placeholder(tf.float32)\\nhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\\nhidden_layer = tf.nn.relu(hidden_layer)\\nhidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\\n\\nlogits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\\n\\n\\n# TODO: save and print session results as variable named \\\"output\\\"\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n    output = sess.run(logits, feed_dict={keep_prob: 0.5})\\n    print(output)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "\n",
    "# TODO: save and print session results as variable named \"output\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(logits, feed_dict={keep_prob: 0.5})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
