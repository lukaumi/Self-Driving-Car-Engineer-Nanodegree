{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so far, we've talked about neural networks in general\n",
    "- but if you know something about your data, for example, if it's an image or a sequence of things you can do a lot better\n",
    "- one of the very popular ways to structure a neural network is called a Convolutional Neural Network\n",
    "- it's become incredibly popular for things like image processing and processing large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if your data has some structure, and your network doesn't have to learn that structure from scratch, it's going to perform better\n",
    "- imagine, for example, that you're trying to classify those letters, and you know that color is really not a factor in what makes an A an A\n",
    "\n",
    "<img src=\"resources/color_doesnt_matter.png\" style=\"width: 70%;\"/>\n",
    "\n",
    "- what do you think would be easier for your classifier to learn?\n",
    "  - a model that uses the color image, or a model that only looks at the gray scale\n",
    "    - intuitively, if a new letter comes up in a color that you've never seen before, it's going to be a lot easier for your model that ignores the color to begin with to classify that letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here's another example\n",
    "  - you have an image, and you want your network to say it's an image with a cat in it\n",
    "  - it doesn't really matter where the cat is, it's still an image with a cat\n",
    "  - if your network has to learn about kittens in the left corner, and about kittens in the right corner independently, that's a lot of work that it has to do\n",
    "  - how about you telling it, instead explicitly, that objects and images are largely the same whether they're on the left or on the right of the picture\n",
    "    - that's what's called **translation invariance**\n",
    "      - different positions, same kitten\n",
    "\n",
    "<img src=\"resources/translation_invariance_kittens.png\" style=\"width: 30%;\"/>\n",
    "\n",
    "- yet another example\n",
    "  - imagine you had a long text that talks about kittens\n",
    "  - does the meaning of kitten change depending on whether it's in the first sentence or in the second one?\n",
    "  - mostly not, so if you're trying to network on text, maybe you want the part of the network that learns what a kitten is to be reused every time you see the word kitten, and not have to re-learn it every time\n",
    "  - the way you achieve this in your own networks, is using what's called **weight sharing**\n",
    "    - when you know that two inputs can contain the same kind of information, then you share their weights\n",
    "    - and train the weights jointly for those inputs\n",
    "    - *statistical invariants*, things that don't change on average across time or space, are everywhere\n",
    "    - for images, the idea of weight sharing will get us to study convolutional networks\n",
    "      - for text and sequences in general, it will lead us to embeddings and recurrent neural networks\n",
    "\n",
    "<img src=\"resources/weight_sharing_kittens.png\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's talk about Convolutional Networks, or **ConvNets**\n",
    "- ConvNets are neural networks that share their parameters across space\n",
    "\n",
    "\n",
    "- imagine you have an image\n",
    "- it can be represented as a flat pancake\n",
    "  - it has a width, a height, and because you typically have red, green, and blue channels, it also has a depth\n",
    "  - in this instance, depth is three\n",
    "- that's your input\n",
    "\n",
    "<img src=\"resources/input_pancake.png\"/>\n",
    "\n",
    "- now, imagine taking a small patch of this image, and running a tiny neural network on it, with say, $K$ outputs\n",
    "- let's represent those outputs vertically in a tiny column like this\n",
    "\n",
    "<img src=\"resources/outputs_column_slider.png\"/>\n",
    "\n",
    "- now, let's slide that little neural network across the image without changing the weights\n",
    "  - just slide across invertically like we're painting it with a brush\n",
    "- on the output, we've drawn another image\n",
    "\n",
    "<img src=\"resources/convolution.png\"/>\n",
    "\n",
    "- it's got a different width, a different height and more importantly, it's got a different depth\n",
    "- instead of just R, G, and B, now you have an output that's got many colored channels, $K$ of them\n",
    "- this operation is called the **convolution**\n",
    "\n",
    "\n",
    "- if your patch size were the size of the whole image, it would be no different than the regular layer of a neural network\n",
    "- but because we have this small patch instead, we have many fewer weights and they are shared across space\n",
    "- a ConvNet is going to basically be a deep network where instead of having stacks of matrix multiply layers, we're going to have stacks of convolutions\n",
    "  - the general idea is that they will form a pyramid\n",
    "  - at the bottom, you have this big image, but very shallow just R, G, and B\n",
    "  - you're going to apply convolutions that are going to progressively squeeze the spacial dimensions while increasing the depth which corresponds roughly to the semantic complexity of your representation\n",
    "  - at the top, you can put your classifier; you have a representation where all this spacial information has been squeezed out, and only parameters that map to content of the image remain\n",
    "\n",
    "<img src=\"resources/pyramid_convolutions.png\" style=\"width:70%\"/>\n",
    "\n",
    "- so that’s the general idea\n",
    "- if you're going to implement this, there are lots of little details to get right, and a fair bit of lingo to get used to\n",
    "- you've met the concept of *patch* and *depth*\n",
    "- patches are sometimes called Kernels\n",
    "\n",
    "<img src=\"resources/patch_and_depth.png\" style=\"width:70%\"/>\n",
    "\n",
    "- each pancake in your stack is called a feature map\n",
    "- here, you're mapping three feature maps to $K$ feature maps\n",
    "\n",
    "<img src=\"resources/feature_maps.png\" style=\"width:70%\"/>\n",
    "\n",
    "- another term that you need to know is *stride*\n",
    "  - it's the number of pixels that you're shifting each time you move your filter\n",
    "  - the stride of one makes the output roughly the same size as the input\n",
    "  - a stride of two means it's about half the size\n",
    "  \n",
    "<img src=\"resources/stride_of_one.png\"/>\n",
    "\n",
    "<img src=\"resources/stride_of_two.png\"/>\n",
    "\n",
    "\n",
    "- I say roughly because it depends a bit about what you do at the edge of your image\n",
    "  - either you don't go pass the edge\n",
    "    - it's often called *valid padding* as a shortcut\n",
    "  - or you go off the edge and pad with zeros in such a way that the output map size is exactly the same size as the input map\n",
    "    - that is often called *same padding* as a shortcut\n",
    "    \n",
    "<img src=\"resources/valid_padding.png\"/>\n",
    "\n",
    "<img src=\"resources/same_padding.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's develop better intuition for how Convolutional Neural Networks (CNN) work\n",
    "- we'll examine how humans classify images, and then see how CNNs use similar approaches\n",
    "\n",
    "\n",
    "- let’s say we wanted to classify the following image of a dog as a Golden Retriever\n",
    "- as humans, how do we do this?\n",
    "  - one thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur\n",
    "  - we essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog\n",
    "  - in this case, we might break down the image into a combination of the following:\n",
    "    - a nose, two eyes, golden fur\n",
    "  - but let’s take this one step further\n",
    "    - how do we determine what exactly a nose is?\n",
    "      - Golden Retriever nose can be seen as an oval with two black holes inside it\n",
    "      - one way of classifying a Retriever’s nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below\n",
    "\n",
    "\n",
    "- broadly speaking, this is what a CNN learns to do\n",
    "- it learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image\n",
    "- finally, the CNN classifies the image by combining the larger, more complex objects\n",
    "\n",
    "\n",
    "- in our case, the levels in the hierarchy are:\n",
    "  - simple shapes, like ovals and dark circles\n",
    "  - complex objects (combinations of simple shapes), like eyes, nose, and fur\n",
    "  - the dog as a whole (a combination of complex objects)\n",
    "\n",
    "\n",
    "- with deep learning, we don't actually program the CNN to recognize these specific features\n",
    "- rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation!\n",
    "- it's amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for\n",
    "\n",
    "<img src=\"resources/golder_retriever_hierarchy_diagram.jpg\"/>\n",
    "\n",
    "- a CNN might have several layers, and each layer might capture a different level in the hierarchy of objects\n",
    "  - the first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors\n",
    "  - the subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.\n",
    "\n",
    "\n",
    "- once again, the CNN ***learns all of this on its own***\n",
    "- we don't ever have to tell the CNN to go looking for lines or curves or noses or fur\n",
    "- the CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking up an Image\n",
    "\n",
    "- the first step for a CNN is to break up the image into smaller pieces\n",
    "- we do this by selecting a width and height that defines a filter\n",
    "- the filter looks at small pieces, or patches, of the image\n",
    "  - these patches are the same size as the filter\n",
    "\n",
    "<img src=\"resources/patches_kitten.png\" style=\"width:60%\"/>\n",
    "\n",
    "- we then simply slide this filter horizontally or vertically to focus on a different piece of the image\n",
    "- the amount by which the filter slides is referred to as the *stride*\n",
    "  - the stride is a hyperparameter which you, the engineer, can tune\n",
    "  - increasing the stride reduces the size of your model by reducing the number of total patches each layer observes\n",
    "    - however, this usually comes with a reduction in accuracy\n",
    "\n",
    "\n",
    "- let’s look at an example\n",
    "- in this zoomed in image of the dog, we first start with the patch outlined in red\n",
    "- the width and height of our filter define the size of this square\n",
    "\n",
    "<img src=\"resources/golden_retriever_one_patch_1.png\"/>\n",
    "\n",
    "- we then move the square over to the right by a given stride (2 in this case) to get another patch\n",
    "\n",
    "<img src=\"resources/golden_retriever_one_patch_2.png\"/>\n",
    "\n",
    "- what's important here is that we are grouping together adjacent pixels and treating them as a collective\n",
    "- in a normal, non-convolutional neural network, we would have ignored this adjacency\n",
    "- in a normal network, we would have connected every pixel in the input image to a neuron in the next layer\n",
    "- in doing so, we would not have taken advantage of the fact that pixels in an image are close together for a reason and have special meaning\n",
    "- by taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=YRhxdVk_sIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Depth\n",
    "\n",
    "- it's common to have more than one filter\n",
    "- different filters pick up different qualities of a patch\n",
    "- for example, one filter might look for a particular color, while another might look for a kind of object of a specific shape\n",
    "- the amount of filters in a convolutional layer is called the *filter depth*\n",
    "\n",
    "\n",
    "- how many neurons does each patch connect to?\n",
    "  - that’s dependent on our filter depth\n",
    "  - if we have a depth of $K$, we connect each patch of pixels to $K$ neurons in the next layer\n",
    "    - this gives us the height of $K$ in the next layer, as shown below\n",
    "    - in practice, $K$ is a hyperparameter we tune, and most CNNs tend to pick the same starting values\n",
    "    \n",
    "<img src=\"resources/filter_depth.png\"/>\n",
    "\n",
    "- but why connect a single patch to multiple neurons in the next layer; isn’t one neuron good enough?\n",
    "  - multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture\n",
    "  - for example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue\n",
    "  - in that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue\n",
    "\n",
    "\n",
    "- having multiple neurons for a given patch ensures that our CNN can learn to capture whatever characteristics the CNN learns are important\n",
    "- remember that the CNN isn't \"programmed\" to look for certain characteristics\n",
    "  - rather, it learns on its own which characteristics to notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Map Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imagine that you have a 28 x 28 image\n",
    "- you run a 3 by 3 convolution on it, with an input depth of 3, and an output depth of 8\n",
    "- what other dimensions are your output feature maps?\n",
    "  - when you're using same padding with a stride of 1?\n",
    "  - when you're using valid padding with a stride of 1?\n",
    "  - when you're using valid padding with a stride of 2?\n",
    "\n",
    "<img src=\"resources/feature_map_original.png\"/>\n",
    "\n",
    "- if you're using the so called *same padding* in a stride of 1, the output width and height are the same as the input  \n",
    "  - we just add zeros to the input image to make the sizes match\n",
    "  - in this case, width and height are 28 and 28\n",
    "\n",
    "<img src=\"resources/feature_map_same_padding.png\"/>\n",
    "\n",
    "- if you use the so called *valid padding* in a stride of 1, then there is no padding at all\n",
    "  - if you want to fit your little filter on the input image without doing and padding, you're going to have to remove one row and one column of the image on each side\n",
    "  - so in this case you're left with 26 features in each of the maps at the output\n",
    "\n",
    "<img src=\"resources/feature_map_valid_padding.png\"/>\n",
    "\n",
    "- if in addition you use a stride of two, then you only get half as many outputs\n",
    "  - so in this case, 13 in width and 13 in height\n",
    "\n",
    "<img src=\"resources/feature_map_stride_two.png\"/>\n",
    "\n",
    "- in all cases, the output depth isn't changed\n",
    "  - in this case, it stays at 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=qSTv_m-KFk0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that's it, you can build a simple ConvNet with just this\n",
    "- stack up your convolutions which thankfully you don't have to implement yourselves\n",
    "- then use strides to reduce the dimensionality and increase the depth of your network layer after layer\n",
    "- once you have a deep and narrow representation, connect the whole thing to a few regular, fully connected layers, and you're ready to train your classifier\n",
    "\n",
    "<img src=\"resources/convolutions_layers_classifier.png\"/>\n",
    "\n",
    "- you might wonder what happens to training and to chain rule; in particular, when you use shared weights like this\n",
    "  - nothing really happens, the math just works\n",
    "  - you just add up the derivatives for all the possible locations on the image $\\dfrac{\\Delta £}{\\Delta W} = \\dfrac{\\Delta £}{\\Delta W}(X_1) + \\dfrac{\\Delta £}{\\Delta W}(X_2)$\n",
    "\n",
    "<img src=\"resources/chain_rule_shared_weights.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sharing\n",
    "\n",
    "- when we are trying to classify a picture of a cat, we don’t care where in the image a cat is\n",
    "- if it’s in the top left or the bottom right, it’s still a cat in our eyes\n",
    "- we would like our CNNs to also possess this ability known as **translation invariance**\n",
    "- as we saw earlier, the classification of a given patch in an image is determined by the weights and biases corresponding to that patch\n",
    "- if we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way\n",
    "- this is exactly what we do in CNNs\n",
    "  - the weights and biases we learn for a given output layer are shared across all patches in a given input layer\n",
    "  - note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels\n",
    "\n",
    "\n",
    "- there’s an additional benefit to sharing our parameters\n",
    "- if we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair\n",
    "- this does not scale well, especially for higher fidelity images\n",
    "- thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "<img src=\"resources/5v5_grid_with_3v3_padding.png\"/>\n",
    "\n",
    "- let's say we have a $5x5$ grid and a filter of size $3x3$ (as shown above) with a stride of $1$\n",
    "- what's the width and height of the next layer?\n",
    "  - it's $3x3$\n",
    "  - we see that we can fit at most three patches in each direction, giving us a dimension of $3x3$ in our next layer\n",
    "\n",
    "\n",
    "- as we can see, the width and height of each subsequent layer decreases in the above scheme\n",
    "- in an ideal world, we'd be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency\n",
    "- one way to achieve this is to simply add a border of $0$s to our original $5x5$ image\n",
    "  - you can see what this looks like in the below image\n",
    "\n",
    "<img src=\"resources/5v5_grid_with_3v3_padding_and_0s_border.png\"/>\n",
    "\n",
    "- this would expand our original image to a $7x7$\n",
    "- with this, we now see how our next layer's size is again a $5x5$, keeping our dimensionality consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality\n",
    "\n",
    "- from what we've learned so far, how can we calculate the number of neurons of each layer in our CNN\n",
    "- given:\n",
    "  - our input layer has a width of $W$ and a height of $H$\n",
    "  - our convolutional layer has a filter size $F$\n",
    "  - we have a stride of $S$\n",
    "  - a padding of $P$\n",
    "  - the number of filters $K$\n",
    "\n",
    "\n",
    "- the following formula gives us the width of the next layer: $W_{out} = [(W−F+2P)/S] + 1$\n",
    "- the output height would be $H_{out} = [(H-F+2P)/S] + 1$\n",
    "- the output depth would be equal to the number of filters $D_{out} = K$\n",
    "- the output volume would be $W_{out} * H_{out} * D_{out}$\n",
    "\n",
    "\n",
    "- knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Convolution Output Shape\n",
    "\n",
    "- for the next few quizzes we'll test your understanding of the dimensions in CNNs\n",
    "- understanding dimensions will help you make accurate tradeoffs between model size and performance\n",
    "- as you'll see, some parameters have a much bigger impact on model size than others\n",
    "\n",
    "\n",
    "- setup\n",
    "  - H = height, W = width, D = depth\n",
    "  - we have an input of shape $32x32x3$ (HxWxD)\n",
    "  - $20$ filters of shape $8x8x3$ (HxWxD)\n",
    "  - a stride of $2$ for both the height and width (S)\n",
    "  - with padding of size $1$ (P)\n",
    "\n",
    "\n",
    "- recall the formula for calculating the new height or width:\n",
    "```python\n",
    "new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    "new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "```\n",
    "\n",
    "**Q:** What's the shape of the output?\n",
    "<br/>\n",
    "**A:** It's 14x14x20 (HxWxD).\n",
    "\n",
    "\n",
    "- this would correspond to the following code:\n",
    "```python\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "padding = 'SAME'\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "```\n",
    "\n",
    "- note the output shape of `conv` will be [1, 16, 16, 20]\n",
    "- it's 4D to account for batch size, but more importantly, it's not [1, 14, 14, 20]\n",
    "- this is because the padding algorithm TensorFlow uses is not exactly the same as the one above\n",
    "- an alternative algorithm is to switch `padding` from `'SAME'` to `'VALID'` which would result in an output shape of [1, 13, 13, 20]\n",
    "- if you're curious how padding works in TensorFlow, read [this document](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n",
    "  - in summary, TensorFlow uses the following equation for `'SAME'` vs `'VALID'`\n",
    "  - SAME Padding, the output height and width are computed as:\n",
    "    - `out_height` = ceil(float(in_height) / float(strides[1]))\n",
    "    - `out_width` = ceil(float(in_width) / float(strides[2]))\n",
    "  - VALID Padding, the output height and width are computed as:\n",
    "    - `out_height` = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "    - `out_width` = ceil(float(in_width - filter_width + 1) / float(strides[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Number of Parameters\n",
    "\n",
    "- we're now going to calculate the number of parameters of the convolutional layer\n",
    "- the answer from the last quiz will come into play here!\n",
    "- being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses\n",
    "\n",
    "\n",
    "- setup\n",
    "  - H = height, W = width, D = depth\n",
    "  - we have an input of shape $32x32x3$ (HxWxD)\n",
    "  - $20$ filters of shape $8x8x3$ (HxWxD)\n",
    "  - a stride of $2$ for both the height and width (S)\n",
    "  - zero padding of size $1$ (P)\n",
    "\n",
    "\n",
    "- output layer: $14x14x20$ (HxWxD)\n",
    "\n",
    "\n",
    "- hint\n",
    "  - without parameter sharing, each neuron in the output layer must connect to each neuron in the filter\n",
    "  - in addition, each neuron in the output layer must also connect to a single bias neuron\n",
    "  \n",
    "\n",
    "**Q:** How many parameters does the convolutional layer have (without parameter sharing)?\n",
    "<br/>\n",
    "**A:** There are $756560$ total parameters: $(8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560$. That's a HUGE amount!\n",
    "- 8 * 8 * 3 is the number of weights, we add 1 for the bias\n",
    "- remember, each weight is assigned to every single part of the output (14 * 14 * 20)\n",
    "- so we multiply these two numbers together and we get the final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Parameter Sharing\n",
    "\n",
    "- now we'd like you to calculate the number of parameters in the convolutional layer, if every neuron in the output layer shares its parameters with every other neuron in its same channel\n",
    "- this is the number of parameters actually used in a convolution layer (`tf.nn.conv2d()`)\n",
    "\n",
    "\n",
    "- setup\n",
    "  - H = height, W = width, D = depth\n",
    "  - we have an input of shape $32x32x3$ (HxWxD)\n",
    "  - $20$ filters of shape $8x8x3$ (HxWxD)\n",
    "  - a stride of $2$ for both the height and width (S)\n",
    "  - zero padding of size $1$ (P)\n",
    "\n",
    "\n",
    "- output layer: $14x14x20$ (HxWxD)\n",
    "\n",
    "\n",
    "- hint\n",
    "  - with parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel\n",
    "  - so the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer\n",
    "\n",
    "\n",
    "**Q:** How many parameters does the convolution layer have (with parameter sharing)?\n",
    "<br/>\n",
    "**A:** There are $3860$ total parameters: $(8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860$. That's $196$ times fewer parameters!\n",
    "\n",
    "- (8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860\n",
    "  - that's 3840 weights and 20 biases\n",
    "- this should look similar to the answer from the previous quiz; the difference being it's just 20 instead of (14 * 14 * 20)\n",
    "- remember, with weight sharing we use the same filter for an entire depth slice\n",
    "  - because of this we can get rid of 14 * 14 and be left with only 20\n",
    "- with weight sharing there's no longer a connection between every parameter in the filter and every neuron in the output\n",
    "- rather, the same filter is used throughout an entire depth slice\n",
    "  - so, we need to figure out how many depth slices we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let’s look at an example CNN to see how it works in action\n",
    "- the CNN we will look at is trained on ImageNet as described in [this paper](https://arxiv.org/abs/1311.2901) by Zeiler and Fergus\n",
    "- in the images below (from the same paper), we’ll see *what* each layer in this network detects and see *how* each layer detects more and more complex ideas\n",
    "  - the images are from Matthew Zeiler and Rob Fergus' [deep visualization toolbox](https://www.youtube.com/watch?v=ghEmQSxT6tw), which lets us visualize what each layer in a CNN focuses on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1\n",
    "\n",
    "<img src=\"resources/cnn_visualization_layer_1_grid.png\"/>\n",
    "\n",
    "- each image in the above grid represents an example pattern that causes the neurons in the first layer to activate\n",
    "  - in other words, they are patterns that the first layer recognizes\n",
    "  - the top left image shows a -45 degree line, while the middle top square shows a +45 degree line\n",
    "  - these squares are shown below again for reference\n",
    "\n",
    "\n",
    "- let's now see some example images that cause such activations\n",
    "- the below grid of images all activated the -45 degree line\n",
    "- notice how they are all selected despite the fact that they have different colors, gradients, and patterns\n",
    "\n",
    "<img src=\"resources/cnn_visualization_layer_1_-45deg_grid.png\"/>\n",
    "\n",
    "- so, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2\n",
    "\n",
    "<img src=\"resources/cnn_visualization_layer_2_grid.png\"/>\n",
    "\n",
    "- image above is a visualization of the second layer in the CNN\n",
    "- notice how we are picking up more complex ideas like circles (second row, second column), stripes (first row, second column), and rectangles (bottom right)\n",
    "- the gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right\n",
    "\n",
    "\n",
    "- the second layer of the CNN captures complex ideas\n",
    "- **the CNN learns to do this on its own**\n",
    "  - there is no special instruction for the CNN to focus on more complex objects in deeper layers\n",
    "  - that's just how it normally works out when you feed training data into a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3\n",
    "\n",
    "<img src=\"resources/cnn_visualization_layer_3_grid.png\"/>\n",
    "\n",
    "- the third layer picks out complex combinations of features from the second layer\n",
    "- these include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column)\n",
    "\n",
    "\n",
    "- we'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 5\n",
    "\n",
    "<img src=\"resources/cnn_visualization_layer_5_grid.png\"/>\n",
    "\n",
    "- the last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles\n",
    "- the gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Convolution Layer\n",
    "\n",
    "- TensorFlow provides the `tf.nn.conv2d()` and `tf.nn.bias_add()` functions to create your own convolutional layers\n",
    "\n",
    "```python\n",
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "```\n",
    "\n",
    "- the code above uses the `tf.nn.conv2d()` function to compute the convolution with `weight` as the filter and `[1, 2, 2, 1]` for the strides\n",
    "- TensorFlow uses a stride for each `input` dimension, `[batch, input_height, input_width, input_channels]`\n",
    "- we are generally always going to set the stride for `batch` and `input_channels` (i.e. the first and fourth element in the `strides` array) to be $1$\n",
    "\n",
    "\n",
    "- you'll focus on changing `input_height` and `input_width` while setting `batch` and `input_channels` to $1$\n",
    "- the `input_height` and `input_width` strides are for striding the filter over `input`\n",
    "- this example code uses a stride of $2$ with $5x5$ filter over input\n",
    "\n",
    "\n",
    "- the `tf.nn.bias_add()` function adds a 1-d bias to the last dimension in a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore The Design Space\n",
    "\n",
    "- now that you've seen what a simple ConvNet looks like, there are many things that we can do to improve it\n",
    "- we're going to talk about three of them: pooling, 1x1 convolutions, and something a bit more advanced called the inception architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pooling** is a better way to reduce the spatial extent of your feature maps in the convolutional pyramid\n",
    "- until now, we've used striding to shift the filters by a few pixels each time and reduce the feature map size\n",
    "- this is a very aggressive way to downsample an image; it removes a lot of information\n",
    "- what if instead of skipping one in every two convolutions, we still run with a very small stride, say for example, $1$ but then took all the convolutions in a neighborhood and combine them somehow?\n",
    "  - that operation is called *pooling*, and there are a few ways to go about it\n",
    "\n",
    "<img src=\"resources/conv_stride_pooling.png\"/>\n",
    "\n",
    "- the most common is the *max pooling*\n",
    "  - $y = max(X_i)$\n",
    "  - at every point of on the feature map, look at a small neighborhood around that point and compute the maximum of all the responses around it\n",
    "  - there are some advantages to using max pooling\n",
    "    - it doesn't add to your number of parameters, so you don't risk an increase in over fitting\n",
    "    - it simply often yields a more accurate model\n",
    "  - however, since the convolutions that run below run at lower stride, the model then becomes a lot more expensive to compute\n",
    "  - and now, you have even more hyper parameters to worry about; the pooling region size and the pooling stride\n",
    "    - they don't have to be the same\n",
    "\n",
    "\n",
    "- a very typical architecture for a ConvNet is a few layers alternating convolutions and max pooling, followed by a few fully connected layers at the top\n",
    "- the first famous model to use this architecture was Lenet-5 designed by Yan Lecun to do character recogniztion back in 1998\n",
    "- modern convolutional networks, such as AlexNet, which famously won the competitive ImagNnet object recognition challenge in 2012, used the same architecture with a few wrinkles\n",
    "\n",
    "<img src=\"resources/CovNet_architecture_example.png\"/>\n",
    "\n",
    "- another notable form of pooling is *average pooling*\n",
    "  - $y = mean(X_i)$\n",
    "  - instead of taking the max, just take an average over the window of pixels around a specific location\n",
    "  - it's a little bit like providing a blurred low resolution view of the feature map below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=ZjM_XQa5s6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/max_pooling_example.png\"/>\n",
    "\n",
    "- the image above is an example of [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) with a $2x2$ filter and stride of $2$\n",
    "  - the four $2x2$ colors represent each time the filter was applied to find the maximum value\n",
    "  - for example, $[[1, 0], [4, 6]]$ becomes $6$, because $6$ is the maximum value in this set\n",
    "  - similarly, $[[2, 3], [6, 8]]$ becomes $8$\n",
    "\n",
    "\n",
    "- conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements\n",
    "- max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values\n",
    "\n",
    "\n",
    "- TensorFlow provides the `tf.nn.max_pool()` function to apply max pooling to your convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "...\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "```\n",
    "\n",
    "- the `tf.nn.max_pool()` function performs max pooling with the `ksize` parameter as the size of the filter and the `strides` parameter as the length of the stride\n",
    "  - $2x2$ filters with a stride of $2x2$ are common in practice\n",
    "- the `ksize` and `strides` parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor (`[batch, height, width, channels]`)\n",
    "- for both `ksize` and `strides`, the batch and channel dimensions are typically set to $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Pooling Intuition\n",
    "\n",
    "- the next few quizzes will test your understanding of pooling layers\n",
    "\n",
    "**Q:** A pooling layer is generally used to ...\n",
    "<br/>\n",
    "**A:** The correct answer is *decrease the size of the output* and *prevent overfitting*. Reducing overfitting is a consequence of the reducing the output size, which in turn, reduces the number of parameters in future layers.\n",
    "\n",
    "- recently, pooling layers have fallen out of favor\n",
    "- some reasons are:\n",
    "  - recent datasets are so big and complex we're more concerned about underfitting\n",
    "  - *dropout* is a much better regularizer\n",
    "  - pooling results in a loss of information (think about the max pooling operation as an example; we only keep the largest of $n$ numbers, thereby disregarding $n-1$ numbers completely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Pooling Mechanics\n",
    "\n",
    "- setup\n",
    "  - H = height, W = width, D = depth\n",
    "  - we have an input of shape $4x4x5$ (HxWxD)\n",
    "  - filter of shape $2x2$ (HxW)\n",
    "  - a stride of $2$ for both the height and width (S)\n",
    "\n",
    "\n",
    "- recall the formula for calculating the new height or width:\n",
    "  - `new_height = (input_height - filter_height)/S + 1`\n",
    "  - `new_width = (input_width - filter_width)/S + 1`\n",
    "\n",
    "\n",
    "- note\n",
    "  - for a pooling layer the output depth is the same as the input depth\n",
    "  - additionally, the pooling operation is applied individually for each depth slice\n",
    "\n",
    "\n",
    "- the image below gives an example of how a max pooling layer works\n",
    "- in this case, the max pooling filter has a shape of $2x2$\n",
    "- as the max pooling filter slides across the input layer, the filter will output the maximum value of the $2x2$ square\n",
    "\n",
    "<img src=\"resources/pooling_mechanics_quiz.png\"/>\n",
    "\n",
    "**Q:** What's the shape of the output? Format is HxWxD.\n",
    "<br/>\n",
    "**A:** It's $2x2x5$. It's calculated using the formulas above. The depth stays the same.\n",
    "\n",
    "- here's the corresponding code:\n",
    "```python\n",
    "input = tf.placeholder(tf.float32, (None, 4, 4, 5))\n",
    "filter_shape = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "padding = 'VALID'\n",
    "pool = tf.nn.max_pool(input, filter_shape, strides, padding)\n",
    "```\n",
    "- the output shape of `pool` will be [1, 2, 2, 5], even if `padding` is changed to `'SAME'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Pooling Practice\n",
    "\n",
    "- now let's practice doing some pooling operations manually\n",
    "\n",
    "**Q:** What's the result of a max pooling operation on the input:\n",
    "```python\n",
    "[[[0, 1, 0.5, 10],\n",
    "   [2, 2.5, 1, -8],\n",
    "   [4, 0, 5, 6],\n",
    "   [15, 1, 2, 3]]]\n",
    "```\n",
    "Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1.\n",
    "The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4.\n",
    "Work from the top left to the bottom right.\n",
    "<br/>\n",
    "<br/>\n",
    "**A:** It's $2.5,10,15,6$. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time.\n",
    "```python\n",
    "max(0, 1, 2, 2.5) = 2.5\n",
    "max(0.5, 10, 1, -8) = 10\n",
    "max(4, 0, 15, 1) = 15\n",
    "max(5, 6, 2, 3) = 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Average Pooling\n",
    "\n",
    "**Q:** What's the result of a average (or mean) pooling?\n",
    "```python\n",
    "[[[0, 1, 0.5, 10],\n",
    "   [2, 2.5, 1, -8],\n",
    "   [4, 0, 5, 6],\n",
    "   [15, 1, 2, 3]]]\n",
    "```\n",
    "Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1.\n",
    "The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4.\n",
    "Answer to 3 decimal places. Work from the top left to the bottom right\n",
    "<br/>\n",
    "<br/>\n",
    "**A:** It's $1.375,0.875,5,4$. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time.\n",
    "```python\n",
    "mean(0, 1, 2, 2.5) = 1.375\n",
    "mean(0.5, 10, 1, -8) = 0.875\n",
    "mean(4, 0, 15, 1) = 5\n",
    "mean(5, 6, 2, 3) = 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to introduce you to another idea; it’s the idea of *1 x 1 convolutions*\n",
    "- you might wonder, why one would ever want to use 1 x 1 convolutions?\n",
    "  - they’re not really looking at a patch of the image just that one pixel\n",
    "\n",
    "<img src=\"resources/1x1_convolutions.png\"/>\n",
    "\n",
    "- look at the classic convolution setting, it’s basically a small classifier for a patch of the image but it's only a linear classifier\n",
    "- but if you add a 1 x 1 convolution in the middle, suddenly you have a mini neural network running over the patch instead of a linear classifier\n",
    "- interspersing your convolutions with 1 x 1 convolutions is a very inexpensive way to make your models deeper and have more parameters, without completely changing their structure\n",
    "- they're also very cheap because if you go through their math, they're not really convolutions at all, they're really just matrix multiplies and they have relatively few parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I mentioned all of these, average pulling and 1 x 1 convolutions because I want to talk about the general strategy that has been very successful at creating ConvNets that are both smaller and better than ConvNets that simply use a pyramid of convolutions\n",
    "  - it's called an *inception module*\n",
    "\n",
    "\n",
    "- the idea is that at each layer of your ConvNet, you can make a choice to have a pooling operation; have a convolution...\n",
    "- and then you need to decide, it is a 1x1 or a 3x3 or a 5x5\n",
    "- all of these are actually beneficial to the modeling power of your network, so why choose?\n",
    "  - let's use them all\n",
    "\n",
    "\n",
    "- here is what an inception module looks like\n",
    "  - instead of having a single convolution, you have a composition of average pooling followed by 1 x 1, then a 1 x 1 convolution then a 1 x 1 followed by 3 x 3, then a 1x1 followed by a 5x5\n",
    "  - and at the top, you simply concatenate the output of each of them\n",
    "\n",
    "<img src=\"resources/inception_module.png\"/>\n",
    "\n",
    "- it looks complicated, but what's interesting is that you can chose these parameters in such a way that the total number of parameters in your model is very small\n",
    "- yet the model performs better than if you had a simple convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow\n",
    "- the structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers\n",
    "- the code you'll be looking at is similar to what you saw in the segment on Deep Neural Network in TensorFlow in the previous lesson, except we restructured the architecture of this network as a CNN\n",
    "\n",
    "\n",
    "- just like in that segment, here you'll study the line-by-line breakdown of the code\n",
    "- if you want, you can even [download the code](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61ca1_cnn/cnn.zip) and run it yourself\n",
    "- thanks to [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples) for providing the original TensorFlow model on which this segment is based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- you've seen this section of code from previous lessons\n",
    "- here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data\n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "\n",
    "```python\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "<img src=\"resources/convolution_schematic.gif\"/>\n",
    "\n",
    "- the above is an example of a [convolution](https://en.wikipedia.org/wiki/Convolution) with a $3x3$ filter and a stride of $1$ being applied to data with a range of $0$ to $1$\n",
    "- the convolution for each $3x3$ section is calculated against the weight, $[[1, 0, 1], [0, 1, 0], [1, 0, 1]]$, then a bias is added to create the convolved feature on the right\n",
    "- in this case, the bias is $0$\n",
    "\n",
    "\n",
    "- in TensorFlow, this is all done using `tf.nn.conv2d()` and `tf.nn.bias_add()`\n",
    "\n",
    "```python\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "```\n",
    "- the `tf.nn.conv2d()` function computes the convolution against weight $W$ as shown above\n",
    "\n",
    "\n",
    "- in TensorFlow, `strides` is an array of $4$ elements\n",
    "  - the first element in this array indicates the stride for batch\n",
    "  - last element indicates stride for features\n",
    "  - the middle two elements are the strides for height and width respectively  \n",
    "- it's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them\n",
    "- you can always set the first and last element to $1$ in `strides` in order to use all batches and features\n",
    "- I've mentioned stride as one number because you usually have a square stride where `height = width`\n",
    "- when someone says they are using a stride of $3$, they usually mean `tf.nn.conv2d(x, W, strides=[1, 3, 3, 1])` \n",
    "\n",
    "\n",
    "- to make life easier, the code is using `tf.nn.bias_add()` to add the bias\n",
    "- using `tf.add()` doesn't work when the tensors aren't the same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling\n",
    "\n",
    "<img src=\"resources/maxpool.jpeg\"/>\n",
    "\n",
    "- the above is an example of [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) with a $2x2$ filter and stride of $2$\n",
    "  - the left square is the input and the right square is the output\n",
    "  - the four $2x2$ colors in input represents each time the filter was applied to create the max on the right side\n",
    "  - for example, $[[1, 1], [5, 6]]$ becomes $6$ and $[[3, 2], [1, 2]]$ becomes $3$\n",
    "\n",
    "```python\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "```\n",
    "- the `tf.nn.max_pool()` function does exactly what you would expect, it performs max pooling with the ksize parameter as the size of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- in the code below, we're creating $ $layers alternating between convolutions and max pooling followed by a fully connected and output layer\n",
    "- the transformation of each layer to new dimensions are shown in the comments\n",
    "- for example, the first layer shapes the images from $28x28x1$ to $28x28x32$ in the convolution step\n",
    "- then next step applies max pooling, turning each sample into $14x14x32$\n",
    "- all the layers are applied from `conv1` to output, producing $10$ class predictions\n",
    "\n",
    "```python\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "- now let's run it!\n",
    "\n",
    "```python\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Convolutional Layer Workspaces\n",
    "\n",
    "- let's now apply what we've learned to build real CNNs in TensorFlow\n",
    "- in the below exercise, you'll be asked to set up the dimensions of the Convolution filters, the weights, the biases\n",
    "  - this is in many ways the trickiest part to using CNNs in TensorFlow\n",
    "- once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straight forward\n",
    "\n",
    "\n",
    "- review\n",
    "  - you should go over the TensorFlow documentation for [2D convolutions](https://www.tensorflow.org/guide#Convolution)\n",
    "  - most of the documentation is straightforward, except perhaps the `padding` argument\n",
    "    - the padding might differ depending on whether you pass `'VALID'` or `'SAME'`\n",
    "- here are a few more things worth reviewing:\n",
    "  - introduction to TensorFlow -> [TensorFlow Variables](https://www.tensorflow.org/guide/variable)\n",
    "  - how to determine the dimensions of the output based on the input size and the filter size (shown below)\n",
    "    - you'll use this to determine what the size of your filter should be\n",
    "```python\n",
    "new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    "new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "```\n",
    "\n",
    "\n",
    "- instructions\n",
    "  - finish off each `TODO` in the `conv2d function`\n",
    "  - setup the `strides`, `padding` and filter weight/bias (`F_w` and `F_b`) such that the output shape is `(1, 2, 2, 3)`\n",
    "    - note that all of these except `strides` should be TensorFlow variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(1, 2, 2, 3) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine\\n\\nimport tensorflow as tf\\nimport numpy as np\\n\\n\\\"\\\"\\\"\\nSetup the strides, padding and filter weight/bias such that\\nthe output shape is (1, 2, 2, 3).\\n\\\"\\\"\\\"\\n# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\\n# (1, 4, 4, 1)\\nx = np.array(\\n    [[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32\\n).reshape((1, 4, 4, 1))\\nX = tf.constant(x)\\n\\n\\ndef conv2d(input_array):\\n    # Filter (weights and bias)\\n    # The shape of the filter weight is (height, width, input_depth, output_depth)\\n    # The shape of the filter bias is (output_depth,)\\n\\n    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\\n    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\\n    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\\n    F_b = tf.Variable(tf.zeros(3))\\n\\n    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\\n    strides = [1, 2, 2, 1]\\n\\n    # TODO: set the padding, either 'VALID' or 'SAME'.\\n    padding = \\\"VALID\\\"\\n\\n    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\\n    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\\n    return tf.nn.conv2d(input_array, F_W, strides, padding) + F_b\\n\\n\\noutput = conv2d(X)\\noutput\";\n",
       "                var nbb_formatted_code = \"# NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine\\n\\nimport tensorflow as tf\\nimport numpy as np\\n\\n\\\"\\\"\\\"\\nSetup the strides, padding and filter weight/bias such that\\nthe output shape is (1, 2, 2, 3).\\n\\\"\\\"\\\"\\n# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\\n# (1, 4, 4, 1)\\nx = np.array(\\n    [[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32\\n).reshape((1, 4, 4, 1))\\nX = tf.constant(x)\\n\\n\\ndef conv2d(input_array):\\n    # Filter (weights and bias)\\n    # The shape of the filter weight is (height, width, input_depth, output_depth)\\n    # The shape of the filter bias is (output_depth,)\\n\\n    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\\n    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\\n    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\\n    F_b = tf.Variable(tf.zeros(3))\\n\\n    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\\n    strides = [1, 2, 2, 1]\\n\\n    # TODO: set the padding, either 'VALID' or 'SAME'.\\n    padding = \\\"VALID\\\"\\n\\n    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\\n    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\\n    return tf.nn.conv2d(input_array, F_W, strides, padding) + F_b\\n\\n\\noutput = conv2d(X)\\noutput\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Setup the strides, padding and filter weight/bias such that\n",
    "the output shape is (1, 2, 2, 3).\n",
    "\"\"\"\n",
    "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array(\n",
    "    [[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32\n",
    ").reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "\n",
    "def conv2d(input_array):\n",
    "    # Filter (weights and bias)\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "\n",
    "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
    "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n",
    "    F_b = tf.Variable(tf.zeros(3))\n",
    "\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = \"VALID\"\n",
    "\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
    "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
    "    return tf.nn.conv2d(input_array, F_W, strides, padding) + F_b\n",
    "\n",
    "\n",
    "output = conv2d(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to transform the input shape `(1, 4, 4, 1)` to `(1, 2, 2, 3)`\n",
    "- I choose 'VALID' for the padding algorithm\n",
    "  - I find it simpler to understand and it achieves the result I'm looking for\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "- plugging in the values:\n",
    "```python\n",
    "out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "```\n",
    "\n",
    "- in order to change the depth from $1$ to $3$, I have to set the output depth of my filter appropriately:\n",
    "```python\n",
    "F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth)\n",
    "F_b = tf.Variable(tf.zeros(3)) # (output_depth)\n",
    "```\n",
    "- the input has a depth of $1$, so I set that as the `input_depth` of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pooling Layers in TensorFlow\n",
    "\n",
    "- in the below exercise, you'll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding\n",
    "- you should go over the TensorFlow documentation for `tf.nn.max_pool()`\n",
    "- padding works the same as it does for a convolution\n",
    "\n",
    "\n",
    "- instructions\n",
    "  - finish off each *TODO* in the `maxpool` function\n",
    "  - setup the `strides`, `padding` and `ksize` such that the output shape after pooling is `(1, 2, 2, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(1, 2, 2, 1) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine\\n\\n\\\"\\\"\\\"\\nSet the values to `strides` and `ksize` such that\\nthe output shape after pooling is (1, 2, 2, 1).\\n\\\"\\\"\\\"\\nimport tensorflow as tf\\nimport numpy as np\\n\\n# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\\n# (1, 4, 4, 1)\\nx = np.array(\\n    [[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32\\n).reshape((1, 4, 4, 1))\\nX = tf.constant(x)\\n\\n\\ndef maxpool(input):\\n    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\\n    ksize = [1, 2, 2, 1]\\n    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\\n    strides = [1, 2, 2, 1]\\n    # TODO: set the padding, either 'VALID' or 'SAME'.\\n    padding = \\\"VALID\\\"\\n    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\\n    return tf.nn.max_pool(input, ksize, strides, padding)\\n\\n\\nout = maxpool(X)\\nout\";\n",
       "                var nbb_formatted_code = \"# NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine\\n\\n\\\"\\\"\\\"\\nSet the values to `strides` and `ksize` such that\\nthe output shape after pooling is (1, 2, 2, 1).\\n\\\"\\\"\\\"\\nimport tensorflow as tf\\nimport numpy as np\\n\\n# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\\n# (1, 4, 4, 1)\\nx = np.array(\\n    [[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32\\n).reshape((1, 4, 4, 1))\\nX = tf.constant(x)\\n\\n\\ndef maxpool(input):\\n    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\\n    ksize = [1, 2, 2, 1]\\n    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\\n    strides = [1, 2, 2, 1]\\n    # TODO: set the padding, either 'VALID' or 'SAME'.\\n    padding = \\\"VALID\\\"\\n    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\\n    return tf.nn.max_pool(input, ksize, strides, padding)\\n\\n\\nout = maxpool(X)\\nout\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine\n",
    "\n",
    "\"\"\"\n",
    "Set the values to `strides` and `ksize` such that\n",
    "the output shape after pooling is (1, 2, 2, 1).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array(\n",
    "    [[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32\n",
    ").reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "\n",
    "def maxpool(input):\n",
    "    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = \"VALID\"\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "\n",
    "\n",
    "out = maxpool(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to transform the input shape `(1, 4, 4, 1)` to `(1, 2, 2, 1)`\n",
    "- I choose 'VALID' for the padding algorithm\n",
    "- I find it simpler to understand and it achieves the result I'm looking for\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "- plugging in the values:\n",
    "```python\n",
    "out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "```\n",
    "- the depth doesn't change during a pooling operation so I don't have to worry about that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs - Additional Resources\n",
    "\n",
    "- there are many wonderful free resources that allow you to go into more depth around Convolutional Neural Networks\n",
    "- in this course, our goal is to give you just enough intuition to start applying this concept on real world problems so you have enough of an exposure to explore more on your own\n",
    "- we strongly encourage you to explore some of these resources more to reinforce your intuition and explore different ideas\n",
    "\n",
    "\n",
    "- these are the resources we recommend in particular:\n",
    "  - Andrej Karpathy's [CS231n Stanford course](https://cs231n.github.io/) on Convolutional Neural Networks\n",
    "  - Michael Nielsen's [free book](http://neuralnetworksanddeeplearning.com/) on Deep Learning\n",
    "  - Goodfellow, Bengio, and Courville's more advanced [free book](https://www.deeplearningbook.org/) on Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
