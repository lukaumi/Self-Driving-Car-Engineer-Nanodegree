{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- localization is what allows an autonomous car to know precisely where it is\n",
    "- without localization, it would be impossible for a self-driving car to drive safely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conceptionally, localization is pretty straightforward\n",
    "- a robot takes information about its environment and compares that to information it already knows about the real world\n",
    "- humans do something similar\n",
    "  - imagine you were suddenly kidnapped and blindfolded--you were stuffed into a car that drove for hours--you would have no idea where in the world you were--then the blindfold were removed and you saw like this (Eiffel Tower)\n",
    "  - now, what would you say if you asked, where are you?\n",
    "    - if you recognized this as the Eiffel Tower, then you will probably say something like Paris or France\n",
    "    - this may not seem very impressive, but it's actually remarkable\n",
    "    - before the blindfold was removed, you had zero understanding of where you were in the world\n",
    "    - you could have made a guess, but you would have had no idea if you were right\n",
    "    - but after being shown a tiny amount of data, a single image, you reduce that uncertainty to a few kilometer radius\n",
    "\n",
    "\n",
    "- this is the main intuition behind localization\n",
    "- a robot gathers information about its current environment and compares that to a known map to understand where it is in the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localizing a Self Driving Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- assume there is a car that is totally lost, which means you, as a driver or as a car, have no clue where you are\n",
    "- now assume that you have a global map of the environment\n",
    "- generally speaking, localization answers a question, where is our car in a given map with a high accuracy?\n",
    "  - a high accuracy means between 3 and 10 centimeters\n",
    "\n",
    "\n",
    "- in a traditional way, we use global navigation satellite systems to find the car, with respect to the map\n",
    "  - but GPS is not precise enough\n",
    "  - most of the time, GPS has an accuracy of the width of a lane-- about one to three meters\n",
    "  - but sometimes it can be as broad as 10 to 50 meters\n",
    "  - clearly this is not reliable enough for a self-driving car so you can't trust GPS and you have to find another technique to localize yourself inside a given map\n",
    "\n",
    "\n",
    "- it is common practice to use the onboard sensor data, along with our global map, to solve the localization issue\n",
    "  - with the onboard sensors it is possible to measure distances to static obstacles, like trees, poles, or walls\n",
    "  - we measure these distances, and the bearing of these static objects in the local coordinate system of our car\n",
    "\n",
    "\n",
    "- when you're lucky, the same obstacles that were observed by the onboard sensors are also part of the map\n",
    "- and, of course, the map has its own global coordinate system\n",
    "- to estimate where the car is in the map, you have to match the observations with the map information\n",
    "  - when you do it correctly, this results in a transformation between both coordinate systems--the local car coordinate system and the global coordinate system of the map\n",
    "  - this transformation should be as accurate as possible-- let's say within a range of 10 centimeters or less\n",
    "  - if you are able to estimate this transformation, you solve the localization issue\n",
    "\n",
    " <img src=\"resources/transform_onboard_global.png\"/>\n",
    "\n",
    "- let's summarize\n",
    "  - first, localization answers the question of where the car is in a given map within an accuracy of 10 centimeters or less\n",
    "  - second, onboard sensors are used to estimate the transformation between local measurements and a given map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Much of the content in this lesson assumes you have a good map first. Without it, the techniques here either won't work or won't work very well. There is also another version of localization called SLAM, or Simultaneous Localization and Mapping, that does not need a good map prior to beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the very first problem I'm trying to solve is called localization\n",
    "- it involves a robot that's lost in space--it could be a car, it could be a mobile robot\n",
    "  - here is the environment, and the poor robot has no clue where it is\n",
    "- similarly, we might have a car driving on a highway, and this car would like to know where it is\n",
    "  - is it inside the lane or is it crossing lane markers?\n",
    "\n",
    "\n",
    "- the traditional way to solve this problem is by using satellites\n",
    "  - these satellites emit signals that the car can perceive\n",
    "    - that's known as GPS, short for \"global positioning system\" and it's what you have in your dashboard if you have a car with GPS that shows you the maps and shows you where you are\n",
    "    - unfortunately, the problem with GPS is its really not very accurate\n",
    "  - it's really common for a car to believe to be at one position but it has 2 meters all the way up to 10 meters of error\n",
    "    - if you try to stay in the lane with 10 meters of error, you're far off, and if you're driving, you crash\n",
    "\n",
    "\n",
    "- for our self-driving cars, to be able to stay in lanes using localization, we need something like $2$ - $10$ centimeters of error\n",
    "  - then we can drive with GPS in lanes\n",
    "- the question is, how can we know where were are with $10 cm$ accuracy?\n",
    "  - that's the localization question\n",
    "\n",
    "\n",
    "- in the Google self-driving car, localization plays a key role\n",
    "- we record images of the road surface and then use the techniques I'm just about to teach you to find out exactly where the robot is\n",
    "  - it does so within a few centimeters of accuracy, and that makes it possible to stay inside the lane even if the lane markers are missing\n",
    "\n",
    " <img src=\"resources/google_sdc_localization.png\"/>\n",
    "\n",
    "- localization has a lot of math, but before I dive into mathematical detail, I want to give you an intuition for the basic principles\n",
    "- I want to tell you the story of how we will localize this, and then we can go through the math together so you can understand it\n",
    "- I also want to let you program your own localizer so you can program a self-driving car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's move into our first programming exercise, and let's program together the very first version of robot localization\n",
    "- here's a bit of program code--an empty list, and what I'd like you to program is a world with 5 different cells or places where each cell has the same probability that the robot might be in that cell\n",
    "  - so probabilities add up to 1\n",
    "\n",
    "\n",
    "- here's a simple quiz: for the cells $x_1$ all the way to $x_5$, what is the probability of any of those $x$'s?\n",
    "  - it's $0.2$ because $1/5 = 0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now in our Python interface, I'd like to take this code over here, which assigns to `p` an empty list and modify it into code where `p` becomes a uniform distribution over $5$ grid cells as expressed in a vector of $5$ probabilities\n",
    "\n",
    "\n",
    "- here's an easy solution; you just initialize the vector with five $0.2$s\n",
    "- or like a loop below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.2, 0.2, 0.2]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"p = []\\nn = 5\\nfor i in range(n):\\n    p.append(1.0 / n)\\nprint(p)\";\n",
       "                var nbb_formatted_code = \"p = []\\nn = 5\\nfor i in range(n):\\n    p.append(1.0 / n)\\nprint(p)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = []\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    p.append(1.0 / n)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability After Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's look at the measurement of this robot in its world with $5$ different grid cells--$x_1$ through $x_5$\n",
    "- let's assume two of those cells are colored red ($x_2$ and $x_3$) whereas the other three are green\n",
    "- as before, we assign uniform probability to each cell of $0.2$, and our robot is now allowed to sense\n",
    "- what it sees is a red color\n",
    "- how will this affect my belief over different places?\n",
    "  - obviously, the one's for $x_2$ and $x_3$ should go up, and the ones for $x_1$, $x_4$, and $x_5$ should go down\n",
    "\n",
    "\n",
    "- we're going incorporate this measurement into our belief with a very simple rule--a product\n",
    "  - any cell where the color is correct--any of the red cells-- we multiply it with a relatively large number--say, $0.6$\n",
    "    - that feels small, but as we will see later, it is actually a large number\n",
    "  - whereas all the green cells will be multiplied with $0.2$\n",
    "- if we look at the ratio of those, then it seems about 3 times as likely to be in a red cell than it is to be in a green cell, because $0.6$ is $3$ times larger than $0.2$\n",
    "- the answer is obviously for the red cells we get a $0.12$ whereas for the green cells we get a $0.04$, which is the product of $0.2 \\times 0.6$ versus $0.2 \\times 0.2$\n",
    "  - but notice that our probabilities don't add up to $1$--it adds up to $0.36$\n",
    "  - we'll have to fix that; let's learn about renormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to turn this back into a probability distribution, we will now divide each of these numbers by $0.36$\n",
    "- put differently, we normalize\n",
    "\n",
    "\n",
    "- so $0.12$ divided by $0.36$ is the same as $12$ divided by $36$ is the same as $1/3$ or $0.333$.\n",
    "- and $0.04$ divided by $0.36$ is the same as $4$ divided by $36$, that is $1/9$\n",
    "- if you look at these numbers, $1/3, 1/3, 1/3, 1/9, 1/9$, they give exactly $1$\n",
    "\n",
    "\n",
    "- so this is a probability distribution, which is often written in the following way: $p(X_i | Z)$\n",
    "  - the probability of each cell, $i$ where $i$ could range from $1-5$, after we've seen our measurement $Z$\n",
    "- the probabilist would also call it posterior distribution of place $x_i$ given measurement $Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pHit and pMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here's our distribution again and here's our factor for getting the color right or for getting it wrong ($0.2$), and let's first start with a non-normalized version\n",
    "- write a piece of code that outputs `p` after multiplying with `pHit` and `pMiss`\n",
    "- also, get the sum of all the `p's`\n",
    "\n",
    "\n",
    "- one way to do this is to go explicitly through all these 5 different cases from 0 to 4 and multiply in manually the miss or hit case\n",
    "  - this is not particularly elegant, but it does the job\n",
    "- we sum elements with `sum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04000000000000001, 0.12, 0.12, 0.04000000000000001, 0.04000000000000001]\n",
      "0.3600000000000001\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Write code that outputs p after multiplying each entry by pHit or pMiss at the appropriate places.\\n# Remember that #the red cells 1 and 2 are hits and the other green cells are misses.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\npHit = 0.6\\npMiss = 0.2\\n\\n# Enter code here\\np[0] *= pMiss\\np[1] *= pHit\\np[2] *= pHit\\np[3] *= pMiss\\np[4] *= pMiss\\n\\np_sum = sum(p)\\n\\nprint(p)\\nprint(p_sum)\";\n",
       "                var nbb_formatted_code = \"# Write code that outputs p after multiplying each entry by pHit or pMiss at the appropriate places.\\n# Remember that #the red cells 1 and 2 are hits and the other green cells are misses.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\npHit = 0.6\\npMiss = 0.2\\n\\n# Enter code here\\np[0] *= pMiss\\np[1] *= pHit\\np[2] *= pHit\\np[3] *= pMiss\\np[4] *= pMiss\\n\\np_sum = sum(p)\\n\\nprint(p)\\nprint(p_sum)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write code that outputs p after multiplying each entry by pHit or pMiss at the appropriate places.\n",
    "# Remember that #the red cells 1 and 2 are hits and the other green cells are misses.\n",
    "\n",
    "p = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "\n",
    "# Enter code here\n",
    "p[0] *= pMiss\n",
    "p[1] *= pHit\n",
    "p[2] *= pHit\n",
    "p[3] *= pMiss\n",
    "p[4] *= pMiss\n",
    "\n",
    "p_sum = sum(p)\n",
    "\n",
    "print(p)\n",
    "print(p_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sense Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to make this a little bit more beautiful\n",
    "- I will introduce a variable called `world`, and for each of the 5 grid cells, world specifies the color of the cell--green, red, red, green, green\n",
    "- further, I define the measurement `Z` to be red\n",
    "\n",
    " \n",
    "- can you define a function, called `sense`, which is the measurement update, which takes as input the initial distribution `p` and the measurement `Z` and all the other global variables and outputs a normalized distribution called `q` in which $q$ reflects the non-normalized product of our input probability, which will be $0.2$ and so on, and the corresponding `pHit` or `pMiss` in accordance to whether these colors over here agree or disagree?\n",
    "- when I call `sense(p, Z)`, I expect to get the vector as output as before, but now in the form of a function\n",
    "  - the reason I'd like to have a function here is because later on as we build our localizer we will apply this to every single measurement over and over again\n",
    "  - this function should really respond to any arbitrary `p` and arbitrary `Z`, either red or green, and give me the non-normalized `q`, which gives me the vector $0.04$ or $0.12$ and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04000000000000001, 0.12, 0.12, 0.04000000000000001, 0.04000000000000001]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Modify the code below so that the function sense, which takes p and Z as inputs, will output the NON-normalized\\n# probability distribution, q, after multiplying the entries in p by pHit or pMiss according to the color in the\\n# corresponding cell in world.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nZ = \\\"red\\\"\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    # ADD YOUR CODE HERE\\n    q = []\\n    for i in range(len(p)):\\n        if world[i] == Z:\\n            q.append(p[i] * pHit)\\n        else:\\n            q.append(p[i] * pMiss)\\n    return q\\n\\n\\nprint(sense(p, Z))\";\n",
       "                var nbb_formatted_code = \"# Modify the code below so that the function sense, which takes p and Z as inputs, will output the NON-normalized\\n# probability distribution, q, after multiplying the entries in p by pHit or pMiss according to the color in the\\n# corresponding cell in world.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nZ = \\\"red\\\"\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    # ADD YOUR CODE HERE\\n    q = []\\n    for i in range(len(p)):\\n        if world[i] == Z:\\n            q.append(p[i] * pHit)\\n        else:\\n            q.append(p[i] * pMiss)\\n    return q\\n\\n\\nprint(sense(p, Z))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify the code below so that the function sense, which takes p and Z as inputs, will output the NON-normalized\n",
    "# probability distribution, q, after multiplying the entries in p by pHit or pMiss according to the color in the\n",
    "# corresponding cell in world.\n",
    "\n",
    "p = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "Z = \"red\"\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    # ADD YOUR CODE HERE\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        if world[i] == Z:\n",
    "            q.append(p[i] * pHit)\n",
    "        else:\n",
    "            q.append(p[i] * pMiss)\n",
    "    return q\n",
    "\n",
    "\n",
    "print(sense(p, Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Sense Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's take that same piece of code and modify it to give me a valid probability distribution\n",
    "  -  so it normalizes the output of the function `sense`o it adds up to 1\n",
    "\n",
    "\n",
    "- with this, we implement the absolute key function of localization, which is called the **measurement update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1111111111111111, 0.3333333333333332, 0.3333333333333332, 0.1111111111111111, 0.1111111111111111]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Modify your code so that it normalizes the output for the function sense.\\n# This means that the entries in q should sum to one.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nZ = \\\"red\\\"\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    # ADD YOUR CODE HERE\\n    q = []\\n    sum_p = sum(p)\\n    for i in range(len(p)):\\n        if world[i] == Z:\\n            q.append(p[i] * pHit)\\n        else:\\n            q.append(p[i] * pMiss)\\n    sum_q = sum(q)\\n    q = [i / sum_q for i in q]\\n    return q\\n\\n\\nprint(sense(p, Z))\";\n",
       "                var nbb_formatted_code = \"# Modify your code so that it normalizes the output for the function sense.\\n# This means that the entries in q should sum to one.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nZ = \\\"red\\\"\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    # ADD YOUR CODE HERE\\n    q = []\\n    sum_p = sum(p)\\n    for i in range(len(p)):\\n        if world[i] == Z:\\n            q.append(p[i] * pHit)\\n        else:\\n            q.append(p[i] * pMiss)\\n    sum_q = sum(q)\\n    q = [i / sum_q for i in q]\\n    return q\\n\\n\\nprint(sense(p, Z))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify your code so that it normalizes the output for the function sense.\n",
    "# This means that the entries in q should sum to one.\n",
    "\n",
    "p = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "Z = \"red\"\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    # ADD YOUR CODE HERE\n",
    "    q = []\n",
    "    sum_p = sum(p)\n",
    "    for i in range(len(p)):\n",
    "        if world[i] == Z:\n",
    "            q.append(p[i] * pHit)\n",
    "        else:\n",
    "            q.append(p[i] * pMiss)\n",
    "    sum_q = sum(q)\n",
    "    q = [i / sum_q for i in q]\n",
    "    return q\n",
    "\n",
    "\n",
    "print(sense(p, Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sense Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's just go back to our example and see what an amazing thing you've just programmed\n",
    "- we had a uniform distribution over places--each place had a probability of $0.2$\n",
    "- then you wrote a piece of code that used the measurement to turn this prior into a posterior, in which the probability of the two red cells was a factor of 3 larger than the posterior of the green cells\n",
    "\n",
    "<img src=\"resources/sense_function_prior_posterior.png\"/>\n",
    "\n",
    "- you've done exactly what I gave you intuitively in the beginning as the secret of localization\n",
    "  - you manipulated a probability distribution over places into a new one by incorporating the measurement\n",
    "\n",
    "\n",
    "- in fact, let's go back to our code and test in your code whether we get a good result when we replace our measurement `red` by `green`\n",
    "  - please type green into your measurement variable and rerun your code to see if you get the correct result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'd like you to modify this code a little bit more in a way that we have multiple measurements\n",
    "- instead of `Z`, we're going to make a measurement vector called `measurements`\n",
    "  - we're going to assume that we're going to first sense red and then green\n",
    "\n",
    "\n",
    "- can you modify the code that so it updates the probability twice and gives me the posterior after both of these measurements are incorporated?\n",
    "  - in fact, can you modify it in a way that any sequence of measurements of any length can be processed?\n",
    "  - do not modify the sense function--add code so that `p` is the correct probability after making the two measurements\n",
    "    - make sure your code works for measurement lists of arbitrary length\n",
    "\n",
    "\n",
    "- the modification is simple; we will call the procedure `sense` multiple times, in fact, as often as we have measurements\n",
    "  - we grab the $k$th measurement element and apply it to the current belief then recursively update that belief into itself\n",
    "  - in this case, we run it twice\n",
    "- for this specific example, we get back the uniform distribution--these are all $0.2$s approximately\n",
    "  - the reason is we up-multiplied each field once for the $0.6$ and down-multiplied for the $0.2$\n",
    "  - these effects were in total the same for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20000000000000004, 0.19999999999999996, 0.19999999999999996, 0.20000000000000004, 0.20000000000000004]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Modify the code so that it updates the probability twice\\n# and gives the posterior distribution after bothmeasurements are incorporated.\\n# Make sure that your code allows for any sequence of measurement of any length.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    # ADD YOUR CODE HERE\\n    q = []\\n    sum_p = sum(p)\\n    for i in range(len(p)):\\n        if world[i] == Z:\\n            q.append(p[i] * pHit)\\n        else:\\n            q.append(p[i] * pMiss)\\n    sum_q = sum(q)\\n    q = [i / sum_q for i in q]\\n    return q\\n\\n\\n# ADD YOUR CODE HERE\\nfor i in range(len(measurements)):\\n    p = sense(p, measurements[i])\\n\\nprint(p)\";\n",
       "                var nbb_formatted_code = \"# Modify the code so that it updates the probability twice\\n# and gives the posterior distribution after bothmeasurements are incorporated.\\n# Make sure that your code allows for any sequence of measurement of any length.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    # ADD YOUR CODE HERE\\n    q = []\\n    sum_p = sum(p)\\n    for i in range(len(p)):\\n        if world[i] == Z:\\n            q.append(p[i] * pHit)\\n        else:\\n            q.append(p[i] * pMiss)\\n    sum_q = sum(q)\\n    q = [i / sum_q for i in q]\\n    return q\\n\\n\\n# ADD YOUR CODE HERE\\nfor i in range(len(measurements)):\\n    p = sense(p, measurements[i])\\n\\nprint(p)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify the code so that it updates the probability twice\n",
    "# and gives the posterior distribution after bothmeasurements are incorporated.\n",
    "# Make sure that your code allows for any sequence of measurement of any length.\n",
    "\n",
    "p = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "measurements = [\"red\", \"green\"]\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    # ADD YOUR CODE HERE\n",
    "    q = []\n",
    "    sum_p = sum(p)\n",
    "    for i in range(len(p)):\n",
    "        if world[i] == Z:\n",
    "            q.append(p[i] * pHit)\n",
    "        else:\n",
    "            q.append(p[i] * pMiss)\n",
    "    sum_q = sum(q)\n",
    "    q = [i / sum_q for i in q]\n",
    "    return q\n",
    "\n",
    "\n",
    "# ADD YOUR CODE HERE\n",
    "for i in range(len(measurements)):\n",
    "    p = sense(p, measurements[i])\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- before we're done with localization, I'd like to talk about robot motion\n",
    "- suppose we have a distribution over those cells--such as this one: $1/9, 1/3, 1/3, 1/9, 1/9$--and even though we don't know where the robot is, the robot moves, and it moves to the right\n",
    "- in fact, the way we're going to program is we will assume the world is cyclic, so if it drops off the right-most cell it finds itself in the left-most cell\n",
    "- suppose we know for a fact the world moved exactly 1 grid cell to the right, including the cyclic motion\n",
    "  - can you tell me for all these 5 values, what the posterior probability is after that motion?\n",
    "\n",
    "\n",
    "- the answer is all of these are shifted to the right--the $1/9$ in the left-most cell goes over here, the $1/3$ over here, and finally the right-most $1/9$ finds itself on the left side\n",
    "- if the robot's motion is perfect (which means it moves exactly as far as it thinks it does), then all of the probabilities move one place to the right\n",
    "\n",
    "<img src=\"resources/exact_robot_motion.png\"/>\n",
    "\n",
    "- in the case of exact motion, we have a perfect robot\n",
    "- we just shift the probabilities by the actual robot motion\n",
    "- now, that's a degenerate case, but it's a good one to program first so let's program this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I define a function `move` with an input distribution `p` and a motion number `U` where U is the number of grid cells that the robot is moving to the right or to the left\n",
    "- I want you to program a function that returns the new distribution `q` after the move\n",
    "  - if `U` equals $0$, `q` is the same as `p`\n",
    "  - if `U` equals $1$, all the values are cyclically shifted to the right by $1$\n",
    "  - if `U` equals $3$, they are cyclically shifted to the right by $3$\n",
    "  - if `U` equals $-1$, they're cyclically shifted to the left\n",
    "- please call the function with argument `p` and a shift to the right by $1$\n",
    "- I've commented out my measurement part because for now I don't want to do measurement updates\n",
    "- in addition to this, I will use a very simple `p`, that has a $1$ at the second position and zeros elsewhere\n",
    "  - otherwise, if we were to use the uniform `p`, we couldn't even see the effect of the motion whether that's programmed correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# Program a function that returns a new distribution q, shifted to the right by U units.\\n# If U=0, q should be the same as p.\\n\\np = [0, 1, 0, 0, 0]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    # ADD CODE HERE\\n    q = []\\n    for i in range(len(p)):\\n        q.append(p[(i - U) % len(p)])\\n    return q\\n\\n\\nprint(move(p, 1))\";\n",
       "                var nbb_formatted_code = \"# Program a function that returns a new distribution q, shifted to the right by U units.\\n# If U=0, q should be the same as p.\\n\\np = [0, 1, 0, 0, 0]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    # ADD CODE HERE\\n    q = []\\n    for i in range(len(p)):\\n        q.append(p[(i - U) % len(p)])\\n    return q\\n\\n\\nprint(move(p, 1))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Program a function that returns a new distribution q, shifted to the right by U units.\n",
    "# If U=0, q should be the same as p.\n",
    "\n",
    "p = [0, 1, 0, 0, 0]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "measurements = [\"red\", \"green\"]\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        hit = Z == world[i]\n",
    "        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\n",
    "    s = sum(q)\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i] / s\n",
    "    return q\n",
    "\n",
    "\n",
    "def move(p, U):\n",
    "    # ADD CODE HERE\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        q.append(p[(i - U) % len(p)])\n",
    "    return q\n",
    "\n",
    "\n",
    "print(move(p, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we start with the empty list\n",
    "- we go through all the elements in `p`\n",
    "- we will construct `q` element-by-element by accessing the corresponding `p`, and `p` is shifted by `U` and if this shift exceeds the range of `p` on the left, we apply the modulo operator with the number of states as an argument\n",
    "  - in this case, it'll be $5$\n",
    "- the reason why there is a minus sign is tricky\n",
    "  - to shift the distribution to the right, `U = 1`, we need to find in `p` the element $1$ place to the left\n",
    "  - rather than shifting `p` to the right directly, what I've done is I've constructed `q` by searching for where the robot might have come from\n",
    "    - that's of course, in hindsight, from the left\n",
    "\n",
    "\n",
    "- think about this, as it's a little bit nontrivial, but it's going to be important as we go forward and define probabilistic convolution and generalize this to the noisy case\n",
    "\n",
    "\n",
    "- alternate solution:\n",
    "```python\n",
    "U = U % len(p)\n",
    "q = p[-U:] + p[:-U]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inexact Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's talk about inaccurate robot motion\n",
    "- we are again given $5$ grid cells\n",
    "- for $U = 2$ let's assume a robot executes its action with high probability correctly say $0.8$, but with $0.1$ chance it finds itself short of the intended action, and yet another $0.1$ probability it finds itself overshooting its target\n",
    "- you can define the same for other U values, say $U = 1$--then with $0.8$ chance it would end up over here, $0.1$ it stays in the same element, and $0.1$ it hops $2$ elements ahead\n",
    "\n",
    "<img src=\"resources/inaccurate_robot_motion.png\"/>\n",
    "\n",
    "- now this is a model of inaccurate robot motion\n",
    "- this robot attempts to go U grid cells, but occasionally falls short of its goal or overshoots\n",
    "- it's a more common case - robots as they move accrue uncertainty, and it's really important to model this, because this is the primary reason why localization is hard, because robots are not very accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we're now going to look into this first from the mathematical side\n",
    "- I will be giving you a prior distribution, and we're going to be using the value of $U = 2$\n",
    "  - for the motion model that shifts the robot exactly $2$ steps, we believe there is a $0.8$ chance\n",
    "  - we assign a $0.1$ to the cases where the robot over or under shoots by exactly $1$\n",
    "  - that's kind of written by this formula over here where the two gets a $0.8$ probability, the one and the three end up with a $0.1$ probability\n",
    "- I'm going to ask you now for the initial distribution that I'm writing up here, can you give me the distribution after the motion?\n",
    "\n",
    "<img src=\"resources/inexact_motion_solution_1.png\"/>\n",
    "\n",
    "- the answer is for our intended field over here $0.8$, the two neighbors $0.1$ and a $0$ and $0$ at at the beginning\n",
    "- notice that this motion has added some uncertainty to the robot's position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's assume we have a $0.5$ in this cell and a $0.5$ in this cell\n",
    "- remember that this is a cyclic-motion model, so whatever falls off on the right side, you'll find on the left side\n",
    "- can you again for $U = 2$ fill in the posterior distribution?\n",
    "\n",
    "<img src=\"resources/inexact_motion_solution_2.png\"/>\n",
    "\n",
    "- this is a pretty tricky question, which I'm going to answer in two phases\n",
    "- let's just look at the first $0.5$ over here\n",
    "  - $0.8$ of that, which is $0.4$, ends up over here, and $0.1$ of this, which is $0.05$ ends up over here\n",
    "  - the reason why I write it so small (green) is because this is not the correct answer quite yet\n",
    "- let's look at the other $0.5$\n",
    "  - $0.4$ goes two steps and ends up over here on the left side, but $0.1$ falls short and makes the $0.05$ over here in the last grid cell\n",
    "- interestingly enough, for the cell on the right side, there's two possibly ways you could've gotten there\n",
    "  - either by overshooting starting in the second cell, or undershooting starting in the right cell\n",
    "  - so the total probability is the sum of these two things--$0.1$\n",
    "- this is the final answer: $0.4, 0.05, 0.05, 0.4, 0.1$\n",
    "- notice that the robot is now pretty uncertain about its location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let me give you a final example in which I assume a uniform distribution, and I want you to fill in for me the distribution after motion\n",
    "\n",
    "<img src=\"resources/inexact_motion_solution_3.png\"/>\n",
    "\n",
    "- the answer as it turns out will be just $0.2$ everywhere, and the reason is with every grid cell being equally likely, applying this motion model will still make each grid cell equally likely\n",
    "- you can't get any more uncertain than the uniform distribution!\n",
    "  - let's pick one of them--say this one over here (4th cell)\n",
    "  - we could have arrived here in 3 different ways\n",
    "    - perhaps we started in $x_2$ and our motion went well--this gives us a $0.2 \\times 0.8$\n",
    "    - perhaps we started in $x_1$ and we overshot, which gives us a $0.2$, for the cell $x_1$, times a $0.1$ for overshooting\n",
    "    - or perhaps we started in $x_3$ and we undershot, which gives us $0.2 \\times 0.1$\n",
    "  - if we add those up, then we find it is the same as $0.2 \\times 1$, because the factors over here add up exactly to $1$, which makes $0.2$\n",
    "  - you can apply this same logic to all the other cells\n",
    "    - that's called a convolution, and as well see later, there's a very nice way to write this mathematically as something called *Theorem of Total Probability*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inexact Move Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm going to give us a `pExact` of $0.8$, `pOvershoot` of $0.1$, and `pUndershoot` of $0.1$\n",
    "- I'd like you to modify the move procedure to accommodate these extra probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.1, 0.8, 0.1, 0.0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# Modify the move function to accommodate the added probabilities\\n# of overshooting or undershooting the intended destination.\\n\\np = [0, 1, 0, 0, 0]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\npExact = 0.8\\npOvershoot = 0.1\\npUndershoot = 0.1\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    q = []\\n    for i in range(len(p)):\\n        s = pExact * p[(i - U) % len(p)]\\n        s = s + pOvershoot * p[(i - U - 1) % len(p)]\\n        s = s + pUndershoot * p[(i - U + 1) % len(p)]\\n        q.append(s)\\n    return q\\n\\n\\nprint(move(p, 1))\";\n",
       "                var nbb_formatted_code = \"# Modify the move function to accommodate the added probabilities\\n# of overshooting or undershooting the intended destination.\\n\\np = [0, 1, 0, 0, 0]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\npExact = 0.8\\npOvershoot = 0.1\\npUndershoot = 0.1\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    q = []\\n    for i in range(len(p)):\\n        s = pExact * p[(i - U) % len(p)]\\n        s = s + pOvershoot * p[(i - U - 1) % len(p)]\\n        s = s + pUndershoot * p[(i - U + 1) % len(p)]\\n        q.append(s)\\n    return q\\n\\n\\nprint(move(p, 1))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify the move function to accommodate the added probabilities\n",
    "# of overshooting or undershooting the intended destination.\n",
    "\n",
    "p = [0, 1, 0, 0, 0]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "measurements = [\"red\", \"green\"]\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "pExact = 0.8\n",
    "pOvershoot = 0.1\n",
    "pUndershoot = 0.1\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        hit = Z == world[i]\n",
    "        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\n",
    "    s = sum(q)\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i] / s\n",
    "    return q\n",
    "\n",
    "\n",
    "def move(p, U):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        s = pExact * p[(i - U) % len(p)]\n",
    "        s = s + pOvershoot * p[(i - U - 1) % len(p)]\n",
    "        s = s + pUndershoot * p[(i - U + 1) % len(p)]\n",
    "        q.append(s)\n",
    "    return q\n",
    "\n",
    "\n",
    "print(move(p, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we're going to introduce the auxiliary variable `s` which we build up in three different steps\n",
    "  - we multiply the `p` value as before for the exact set off by `pExact`\n",
    "  - then we add to it two more multiplied by `pOvershoot` or `pUndershoot` where we are overshooting by going yet $1$ step further than `U` or undershooting by cutting it short by $1$\n",
    "  - then we add these things up and finally append the sum of those to our output probability `q`\n",
    "  - when we run this, we get for our example prior of $0, 1, 0, 0, 0$ the answer $0, 0.1, 0.8, 0.1, 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppose we have $5$ grid cells as before with an initial distribution that assigns $1$ to the first grid cell and $0$ to all the other ones\n",
    "- let's assume we do $U = 1$, which means with $0.8$ chance in each action we transition $1$ to the right, with $0.1$ chance we don't move at all, and with $0.1$ chance again we skip and move $2$ steps\n",
    "- again, let's assume the world is cyclic, so every time I fall off on the right side, I find myself back on the left side\n",
    "\n",
    "\n",
    "- suppose I run infinitely many motion steps--then I actually get a what's called a **limit distribution**\n",
    "- what's going to happen to my robot if it never senses but executes the action of going $1$ to the right on our little cyclic environment forever?\n",
    "- what will be the so-called limit or stationary distribution be in the very end?\n",
    "\n",
    "\n",
    "- the answer is the uniform distribution--there's an intuitive reasoning behind this\n",
    "- every time we move, we lose information--that is, in the initial distribution we know exactly where we are\n",
    "- one step in we have a $0.8$ chance, but the $0.8$ will fall to something smaller as we move on--$0.64$ and so on\n",
    "- the distribution of the absolute least information is the uniform distribution--it has no preference whatsoever\n",
    "  - that is really the result of moving many, many times\n",
    "- as the robot continues to get more and more uncertain about where it is, eventually it will reach the state of maximal uncertainty: the uniform distribution\n",
    "\n",
    "<img src=\"resources/limit_distribution.png\"/>\n",
    "\n",
    "- there is a way to derive this mathematically, and I can prove a property that's highly related, which is a *balance property*\n",
    "  - say we take $x_4$, and we'd like to understand how $x_4$ at some timestamp $t$ corresponds to the previous time distribution over all these variables\n",
    "  - for this to be stationary, it has to be the same\n",
    "  - put differently, the probability of $x_4$ must be the same as $0.8 p(x2) + 0.1p(x1) + 0.1p(x3)$\n",
    "    - this is exactly the same calculation we did before where we asked what's the chance of being $x_4$--well, you might be coming from $x2, x1,$ or $x3$, and there's these probabilities are $0.8, 0.1$, and $0.1$, they govern the likelihood you might have been coming from there\n",
    "    - those together must hold true in the limit when things don't move anymore\n",
    "    - now, you might think there are many different ways to solve this and the $0.2$ is just one solution, but it turns out $0.2$ is the only solution\n",
    "      - if you plug in $0.2$ over here and $0.2$ over here and $0.2$ over here, you get $1 \\times 0.2$, and that's $0.2$ on the right side\n",
    "      - clearly, those $0.2$s over here meet the balance that is necessary to define a valid solution in the limit\n",
    "    - the formula given is for $U=2$, not for $U=1$--this mistake in the formula does not change the result, however: at the end we get a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move Twice and Move 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now let's go back to our code and move many times\n",
    "- let's move twice, so please write a piece of code that makes the robot move twice, starting with the initial distribution as shown over here--$0, 1, 0, 0, 0$\n",
    "  - here's a piece of code that moves twice by the same amount as before, and the output now is a vector that assigns $0.66$ as the largest value and not $0.8$ anymore\n",
    "\n",
    "\n",
    "- let's move 1,000 times--write a piece of code that moves 1,000 steps and give me the final distribution\n",
    "  - we have a loop for 1,000 steps--we move 1,000 times, and we print the corresponding distribution over here--it's $0.2$ in each case as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20000000000000365, 0.20000000000000373, 0.20000000000000365, 0.2000000000000035, 0.2000000000000035]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"p = [0, 1, 0, 0, 0]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\npExact = 0.8\\npOvershoot = 0.1\\npUndershoot = 0.1\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    q = []\\n    for i in range(len(p)):\\n        s = pExact * p[(i - U) % len(p)]\\n        s = s + pOvershoot * p[(i - U - 1) % len(p)]\\n        s = s + pUndershoot * p[(i - U + 1) % len(p)]\\n        q.append(s)\\n    return q\\n\\n\\n# Write code that makes the robot move twice and then\\n# prints out the resulting distribution, starting with the initial distribution p = [0, 1, 0, 0, 0]\\n# for i in range(2):\\n#     p = move(p, 1)\\n\\n# Write code that moves 1000 times and then prints the resulting probability distribution.\\nfor i in range(1000):\\n    p = move(p, 1)\\n\\n# Make sure to print out p!\\nprint(p)\";\n",
       "                var nbb_formatted_code = \"p = [0, 1, 0, 0, 0]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\nmeasurements = [\\\"red\\\", \\\"green\\\"]\\npHit = 0.6\\npMiss = 0.2\\npExact = 0.8\\npOvershoot = 0.1\\npUndershoot = 0.1\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    q = []\\n    for i in range(len(p)):\\n        s = pExact * p[(i - U) % len(p)]\\n        s = s + pOvershoot * p[(i - U - 1) % len(p)]\\n        s = s + pUndershoot * p[(i - U + 1) % len(p)]\\n        q.append(s)\\n    return q\\n\\n\\n# Write code that makes the robot move twice and then\\n# prints out the resulting distribution, starting with the initial distribution p = [0, 1, 0, 0, 0]\\n# for i in range(2):\\n#     p = move(p, 1)\\n\\n# Write code that moves 1000 times and then prints the resulting probability distribution.\\nfor i in range(1000):\\n    p = move(p, 1)\\n\\n# Make sure to print out p!\\nprint(p)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = [0, 1, 0, 0, 0]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "measurements = [\"red\", \"green\"]\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "pExact = 0.8\n",
    "pOvershoot = 0.1\n",
    "pUndershoot = 0.1\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        hit = Z == world[i]\n",
    "        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\n",
    "    s = sum(q)\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i] / s\n",
    "    return q\n",
    "\n",
    "\n",
    "def move(p, U):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        s = pExact * p[(i - U) % len(p)]\n",
    "        s = s + pOvershoot * p[(i - U - 1) % len(p)]\n",
    "        s = s + pUndershoot * p[(i - U + 1) % len(p)]\n",
    "        q.append(s)\n",
    "    return q\n",
    "\n",
    "\n",
    "# Write code that makes the robot move twice and then\n",
    "# prints out the resulting distribution, starting with the initial distribution p = [0, 1, 0, 0, 0]\n",
    "# for i in range(2):\n",
    "#     p = move(p, 1)\n",
    "\n",
    "# Write code that moves 1000 times and then prints the resulting probability distribution.\n",
    "for i in range(1000):\n",
    "    p = move(p, 1)\n",
    "\n",
    "# Make sure to print out p!\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sense and Move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we talked about measurement updates, and we talked about motion\n",
    "  - we called these two routines *sense* and *move*\n",
    "- localization is nothing else but the iteration of *sense* and *move*\n",
    "  - there is an initial belief that is tossed into this loop\n",
    "  - if you sense first, if comes to the left side--then localization cycles through these--move, sense cycle\n",
    "\n",
    "\n",
    "- every time the robot moves, it loses information as to where it is\n",
    "  - that's because robot motion is inaccurate\n",
    "- every time it senses it gains information\n",
    "- that is manifest by the fact that after motion, the probability distribution is a little bit flatter and a bit more spread out and after sensing, it's focused a little bit more\n",
    "  - in fact, as a footnote, there is a measure of information called *entropy*\n",
    "    - here is one of the many ways you can write it: $-\\sum p(X_i)\\log p(X_i)$ as the expected log (logarithmic) likelihood of the probability of each grid cell\n",
    "    - without going into detail, this is a measure of information that the distribution has, and it can be shown that the update step, the motion step, makes the entropy go down, and the measurement step makes it go up--you're really losing and gaining information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarification Regarding Entropy\n",
    "\n",
    "- the video mentions that entropy will decrease after the motion update step and that entropy will increase after measurement step\n",
    "- what is meant is that that entropy will decrease after the measurement update (sense) step and that entropy will increase after the movement step (move)\n",
    "\n",
    "\n",
    "- in general, entropy represents the amount of uncertainty in a system\n",
    "- since the measurement update step decreases uncertainty, entropy will decrease\n",
    "- the movement step increases uncertainty, so entropy will increase after this step\n",
    "\n",
    "\n",
    "- let's look at our current example where the robot could be at one of five different positions\n",
    "- the maximum uncertainty occurs when all positions have equal probabilities $[0.2, 0.2, 0.2, 0.2, 0.2]$\n",
    "- following the formula $Entropy = \\sum (-p \\times log(p))$, we get $-5 \\times (.2)\\times log(0.2) = 0.699$\n",
    "\n",
    "\n",
    "- taking a measurement will decrease uncertainty and entropy\n",
    "- let's say after taking a measurement, the probabilities become $[0.05, 0.05, 0.05, 0.8, 0.05]$\n",
    "- now we have a more certain guess as to where the robot is located and our entropy has decreased to 0.338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I would now love to implement this in our code\n",
    "- in addition to the two measurements we had before, red and green, I'm going to give you 2 motions--1 and 1, which means the robot moves right and right again\n",
    "- can you compute the posterior distribution if the robot first senses red, then moves right by 1, then senses green, then moves right again?\n",
    "- let's start with a uniform prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07882352941176471, 0.07529411764705884, 0.22470588235294123, 0.4329411764705882, 0.18823529411764706]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# Given the list motions=[1,1] which means the robot\\n# moves right and then right again, compute the posterior\\n# distribution if the robot first senses red, then moves\\n# right one, then senses green, then moves right again,\\n# starting with a uniform prior distribution.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\n# measurements = ['red', 'green']\\nmeasurements = [\\\"red\\\", \\\"red\\\"]\\nmotions = [1, 1]\\npHit = 0.6\\npMiss = 0.2\\npExact = 0.8\\npOvershoot = 0.1\\npUndershoot = 0.1\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    q = []\\n    for i in range(len(p)):\\n        s = pExact * p[(i - U) % len(p)]\\n        s = s + pOvershoot * p[(i - U - 1) % len(p)]\\n        s = s + pUndershoot * p[(i - U + 1) % len(p)]\\n        q.append(s)\\n    return q\\n\\n\\n# ADD CODE HERE\\nfor i in range(len(measurements)):\\n    p = sense(p, measurements[i])\\n    p = move(p, motions[i])\\n\\nprint(p)\";\n",
       "                var nbb_formatted_code = \"# Given the list motions=[1,1] which means the robot\\n# moves right and then right again, compute the posterior\\n# distribution if the robot first senses red, then moves\\n# right one, then senses green, then moves right again,\\n# starting with a uniform prior distribution.\\n\\np = [0.2, 0.2, 0.2, 0.2, 0.2]\\nworld = [\\\"green\\\", \\\"red\\\", \\\"red\\\", \\\"green\\\", \\\"green\\\"]\\n# measurements = ['red', 'green']\\nmeasurements = [\\\"red\\\", \\\"red\\\"]\\nmotions = [1, 1]\\npHit = 0.6\\npMiss = 0.2\\npExact = 0.8\\npOvershoot = 0.1\\npUndershoot = 0.1\\n\\n\\ndef sense(p, Z):\\n    q = []\\n    for i in range(len(p)):\\n        hit = Z == world[i]\\n        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\\n    s = sum(q)\\n    for i in range(len(q)):\\n        q[i] = q[i] / s\\n    return q\\n\\n\\ndef move(p, U):\\n    q = []\\n    for i in range(len(p)):\\n        s = pExact * p[(i - U) % len(p)]\\n        s = s + pOvershoot * p[(i - U - 1) % len(p)]\\n        s = s + pUndershoot * p[(i - U + 1) % len(p)]\\n        q.append(s)\\n    return q\\n\\n\\n# ADD CODE HERE\\nfor i in range(len(measurements)):\\n    p = sense(p, measurements[i])\\n    p = move(p, motions[i])\\n\\nprint(p)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Given the list motions=[1,1] which means the robot\n",
    "# moves right and then right again, compute the posterior\n",
    "# distribution if the robot first senses red, then moves\n",
    "# right one, then senses green, then moves right again,\n",
    "# starting with a uniform prior distribution.\n",
    "\n",
    "p = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "world = [\"green\", \"red\", \"red\", \"green\", \"green\"]\n",
    "# measurements = ['red', 'green']\n",
    "measurements = [\"red\", \"red\"]\n",
    "motions = [1, 1]\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "pExact = 0.8\n",
    "pOvershoot = 0.1\n",
    "pUndershoot = 0.1\n",
    "\n",
    "\n",
    "def sense(p, Z):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        hit = Z == world[i]\n",
    "        q.append(p[i] * (hit * pHit + (1 - hit) * pMiss))\n",
    "    s = sum(q)\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i] / s\n",
    "    return q\n",
    "\n",
    "\n",
    "def move(p, U):\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        s = pExact * p[(i - U) % len(p)]\n",
    "        s = s + pOvershoot * p[(i - U - 1) % len(p)]\n",
    "        s = s + pUndershoot * p[(i - U + 1) % len(p)]\n",
    "        q.append(s)\n",
    "    return q\n",
    "\n",
    "\n",
    "# ADD CODE HERE\n",
    "for i in range(len(measurements)):\n",
    "    p = sense(p, measurements[i])\n",
    "    p = move(p, motions[i])\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the world has a green, a red, a red, and a green, and a green field\n",
    "- the robot saw red, followed by a right motion, and green\n",
    "- that suggests that it probably started with with the highest likelihood in grid cell number 3, which is the right-most of the two red cells\n",
    "- it saw red correctly and then moved to the right by 1\n",
    "- it saw green correctly, moved right again\n",
    "- it now finds itself most likely in the right-most cell\n",
    "\n",
    "\n",
    "- let's pick a different base\n",
    "  - let's assume the robot saw red twice\n",
    "  - it senses red, it moves, it senses red, it moves again\n",
    "  - what is the most likely cell?\n",
    "    - we find that the most likely cell is the 4th cell\n",
    "    - that makes sense, because the best match of red, red to the world is red at indexes 1 and 2\n",
    "    - after seeing the 2nd red, the robot still moved 1 to the right and finds itself in the 4th cell as shown over here\n",
    "\n",
    "\n",
    "- now I want to celebrate with you the code that you just wrote, which is a piece of software that implements the essence of Google's self-driving car's localization approach\n",
    "- as I said in the beginning, it's absolutely crucial that the car knows exactly where it is relative to the map of its road\n",
    "- while the road isn't painted green and red, the road has lane markers\n",
    "- instead of those green and red cells over here, we plug in the color of the lane markings relative to the color of the pavement\n",
    "- it isn't just one observation per time step, it's an entire field of observations, an entire camera image, but you can do the same with a camera image as long as you can correspond a camera image in your model with a camera image in your measurements\n",
    "- then a piece of code not much more difficult than what you coded yourself is responsible for localizing the Google self-driving car (last for lop)\n",
    "  - you just implemented a major, major function that makes Google's car drive itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we learned that localization maintains a function over all possible places where a road might be, where each cell has an associated probability value\n",
    "  - belief = probability\n",
    "\n",
    "\n",
    "- the measurement update function, or *sense*, is nothing else but a product in which we take those probability values and multiply them up or down depending on the exact measurement\n",
    "- because the product might violate the fact that probabilities add up to $1$, there was a product followed by normalization\n",
    "  - sense = product followed by normalization\n",
    "\n",
    "\n",
    "- motion was a convolution (addition)\n",
    "  - this word itself might sound cryptic, but what it really means is for each possible location after the motion, we reverse engineered the situation and guessed where the world might have come from and then collected, we added, the corresponding probabilities\n",
    "\n",
    "\n",
    "- something as simple as multiplication and addition solves all of localization and is the foundation for autonomous driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal Definition of Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to spend a few minutes and go over the formal definition of localization\n",
    "- I'm going to introduce probability and ask you lots of questions\n",
    "\n",
    "\n",
    "- formally, we define a probability function to be $P(X)$, and it's a value that is bounded below and above by $0$ and $1$: $0 \\leq P(X) \\leq 1$\n",
    "  - $X$ often can take multiple values\n",
    "  - probabilities always add up to $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's look into measurements, and they will lead to something called *Bayes Rule*\n",
    "- you might have heard about Bayes Rule before--it's the most fundamental consideration in probabilistic inference, but the Bayes Rule is really, really simple\n",
    "- suppose $X$ is my grid cell and $Z$ is my measurement\n",
    "  - then the measurement update seeks to calculate a belief over my location after seeing the measurement\n",
    "  - Bayes Rule looks like this: $P(X_i|Z) = \\dfrac{P(Z|X_i)P(X_i)}{P(Z)}$\n",
    "    - what it does is it takes my prior distribution, $P(X)$, and multiplies in the chances of seeing a red or green tile for every possible location and out comes the non-normalized posterior distribution we had before\n",
    "    - $P(X)$ is prior and $P(Z|X)$ is measurement probability\n",
    "    - if we put a little index *i* over here, then just the product of the prior of the grid cell times the measurement probability, which was large if the measurement corresponded to the correct color and small if it corresponded to a false color\n",
    "      - that product gave us the non-normalized posterior distribution for the grid cell\n",
    "      - we programmed this; a product between the prior probability distribution and a number\n",
    "    - the normalization is now the constant $P(Z)$\n",
    "      - technically, that is the probability of seeing a measurement devoid of any location information\n",
    "\n",
    "\n",
    "- the easiest way to understand what's going on is to realize that this is a function that assigns to each grid cell a number, and the $P(Z)$ doesn't have the grid cell as an index so no matter what grid cell we consider, the $P(Z)$ is the same\n",
    "- no matter what $P(Z)$ is, because the final posterior has to be a probability distribution, by normalizing these non-normalized products, we will exactly calculate $P(Z)$\n",
    "  - put differently, $P(Z)$ is the sum over all $i$ of just this product: $P(Z) = \\sum_{i} P(Z|X_i)P(X_i)$\n",
    "    - it's a product of our prior distribution with a measurement probability, which we know to be large if the color is correct and small otherwise\n",
    "\n",
    "\n",
    "- we do this and assign it to a so-called non-normalized probability $\\overline{P}(X_i|Z) \\leftarrow {P(Z|X_i)P(X_i)}$\n",
    "- then we  compute the normalizer $\\alpha \\leftarrow \\sum \\overline{P}(X_i|Z)$\n",
    "- then we just normalize\n",
    "- our resulting probability will be $\\dfrac{1}{\\alpha}$ of the non-normalized probability: $P(X_i|Z) \\leftarrow \\dfrac{1}{\\alpha} \\overline{P}(X_i|Z)$\n",
    "\n",
    "\n",
    "- this is exactly what we did, and this is exactly Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let me ask you Bayes Rule in the context of a completely different example to see if you understand how to apply Bayes Rule\n",
    "- this time it's about cancer testing; it is an example that is commonly studied in statistics classes\n",
    "- suppose there exists a certain type of cancer, but the cancer is rare--only 1 in a 1000 people has the cancer--where as 999 in 1000 people don't have it, illustrated by the probability of cancer and the probability of not cancer\n",
    "- suppose we have a test, and the test can come out positive or negative\n",
    "  - the probability that the test triggers positive if you have cancer is $0.8$, and the probability that the test comes out positive given that I'm cancer free is only $0.1$\n",
    "  - clearly the test has a strong correlation to whether I have cancer\n",
    "\n",
    "\n",
    "- here's a really difficult question\n",
    "- can you compute for me the probability of cancer given that I just received a positive test\n",
    "- think of the cancer/non cancer as the robot position and think of the positive as whether the colored door observed is the correct one\n",
    "\n",
    "<img src=\"resources/cancer_test.png\"/>\n",
    "\n",
    "- the result of Bayes Rule, non-normalized of C given POS is simply the product of my prior probability, $0.001 \\times 0.8$, which is the probability of a positive result in the cancer state\n",
    "  - that ends up to be $0.0008$\n",
    "- the non-normalized probability for the opposite event, the non-cancerous event, given a positive test, is $0.999 \\times 0.1$\n",
    "  - that's obviously $0.0999$\n",
    "- our normalizer is the sum of both of those, which is $0.1007$\n",
    "- dividing $0.0008$, the non-normalized probability, by $0.1007$ gives us $0.0079$\n",
    "- the answer is 0.0079--in other words, there's only $0.79%$ chance, $0.79$ out of $100$ that, despite the positive test result, that you have cancer\n",
    "\n",
    "\n",
    "- we just applied Bayes Rule to compute a really involved probability of having cancer after seeing a test result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theorem of Total Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's look at motion, which will turn out to be something we will call total probability\n",
    "- you remember that we cared about a grid cell $X_i$ and we asked what is the chance of being in $X_i$ after robot motion?\n",
    "- to indicate the after and before, let me add a time index--$t$ up here, is an index for time\n",
    "  - I write it superscript so there is no confusion with the index $i$, which is the grid cell\n",
    "\n",
    "\n",
    "- you might remember the way we computed this was by looking at all the grid cells the robot could have come from on time step earlier--indexed here by $j$\n",
    "- we looked at the prior probability of those grid cells at time $t - 1$\n",
    "- we multiply with the probability that our motion command would carry us from $X_j$ to $X_i$\n",
    "- this is written as a condition distribution as follows: $P(X_i^t) = \\sum_{j}P(X_j^{t-1}) \\cdot P(X_i|X_j)$\n",
    "\n",
    "<img src=\"resources/total_probability.png\"/>\n",
    "\n",
    "- this was exactly what we implemented\n",
    "  - if there was our grid cells over here and we asked one time step later about a specific grid cell over here, we would combine $0.8$ from over here, $0.1$ from over here, and $0.1$ from over here into the probability of this grid cell\n",
    "  - it's the same formula as above\n",
    "    - this is now $X_i$, and the way we find the posterior probability for $X_i$ is to go through all possible places from which we could have come, all the different $j$'s, look at the prior probabilities, multiply it by the probability that $i$ transition from $j$ to $i$ given my motion command, which in this case is go $1$ to the right side\n",
    "\n",
    "\n",
    "- in probability terms, people often write it as follows: $P(A) = \\sum_{B} P(A|B)P(B)$\n",
    "  - this is just the way you'd find it in text books, and you can see directly the correspondence of $A$ as a place $i$ of time $t$ and all the different $B$s as the possible prior locations\n",
    "  - that is often called the *Theorem of Total Probability*\n",
    "- the operation of a weighted sum over other variables is often called a *convolution*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppose I flip a coin, and the coin comes up tails or heads\n",
    "- suppose it's a fair coin; the probability of tails or of heads is both $\\frac{1}{2}$\n",
    "- let's say that the coin comes up tails, and I just accept and don't do anything\n",
    "- but suppose it comes up heads, and I flip it again, and after $1$ flip, I accept the result\n",
    "- my quiz for you is what is the probability that the final result is heads?\n",
    "  - that's an example of total probability\n",
    "\n",
    "<img src=\"resources/coin_flip.png\"/>\n",
    "\n",
    "- the answer is $\\frac{1}{4}$\n",
    "- it's easy to see that the probability of heads in step $2$ is the probability of heads in step $2$ conditioned on heads in step $1$ times probability of heads in $1$ plus, that's the sum, probability of heads in step $2$ given we had tails in step $1$ times probability of tails in step $1$\n",
    "  - now, the way I set it up, those things here are equally likely\n",
    "  - however, if we did have tails in step $1$, we would never toss the coin again and just accept it\n",
    "  - it's impossible that in step $2$ I flip over the heads; it's probability is zero\n",
    "  - whereas if I found heads, I would flip again and then the $0.5$ chance I arrive at heads\n",
    "  - if I look at this, then this all becomes zero, and these guys multiply to $\\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there are multiple coins; one is fair and one is loaded\n",
    "  - the fair coin has a probability of heads of $0.5$\n",
    "  - the loaded coin has a probability of heads of $0.1$\n",
    "- I'm going to grab a random coin with $50\\%$ chance; the fair coin will be chosen with $50\\%$ chance, and the loaded coin will be chosen with $50\\%$ chance, but I don't know which one it is\n",
    "- I flip it and I observe heads\n",
    "- what's the probability that the coin I hold in my hand is fair?\n",
    "  - apply anything you've learned before--one of the rules you've learned before is exactly the right one to apply here\n",
    "\n",
    "<img src=\"resources/two_coins.png\"/>\n",
    "\n",
    "- what I'm really asking you is the probability of a fair coin $F$  given that I observed $H$\n",
    "- this has nothing to do with total probability and all with Bayes Rules, because I'm talking about observations\n",
    "- the non-normalized probability according to Bayes Rule is obtained as follows:\n",
    "  - the probability of observing $H$ for the fair coin is $0.5$, and the probability of having grabbed the fair coin is $0.5$ as well\n",
    "- the non-normalized probability of not $F$ given $H$, which is the loaded coin, is probability of $H$ given not $F$, which we know to be $0.1$ times the probability of not picking the fair coin, which is $0.5$, ends up to be $0.05$\n",
    "\n",
    "\n",
    "- when we now normalize, we get $\\alpha = 0.25 + 0.05$, which is $0.3$\n",
    "- if we now normalize the $0.25$ over here with the $0.3$, we get $0.833$, which is the same as $\\frac{5}{6}$\n",
    "- that's our posterior probability we hold the fair coin after we observed $H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
