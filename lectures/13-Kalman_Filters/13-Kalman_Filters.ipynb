{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here you can see the Google self-driving car using a road map, localizing itself, but in addition what's shown here in red are measurements of other vehicles\n",
    "  - the car uses lasers and radars to track other vehicles\n",
    "\n",
    "<img src=\"resources/googlecar_sensor_map.png\"/>\n",
    "\n",
    "- we're going to talk about how to find other cars\n",
    "- the reason why we'd like to find other cars is because you wouldn't want to run into them\n",
    "- we have to understand how to interpret sensor data to make assessments not just where these other cars are, as in the localization case, but also how fast they're moving\n",
    "- so you you can drive in a way that avoids collisions with them in the future; it's important not just for cars and for pedestrians and for bicyclists\n",
    "- understanding where the cars are and making predictions where they're going to move is absolutely essential for safe driving\n",
    "\n",
    "\n",
    "- in this class we will talk about tracking, and the technique I'd like to teach you is called a **Kalman filter**\n",
    "  - this is an insanely popular technique for estimating the state of a system\n",
    "  - Kalman filters estimate a continuous state and as a result, they give us a uni-modal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's beginwith an example\n",
    "  - consider the car down here\n",
    "  - let's assume it senses this measurement: objects at the times $t=0$, $t=1$, $t=2$, $t=3$\n",
    "  - where would you assume the object would be at $t=4$?\n",
    "\n",
    "<img src=\"resources/tracking_question.png\"/>\n",
    "\n",
    "- from those observations you would say that the velocity points in the direction of this vector\n",
    "- assuming no drastic change in velocity, you expect that the 5th position would be over here\n",
    "\n",
    "<img src=\"resources/tracking_answer.png\"/>\n",
    "\n",
    "- the Kalman filter takes observations like these and estimates future locations and velocities based on data like this\n",
    "\n",
    "\n",
    "- I'm going to teach you how to write a piece of software that let's you take points like those--even if they're noisy and uncertain-- and estimate automatically where future locations might be and at what velocity the object is moving\n",
    "- the Google self-driving car uses methods like these to understand where other traffic is based on radar and laser-range data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Kalman filters, the distribution is given by what's called a *Gaussian*\n",
    "- Gaussian is a continuous function over the space of locations in the area underneath which sums up to $1$\n",
    "\n",
    "<img src=\"resources/gaussian.png\"/>\n",
    "\n",
    "\n",
    "- here's our Gaussian again and if we call the space X then the Gaussian is characterized by two parameters\n",
    "  - the mean, $\\mu$\n",
    "  - the width of the Gaussian, often called the variance, $\\sigma^2$\n",
    "    - for reasons that I don't want to go into, it's often written as a quadratic variable\n",
    "-  any Gaussian in 1-D, which means the parameter space is one dimensional, is characterized by $\\mu$ and $\\sigma^2$\n",
    "\n",
    "<img src=\"resources/gaussian_mu_sigma.png\"/>\n",
    "\n",
    "- our task in common spaces is to maintain a $\\mu$  and a $\\sigma^2$ as our best estimate of the location of the option we are trying to find\n",
    "- the exact formula is $f(x) = \\frac{1}{\\sigma \\sqrt {2\\pi}}e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}}$\n",
    "\n",
    "  - an exponential of a quadratic function where we take the exponent of this complicated expression over here, the quadratic difference of our query point X relative to the mean Mu divided by Sigma square by multiplied by minus a half\n",
    "  - if X equals Mu then the enumerator becomes 0 and we have exp of zero which is one\n",
    "  - it turns out we have to normalize this by a constant, one over the squared root of two Pi Sigma squared\n",
    "\n",
    "\n",
    "- let me draw you a couple of functions and you tell me which one you believe are Gaussian by checking the box on the right side\n",
    "\n",
    "<img src=\"resources/gaussian_question.png\"/>\n",
    "\n",
    "- the answer are these 3 functions\n",
    "  - they are all characterized by this exponential drop-off on both sides that are symmetrical, and they have a single peak\n",
    "    - they are what's called \"unimodal\"\n",
    "  - other functions are \"bimodal\" functions that have two peaks and as a result are not Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let me check about your intuition and draw three more Gaussians\n",
    "- I'm going to ask you about the variance\n",
    "  - for each of those check exactly one box\n",
    "  - is the covariance large, medium, or small?\n",
    "  - obviously, one of those is the largest, one is a medium, and one is small\n",
    "\n",
    "<img src=\"resources/variance_question.png\"/>\n",
    "\n",
    "- the answer is shown on the image above\n",
    "- wide Gaussians have higher variance than narrow ones\n",
    "\n",
    "\n",
    "- to see how this is being found in the formula, the difference between $x$ and $\\mu$ is being normalized by the variance\n",
    "  - the larger this value, the less the difference matters and, as a result, the more the function is spread out\n",
    "- put differently, the $\\sigma^2$ variance is a measure of uncertainty\n",
    "  - the larger the $\\sigma^2$, the more uncertain we are about the actual state\n",
    "  - second function in the image is a very certain distribution where expected deviation is small\n",
    "  - third function in the image is a relative uncertain distribution where we know very little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preferred Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if we track another car with our Google self-driving car, which Gaussian would we prefer?\n",
    "  - the first, second, or third?\n",
    "\n",
    "<img src=\"resources/preferred_gaussian_answer.png\"/>\n",
    "\n",
    "- the answer is the third function, because that's the one that's most certain, and because it is most certain, it makes a chance of accidentally hitting another car the smallest just by the fact that we know more about the car than in the two other distributions\n",
    "- we would definitely prefer a narrow Gaussian, since that means we are confident about our location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximize Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- starting with the following source code, here's a question for you\n",
    "  - how do I have to modify x (the 8) to get the maximum return value for this function `f`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12098536225957168\n",
      "0.19947114020071635\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# For this problem, you aren't writing any code.\\n# Instead, please just change the last argument in f() to maximize the output.\\n\\nfrom math import *\\n\\n\\ndef f(mu, sigma2, x):\\n    return 1 / sqrt(2.0 * pi * sigma2) * exp(-0.5 * (x - mu) ** 2 / sigma2)\\n\\n\\nprint(f(10.0, 4.0, 8.0))  # Change the 8. to something else!\\nprint(f(10.0, 4.0, 10.0))  # Change the 8. to something else!\";\n",
       "                var nbb_formatted_code = \"# For this problem, you aren't writing any code.\\n# Instead, please just change the last argument in f() to maximize the output.\\n\\nfrom math import *\\n\\n\\ndef f(mu, sigma2, x):\\n    return 1 / sqrt(2.0 * pi * sigma2) * exp(-0.5 * (x - mu) ** 2 / sigma2)\\n\\n\\nprint(f(10.0, 4.0, 8.0))  # Change the 8. to something else!\\nprint(f(10.0, 4.0, 10.0))  # Change the 8. to something else!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For this problem, you aren't writing any code.\n",
    "# Instead, please just change the last argument in f() to maximize the output.\n",
    "\n",
    "from math import *\n",
    "\n",
    "\n",
    "def f(mu, sigma2, x):\n",
    "    return 1 / sqrt(2.0 * pi * sigma2) * exp(-0.5 * (x - mu) ** 2 / sigma2)\n",
    "\n",
    "\n",
    "print(f(10.0, 4.0, 8.0))  # Change the 8. to something else!\n",
    "print(f(10.0, 4.0, 10.0))  # Change the 8. to something else!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the answer is to assert with the same value of mu, in which case exponent expression becomes zero, and we get the maximum; we get the peak of the Gaussian\n",
    "- we set x to the same value as mu, to $10$, and the output is $0.2$ approximately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement and Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the Kalman filter represents our distributions by Gaussians and iterates on two main cycles\n",
    "- Sebastian summarizes some of the key concepts from these cycles in the below referenced links\n",
    "- if you are interested, please feel free to check out these links directly from Sebastian's class on Artificial Intelligence for Robotics\n",
    "\n",
    "\n",
    "- the first cycle is the Measurement Update\n",
    "  - requires a [product](https://classroom.udacity.com/courses/cs373/lessons/48739381/concepts/487235990923#)\n",
    "  - uses [Bayes rule](https://classroom.udacity.com/courses/cs373/lessons/48739381/concepts/487221690923#)\n",
    "  \n",
    "- the second cycle is the Motion Update\n",
    "  - involves a convolution\n",
    "  - uses [total probability](https://classroom.udacity.com/courses/cs373/lessons/48739381/concepts/486736290923#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting the Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Kalman filters we iterate measurement (often called \"measurement update\") and motion (often called \"prediction\")\n",
    "  - in the update we'll use Bayes rule, which is nothing else but a product, or a multiplication\n",
    "  - in the prediction we'll use total probability, which is a convolution, or simply an addition\n",
    "- let's talk first about the measurement cycle and then the prediction cycle, using Gaussians for implementing those steps\n",
    "\n",
    "\n",
    "- suppose you're localizing another vehicle, and you have a prior distribution that looks as follows (black)\n",
    "  - it's a very wide Gaussian with the mean over here\n",
    "- now, say we get a measurement that tells us something about the localization of the vehicle, and it comes in like this (blue)\n",
    "  - it has a mean over here called $\\nu$ (Nu) and this example has a much smaller covariance for the measurement\n",
    "- this is an example where in our prior we were fairly uncertain about a location, but the measurement told us quite a bit as to where the vehicle is\n",
    "\n",
    "\n",
    "- where will the new mean of the subsequent Gaussian be?\n",
    "\n",
    "<img src=\"resources/shifting_the_mean_answer.png\"/>\n",
    "\n",
    "- the answer is over here in the middle\n",
    "  - it's between the two old means--the mean of the prior and the mean of the measurement\n",
    "  - it's slightly further on the measurement side, because the measurement was more certain as to where the vehicle is than the prior\n",
    "\n",
    "\n",
    "- the more certain we are, the more we pull the mean in the direction of the certain answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when we graph the new Gaussian, I can graph one that's very wide and very peaky\n",
    "- if I were to measure where the peak of the new Gaussian is\n",
    "  - the first point would be a very narrow and skinny Gaussian\n",
    "  - the second point would be one whose width would be in between the two Gaussians\n",
    "  - the third point is one that's even wider than the two original Gaussians\n",
    "\n",
    "\n",
    "- which one do you believe is the correct posterior after multiplying these two Gaussians?\n",
    "\n",
    "<img src=\"resources/predicting_the_peak_answer.png\"/>\n",
    "\n",
    "- the resulting Gaussian is more certain than the two component Gaussians\n",
    "  - that is, the variance is smaller than either of the two variances in isolation\n",
    "  - intuitively speaking, this is the case because we actually gain information\n",
    "  - the two Gaussians together have a higher information content than either Gaussian in isolation;\n",
    "- the new belief will be more certain than either the previous belief OR the measurement\n",
    "- the takeaway lesson here: more measurements means greater certainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppose we multiply two Gaussians as in Bayes rule-- a prior $p(x)$ and a measurement probability $p(z)$\n",
    "  - the prior has a mean of $\\mu$ and a variance of $\\sigma^2$\n",
    "  - the measurement has a mean of $\\nu$ and a covariance of $r^2$\n",
    "- the new mean is $\\mu' = \\frac{r^2\\mu + \\sigma^2\\nu}{r^2+\\sigma^2}$\n",
    "- the new variance is ${\\sigma^2}^\\prime = \\frac{1}{\\frac{1}{r^2} + \\frac{1}{\\sigma^2}}$\n",
    "\n",
    "\n",
    "- for the previous example\n",
    "  - clearly, the prior Guassian has a much higher uncertainty, therefore $\\sigma^2$ is larger\n",
    "    - that means that $\\nu$ is weighted much, much larger than the $\\mu$ so the mean will be closer to the $\\nu$ than the $\\mu$, which means that it'll be somewhere like the drawn green area above\n",
    "  - interestingly enough, the variance term is unaffected by the actual means\n",
    "    - it just uses the previous variances and comes up with a new one that's even peakier\n",
    "- the result is called posterior $p(x|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separated Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suppose we have a prior that sits over here (left) and a measurement probability that sits over here (right)--really far away--and both have the same covariance\n",
    "- where the new mean would be?\n",
    "\n",
    "<img src=\"resources/separated_gaussians_answer.png\"/>\n",
    "\n",
    "- the answer is in the middle; it's in the straight middle, because these two variances are the same (they have the same width which means same certainty), so we just average the means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let me ask the hard question now\n",
    "- will it be a Gaussian like this where the variance is larger, a Guassian with the exact same variance, or an even more peaked Guassian that's more certain than the two original factors in this calculation\n",
    "\n",
    "<img src=\"resources/separated_gaussians_2_answer.png\"/>\n",
    "\n",
    "- the answer is the more peaky Gaussian\n",
    "  - that is somewhat counter-intuitive\n",
    "  - this can be hard to wrap your head around, but multiple measurements ALWAYS gives us a more certain (and therefore taller and narrower) belief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.4, 1.6]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Write a program to update your mean and variance\\n# when given the mean and variance of your belief\\n# and the mean and variance of your measurement.\\n\\n# This program will update the parameters of your belief function.\\n\\n\\ndef update(mean1, var1, mean2, var2):\\n    new_mean = (var2 * mean1 + var1 * mean2) / (var1 + var2)\\n    new_var = 1 / (1 / var1 + 1 / var2)\\n    return [new_mean, new_var]\\n\\n\\nprint(update(10.0, 8.0, 13.0, 2.0))\";\n",
       "                var nbb_formatted_code = \"# Write a program to update your mean and variance\\n# when given the mean and variance of your belief\\n# and the mean and variance of your measurement.\\n\\n# This program will update the parameters of your belief function.\\n\\n\\ndef update(mean1, var1, mean2, var2):\\n    new_mean = (var2 * mean1 + var1 * mean2) / (var1 + var2)\\n    new_var = 1 / (1 / var1 + 1 / var2)\\n    return [new_mean, new_var]\\n\\n\\nprint(update(10.0, 8.0, 13.0, 2.0))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a program to update your mean and variance\n",
    "# when given the mean and variance of your belief\n",
    "# and the mean and variance of your measurement.\n",
    "\n",
    "# This program will update the parameters of your belief function.\n",
    "\n",
    "\n",
    "def update(mean1, var1, mean2, var2):\n",
    "    new_mean = (var2 * mean1 + var1 * mean2) / (var1 + var2)\n",
    "    new_var = 1 / (1 / var1 + 1 / var2)\n",
    "    return [new_mean, new_var]\n",
    "\n",
    "\n",
    "print(update(10.0, 8.0, 13.0, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's step back and look at what we've achieved\n",
    "  - we knew there was a measurement update and a motion update\n",
    "  - measurement update is implemented by multiplication, which is the same as Bayes rule\n",
    "  - motion update is also called prediction and is done by total probability or an addition\n",
    "\n",
    "\n",
    "- motion update is a really, really easy step\n",
    "- suppose you live in a world like this\n",
    "  - this is your current best estimate of where you are (in blue), and this is your uncertainty\n",
    "  - now say you move to the right side a certain distance and that motion itself has its own set of uncertainty (in green)\n",
    "  - then you arrive at a prediction (in red) that adds the motion of command to the mean, and it has an increased uncertainty over the initial uncertainty\n",
    "\n",
    "<img src=\"resources/gaussian_motion.png\"/>\n",
    "\n",
    "- intuitively this makes sense\n",
    "  - if you move to the right by this distance, in expectation you're exactly where you wish to be but you've lost information because your motion tends to lose information as manifested by this uncertainty over here (in green)\n",
    "\n",
    "\n",
    "- the math for this is really, really easy\n",
    "  - your new mean is your old mean plus the motion, often called $u$\n",
    "    - $\\mu' \\leftarrow \\mu + u$\n",
    "    - if you move over 10 meters, $u$ will be 10 meters\n",
    "  - your new variance is your old variance plus a variance of the motion Gaussian $r^2$\n",
    "    - ${\\sigma^2}^\\prime \\leftarrow \\sigma^2 + r^2$\n",
    "\n",
    "\n",
    "- this is all you need to know; it's just an addition\n",
    "- in summary, we have a Gaussian over here (in blue), we have a Gaussian for the motion (in green), with $u$ as the mean and $r^2$ as its own motion uncertainty, and the resulting Gaussian in the prediction step (in red) just adds these two things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.0, 8.0]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def predict(mean1, var1, mean2, var2):\\n    new_mean = mean1 + mean2\\n    new_var = var1 + var2\\n    return [new_mean, new_var]\\n\\n\\nprint(predict(10.0, 4.0, 12.0, 4.0))\";\n",
       "                var nbb_formatted_code = \"def predict(mean1, var1, mean2, var2):\\n    new_mean = mean1 + mean2\\n    new_var = var1 + var2\\n    return [new_mean, new_var]\\n\\n\\nprint(predict(10.0, 4.0, 12.0, 4.0))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(mean1, var1, mean2, var2):\n",
    "    new_mean = mean1 + mean2\n",
    "    new_var = var1 + var2\n",
    "    return [new_mean, new_var]\n",
    "\n",
    "\n",
    "print(predict(10.0, 4.0, 12.0, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's put everything together\n",
    "- let's write a main program that takes these 2 functions, `update` and `predict` and feeds into a sequence of measurements and motions\n",
    "\n",
    "\n",
    "- in the example I've chosen the measurements are $5., 6., 7., 9., 10.$\n",
    "- the motions are $1., 1., 2., 1., 1.$\n",
    "- this all would work out really well if the initial estimate was $5$, but we're setting it to $0$ with a very large uncertainty of $10,000$\n",
    "- let's assume the measurement uncertainty is constant $4$ and the motion uncertainty is constant $2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update:  [4.998000799680128, 3.9984006397441023]\n",
      "predict:  [5.998000799680128, 5.998400639744102]\n",
      "update:  [5.999200191953932, 2.399744061425258]\n",
      "predict:  [6.999200191953932, 4.399744061425258]\n",
      "update:  [6.999619127420922, 2.0951800575117594]\n",
      "predict:  [8.999619127420921, 4.09518005751176]\n",
      "update:  [8.999811802788143, 2.0235152416216957]\n",
      "predict:  [9.999811802788143, 4.023515241621696]\n",
      "update:  [9.999906177177365, 2.0058615808441944]\n",
      "predict:  [10.999906177177365, 4.005861580844194]\n",
      "\n",
      "final:  [10.999906177177365, 4.005861580844194]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Write a program that will iteratively update and predict\\n# based on the location measurements and inferred motions shown below.\\n\\n\\ndef update(mean1, var1, mean2, var2):\\n    new_mean = float(var2 * mean1 + var1 * mean2) / (var1 + var2)\\n    new_var = 1.0 / (1.0 / var1 + 1.0 / var2)\\n    return [new_mean, new_var]\\n\\n\\ndef predict(mean1, var1, mean2, var2):\\n    new_mean = mean1 + mean2\\n    new_var = var1 + var2\\n    return [new_mean, new_var]\\n\\n\\nmeasurements = [5.0, 6.0, 7.0, 9.0, 10.0]\\nmotion = [1.0, 1.0, 2.0, 1.0, 1.0]\\nmeasurement_sig = 4.0\\nmotion_sig = 2.0\\nmu = 0.0\\nsig = 10000.0\\n\\nfor n in range(len(measurements)):\\n    [mu, sig] = update(mu, sig, measurements[n], measurement_sig)\\n    print(\\\"update: \\\", [mu, sig])\\n    [mu, sig] = predict(mu, sig, motion[n], motion_sig)\\n    print(\\\"predict: \\\", [mu, sig])\\n\\nprint(\\\"\\\\nfinal: \\\", [mu, sig])\";\n",
       "                var nbb_formatted_code = \"# Write a program that will iteratively update and predict\\n# based on the location measurements and inferred motions shown below.\\n\\n\\ndef update(mean1, var1, mean2, var2):\\n    new_mean = float(var2 * mean1 + var1 * mean2) / (var1 + var2)\\n    new_var = 1.0 / (1.0 / var1 + 1.0 / var2)\\n    return [new_mean, new_var]\\n\\n\\ndef predict(mean1, var1, mean2, var2):\\n    new_mean = mean1 + mean2\\n    new_var = var1 + var2\\n    return [new_mean, new_var]\\n\\n\\nmeasurements = [5.0, 6.0, 7.0, 9.0, 10.0]\\nmotion = [1.0, 1.0, 2.0, 1.0, 1.0]\\nmeasurement_sig = 4.0\\nmotion_sig = 2.0\\nmu = 0.0\\nsig = 10000.0\\n\\nfor n in range(len(measurements)):\\n    [mu, sig] = update(mu, sig, measurements[n], measurement_sig)\\n    print(\\\"update: \\\", [mu, sig])\\n    [mu, sig] = predict(mu, sig, motion[n], motion_sig)\\n    print(\\\"predict: \\\", [mu, sig])\\n\\nprint(\\\"\\\\nfinal: \\\", [mu, sig])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a program that will iteratively update and predict\n",
    "# based on the location measurements and inferred motions shown below.\n",
    "\n",
    "\n",
    "def update(mean1, var1, mean2, var2):\n",
    "    new_mean = float(var2 * mean1 + var1 * mean2) / (var1 + var2)\n",
    "    new_var = 1.0 / (1.0 / var1 + 1.0 / var2)\n",
    "    return [new_mean, new_var]\n",
    "\n",
    "\n",
    "def predict(mean1, var1, mean2, var2):\n",
    "    new_mean = mean1 + mean2\n",
    "    new_var = var1 + var2\n",
    "    return [new_mean, new_var]\n",
    "\n",
    "\n",
    "measurements = [5.0, 6.0, 7.0, 9.0, 10.0]\n",
    "motion = [1.0, 1.0, 2.0, 1.0, 1.0]\n",
    "measurement_sig = 4.0\n",
    "motion_sig = 2.0\n",
    "mu = 0.0\n",
    "sig = 10000.0\n",
    "\n",
    "for n in range(len(measurements)):\n",
    "    [mu, sig] = update(mu, sig, measurements[n], measurement_sig)\n",
    "    print(\"update: \", [mu, sig])\n",
    "    [mu, sig] = predict(mu, sig, motion[n], motion_sig)\n",
    "    print(\"predict: \", [mu, sig])\n",
    "\n",
    "print(\"\\nfinal: \", [mu, sig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when you run this, your first estimate for position should basically become $5$--$4.99$, and the reason is your initial uncertainty is so large, the estimate is dominated by the first measurement\n",
    "- your uncertainty shrinks to $3.99$, which is slightly better than the measurement uncertainty\n",
    "- you then predict that you add $1$, but the uncertainty increases to $5.99$, which is the motion uncertainty of $2$\n",
    "- you update again based on the measurement $6$, you get your estimate of $5.99$, which is almost $6$\n",
    "- you move $1$ again, you measure $7$\n",
    "- you move $2$, you measure $9$\n",
    "- you move $1$, you measure $10$\n",
    "- and you move a final $1$ and out comes as the final result\n",
    "  - a prediction of $10.99$ for the position, which is your $10$ position moved by $1$, and the uncertainty--residual uncertainty of $4$\n",
    "\n",
    "\n",
    "- this code that you just wrote implements a full Kalman filter for 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we understand a lot about the 1D Kalman filter; you've programmed one\n",
    "- you understand how to incorporate measurements, you understand how to incorporate motion\n",
    "- in reality, we often have many Ds, and then things become more involved, so I'm going to just tell you how things work with an example, and why it's great to estimate in higher dimensional state spaces\n",
    "\n",
    "\n",
    "- suppose you have a 2-dimensional state space of x and y--like a camera image, or in our case, we might have a car that uses a radar to detect the location of a vehicle over time\n",
    "- then what the 2D Kalman filter affords you is something really amazing, and here is how it goes\n",
    "\n",
    "\n",
    "- suppose at time t = 0, you observe the object of interest to be at this coordinate\n",
    "  - this might be another car in traffic for the Google self-driving car\n",
    "- one time step later, you see it over here, other time step later, you see it right over here\n",
    "- where would you now expect at time $t = 3$ the object to be?\n",
    "\n",
    "<img src=\"resources/kalman_prediction.png\"/>\n",
    "\n",
    "- we'd expect the car to continue in a straight line\n",
    "- what the Kalman filter does for you, if you do estimation and higher dimensional spaces, is to not just go into x and y spaces, but allows you to implicitly figure out what the velocity of the object is, and then use the velocity estimate to make a really good prediction about the future\n",
    "\n",
    "\n",
    "- notice the sensor itself only sees position; it never sees the actual velocity\n",
    "- velocity is inferred from seeing multiple positions\n",
    "- one of the most amazing things about Kalman filters in tracking applications is it's able to figure out, even though it never directly measures it, the velocity of the object, and from there is able to make predictions about future locations that incorporate velocity\n",
    "- that's one of the reasons that Kalman filters are such a popular algorithm in artificial intelligence and in control theory at large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Land"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to explain how this works, I have to talk about high dimesional Gaussians\n",
    "  - these are often called multivariate Gaussians\n",
    "  - the mean is now a vector with 1 element for each of the dimensions\n",
    "  - the variance here is replaced by what's called a co-variance, and it's a matrix with D rows and D columns, if the dimensionality of the estimate is D\n",
    "  - the formula is something you have to get used to; to tell you the truth, even I have to look up the formula for this class, so I don't have it in my head, and please, don't get confused\n",
    "\n",
    "\n",
    "- let me explain it to you more intuitively\n",
    "- here's a 2-dimensional space\n",
    "- a 2-dimensional Gaussian is defined over that space, and it's possible to draw the contour lines of the Gaussian\n",
    "\n",
    "<img src=\"resources/2d_gaussian.png\"/>\n",
    "\n",
    "- the mean of this Gaussian is this $x0, y0$ pair,\n",
    "- the co-variance now defines the spread of the Gaussian as indicated by these contour lines\n",
    "  - it might be possible to have a fairly small uncertainty in one dimension, but a huge uncertainty in the other\n",
    "- when the Gaussian is tilted as showed over here, then the uncertainty of x and y is correlated, which means if I get information about x--it actually sits over here--that would make me believe that y probably sits somewhere over here\n",
    "  - that's called correlation\n",
    "\n",
    "\n",
    "- I can explain to you the entire effect of estimating velocity and using it in filtering using Gaussians like these, and it becomes really simple\n",
    "- the problem I'm going to choose is a 1-dimensional motion example\n",
    "  - let's assume a $t = 1$, a $t = 2$, and a $t = 3$ like in the image\n",
    "  - you would assume that at $t = 4$, the object sits over here, and the reason why you would assume this is--even though it's just seen these different discrete locations, you can infer from it there is actually velocity that drives the object to the right side to the point\n",
    "\n",
    "<img src=\"resources/estimation_velocity.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Kalman filter land, we're going to build a 2-dimensional estimate\n",
    "  - one for the location $x$, and one for the velocity denoted $\\dot{x}$\n",
    "  - the velocity can be $0$, it can be negative, or it can be positive\n",
    "\n",
    "\n",
    "- if initially I know my location, but not my velocity, then I represent it with a Gaussian that's elevated around the correct location, but really, really broad in the space of velocities\n",
    "- let's look at the prediction step\n",
    "  - in the prediction step, I don't know my velocity, so I can't possibly predict for location; I'm going to assume\n",
    "  - but miraculously, there'll be some interesting correlation\n",
    "  - so let's for a second, just pick a point on this distribution over here\n",
    "    - let me assume my velocity is $0$ (of course, in practice, I don't know the velocity, but let me assume for a moment the velocity is $0$)\n",
    "    - where would my posterior be after the prediction?\n",
    "      - well, we know we started in location $1$, the velocity is $0$, so my location would likely be at $(1,0)$\n",
    "  - now let's change my belief in velocity and pick a different one\n",
    "    - let's say the velocity is $1$\n",
    "    - where would my prediction be $1$ time step later starting at location $1$ and velocity $1$?\n",
    "      - the answer is at $(2, 1)$\n",
    "      - if we advance by one time step, we should also move forward in the x direction by one\n",
    "      - if a car's starting point is the point $(1, 1)$, for which we know the location is $1$, and the velocity is $1$, and if we predict $1$ time step in the future, then for that prediction, we know the location will be $2$, and the velocity might be a little uncertain, but it stays about the same \n",
    "\n",
    "<img src=\"resources/kalman_filter_prediction.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/kalman_filters_prediction_multiple_gaussian.png\"/>\n",
    "\n",
    "- when you put all this together, you find that all these possibilites on the Gaussian over here (blue), link to a Gaussian that looks like this (red)\n",
    "  - this is a really interesting 2-dimensional Gaussian, which you should really think about\n",
    "  - clearly, if I were to project this Gaussian uncertainty into the space of possible locations, I can't predict a thing\n",
    "  - it's impossible to predict where the object is; the reason is, I don't know the velocity\n",
    "  - also, clearly if I project this Gaussian into the space of $\\dot{x}$ it is impossible to say what the velocity is\n",
    "  - a single observation or single prediction is insufficient to make that observation\n",
    "  - however, what we know is our location is correlated to the velocity\n",
    "    - the faster I move, the further on the right is the location; this Gaussian (red) expresses this\n",
    "      - if I, for example, figure out that my velocity was $2$, then I was able, under this Gaussian, to really nail that my location is $3$\n",
    "    - we still haven't figured out where we are, and we haven't figured out how fast we're moving, but we've learned so much about the relation of these two things with this tilted Gaussian\n",
    "\n",
    "\n",
    "- to understand how powerful this is, let's now fold in the second observation at time $t = 2$\n",
    "  - this observation tells us nothing about the velocity and only something about the location\n",
    "  - if I were to draw this as a Gaussian--it's a Gaussian just like this (green), which says something about the location but not about the velocity\n",
    "  - but if we multiply my prior (red) from the prediction step with the measurement probability (green), then miraculously, I get a Gaussian that sits at $(2, 1)$\n",
    "    - this Gaussian now has a really good estimate what my velocity is and a really good estimate where I am\n",
    "      - if I take this Gaussian, and predict $1$ step forward, then I find myself at $(3, 2)$\n",
    "\n",
    "\n",
    "- think about this; this is a really deep insight about how Kalman filters work\n",
    "- in particular, we've only been able to *observe* one variable, $x$ and we've been able to measure observation to *infer* this other variable, $\\dot{x}$\n",
    "  - the way we've been able to infer is that there's a set of physical equations which say that my location, after a times step, is my old location plus my velocity $x' = x + \\dot{x}$\n",
    "    - this has been able to propagate constraints from subsequent measurements back to this unobservable variable, velocity, so we are able to estimate the velocity as well\n",
    "    - this is really key to understanding Kalman filter; it is key to understanding how a Google self-driving car, estimates the locations of other cars, and is able to make predictions even if it's unable to measure velocity directly\n",
    "\n",
    "\n",
    " - the variables of a Kalman filter--they're often called *states* because they reflect states of the physical world like where the other car is and how fast it's moving.\n",
    "  - they separate into 2 subsets\n",
    "    - the observables, like the momentary location\n",
    "    - the hidden, which in our example is the velocity, which I can never directly observe\n",
    "  - because those 2 things interact, subsequent observations of the observable variables give us information about these hidden variables, so we can also estimate what these hidden variables are\n",
    "  - from multiple observations of the places of the object--the location--we can estimate how fast it's moving\n",
    "    - that is actually true for all of the different filters but because Kalman filters happen to be very efficient to calculate, when we have a problem like this, you tend to often use just a Kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when we design a Kalman filter, you need effectively 2 things\n",
    "  - we know that the new location is the old location + velocity, $x' \\leftarrow x + \\dot{x}$\n",
    "  - the new velocity should just be the old velocity, $\\dot{x'} \\leftarrow \\dot{x}$\n",
    "  \n",
    "  \n",
    "- for the state, you need a state transition function, and that's usually a matrix, so we're now in the world of linear algebra\n",
    "  - $\\begin{pmatrix} x' \\\\ \\dot{x'} \\end{pmatrix} \\leftarrow \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\dot{x} \\end{pmatrix}$\n",
    "      - matrix with 1s and 0s would be called $F$\n",
    "- for the measurements, you need a measurement function\n",
    "  - we only observe the first component of the place, not velocity, and that uses a matrix like this\n",
    "  - $z \\leftarrow \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\dot{x} \\end{pmatrix}$\n",
    "      - matrix with 1s and 0s would be called $H$\n",
    "\n",
    "\n",
    "- the actual update equations for a Kalman filter are involved, and I give them to you, but please, don't memorize them, and I won't prove them for you even the proof is very involved\n",
    "  - variables\n",
    "    - x = estimate\n",
    "    - P = uncertainity covariance\n",
    "    - F = state transition matrix\n",
    "    - u = motion vector\n",
    "    - z = measurement\n",
    "    - H = measurement function\n",
    "    - y = error\n",
    "    - R = measurement noise\n",
    "    - S = matrix which is obtained by projecting the system uncertainty into the measurement space using the measurement function projection\n",
    "    - K = Kalman gain\n",
    "  - prediction\n",
    "    - $x' = F \\cdot x + u$\n",
    "    - $P' = F \\cdot P \\cdot F^T$\n",
    "  - measurement update\n",
    "    - $y = z - H \\cdot x$\n",
    "    - $S = H \\cdot P \\cdot H^T + R$\n",
    "    - $K = P \\cdot H^T \\cdot S^{-1}$        \n",
    "    - update the estimate: $x' = x + (K \\cdot y)$ \n",
    "    - update the uncertainty: $P' = (I - K \\cdot H) \\cdot P$\n",
    "\n",
    "\n",
    "- I wrote this down so that you have a complete definition, but this is something you should not memorize\n",
    "- if you really wish to understand this math, it happens to be just a generalization of the math I gave you to higher dimensional spaces, but I would recommend just not to worry about this\n",
    "- there's a set of linear algebra equations that implement the Kalman filter in higher dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have a new, challenging programming assignment for you; I would like you to implement a multidimensional Kalman filter for the example I've just given you\n",
    "- the matrix class is a class for manipulating matrices that should make it really easy\n",
    "  - it has a function that initializes matrices\n",
    "  - it can set them down to $0$\n",
    "  - it can compute an identity matrix\n",
    "  - it can print out a matrix with show\n",
    "  - it overloads operators like addition, subtraction, multiplication\n",
    "  - it even computes the transpose of a matrix\n",
    "  - it can invert a matrix using Cholesky factorization\n",
    "- this matrix class is a small version of what is found in typical libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[3.9996664447958645], [0.9999998335552873]], [[2.3318904241194827, 0.9991676099921091], [0.9991676099921067, 0.49950058263974184]])\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Write a function 'kalman_filter' that implements a multi-\\n# dimensional Kalman Filter for the example given\\n\\nfrom math import *\\n\\n\\nclass matrix:\\n\\n    # implements basic operations of a matrix class\\n\\n    def __init__(self, value):\\n        self.value = value\\n        self.dimx = len(value)\\n        self.dimy = len(value[0])\\n        if value == [[]]:\\n            self.dimx = 0\\n\\n    def zero(self, dimx, dimy):\\n        # check if valid dimensions\\n        if dimx < 1 or dimy < 1:\\n            raise ValueError(\\\"Invalid size of matrix\\\")\\n        else:\\n            self.dimx = dimx\\n            self.dimy = dimy\\n            self.value = [[0 for row in range(dimy)] for col in range(dimx)]\\n\\n    def identity(self, dim):\\n        # check if valid dimension\\n        if dim < 1:\\n            raise ValueError(\\\"Invalid size of matrix\\\")\\n        else:\\n            self.dimx = dim\\n            self.dimy = dim\\n            self.value = [[0 for row in range(dim)] for col in range(dim)]\\n            for i in range(dim):\\n                self.value[i][i] = 1\\n\\n    def show(self):\\n        for i in range(self.dimx):\\n            print(self.value[i])\\n        print(\\\" \\\")\\n\\n    def __add__(self, other):\\n        # check if correct dimensions\\n        if self.dimx != other.dimx or self.dimy != other.dimy:\\n            raise ValueError(\\\"Matrices must be of equal dimensions to add\\\")\\n        else:\\n            # add if correct dimensions\\n            res = matrix([[]])\\n            res.zero(self.dimx, self.dimy)\\n            for i in range(self.dimx):\\n                for j in range(self.dimy):\\n                    res.value[i][j] = self.value[i][j] + other.value[i][j]\\n            return res\\n\\n    def __sub__(self, other):\\n        # check if correct dimensions\\n        if self.dimx != other.dimx or self.dimy != other.dimy:\\n            raise ValueError(\\\"Matrices must be of equal dimensions to subtract\\\")\\n        else:\\n            # subtract if correct dimensions\\n            res = matrix([[]])\\n            res.zero(self.dimx, self.dimy)\\n            for i in range(self.dimx):\\n                for j in range(self.dimy):\\n                    res.value[i][j] = self.value[i][j] - other.value[i][j]\\n            return res\\n\\n    def __mul__(self, other):\\n        # check if correct dimensions\\n        if self.dimy != other.dimx:\\n            raise ValueError(\\\"Matrices must be m*n and n*p to multiply\\\")\\n        else:\\n            # multiply if correct dimensions\\n            res = matrix([[]])\\n            res.zero(self.dimx, other.dimy)\\n            for i in range(self.dimx):\\n                for j in range(other.dimy):\\n                    for k in range(self.dimy):\\n                        res.value[i][j] += self.value[i][k] * other.value[k][j]\\n            return res\\n\\n    def transpose(self):\\n        # compute transpose\\n        res = matrix([[]])\\n        res.zero(self.dimy, self.dimx)\\n        for i in range(self.dimx):\\n            for j in range(self.dimy):\\n                res.value[j][i] = self.value[i][j]\\n        return res\\n\\n    # Thanks to Ernesto P. Adorio for use of Cholesky and CholeskyInverse functions\\n\\n    def Cholesky(self, ztol=1.0e-5):\\n        # Computes the upper triangular Cholesky factorization of\\n        # a positive definite matrix.\\n        res = matrix([[]])\\n        res.zero(self.dimx, self.dimx)\\n\\n        for i in range(self.dimx):\\n            S = sum([(res.value[k][i]) ** 2 for k in range(i)])\\n            d = self.value[i][i] - S\\n            if abs(d) < ztol:\\n                res.value[i][i] = 0.0\\n            else:\\n                if d < 0.0:\\n                    raise ValueError(\\\"Matrix not positive-definite\\\")\\n                res.value[i][i] = sqrt(d)\\n            for j in range(i + 1, self.dimx):\\n                S = sum([res.value[k][i] * res.value[k][j] for k in range(self.dimx)])\\n                if abs(S) < ztol:\\n                    S = 0.0\\n                try:\\n                    res.value[i][j] = (self.value[i][j] - S) / res.value[i][i]\\n                except:\\n                    raise ValueError(\\\"Zero diagonal\\\")\\n        return res\\n\\n    def CholeskyInverse(self):\\n        # Computes inverse of matrix given its Cholesky upper Triangular\\n        # decomposition of matrix.\\n        res = matrix([[]])\\n        res.zero(self.dimx, self.dimx)\\n\\n        # Backward step for inverse.\\n        for j in reversed(range(self.dimx)):\\n            tjj = self.value[j][j]\\n            S = sum(\\n                [self.value[j][k] * res.value[j][k] for k in range(j + 1, self.dimx)]\\n            )\\n            res.value[j][j] = 1.0 / tjj ** 2 - S / tjj\\n            for i in reversed(range(j)):\\n                res.value[j][i] = res.value[i][j] = (\\n                    -sum(\\n                        [\\n                            self.value[i][k] * res.value[k][j]\\n                            for k in range(i + 1, self.dimx)\\n                        ]\\n                    )\\n                    / self.value[i][i]\\n                )\\n        return res\\n\\n    def inverse(self):\\n        aux = self.Cholesky()\\n        res = aux.CholeskyInverse()\\n        return res\\n\\n    def __repr__(self):\\n        return repr(self.value)\\n\\n\\n########################################\\n\\n# Implement the filter function below\\n\\n\\ndef kalman_filter(x, P):\\n    for n in range(len(measurements)):\\n\\n        # measurement update\\n        Z = matrix([[measurements[n]]])\\n        y = Z - (H * x)\\n        S = H * P * H.transpose() + R\\n        K = P * H.transpose() * S.inverse()\\n        x = x + (K * y)\\n\\n        P = (I - (K * H)) * P\\n\\n        # prediction\\n        x = (F * x) + u\\n        P = F * P * F.transpose()\\n\\n    return x, P\\n\\n\\n############################################\\n### use the code below to test your filter!\\n############################################\\n\\nmeasurements = [1, 2, 3]  # filter with these measurements\\n\\nx = matrix([[0.0], [0.0]])  # initial state (location and velocity)\\nP = matrix([[1000.0, 0.0], [0.0, 1000.0]])  # initial uncertainty\\nu = matrix([[0.0], [0.0]])  # external motion\\nF = matrix([[1.0, 1.0], [0, 1.0]])  # next state function\\nH = matrix([[1.0, 0.0]])  # measurement function\\nR = matrix([[1.0]])  # measurement uncertainty\\nI = matrix([[1.0, 0.0], [0.0, 1.0]])  # identity matrix\\n\\nprint(kalman_filter(x, P))\\n# output should be:\\n# x: [[3.9996664447958645], [0.9999998335552873]]\\n# P: [[2.3318904241194827, 0.9991676099921091], [0.9991676099921067, 0.49950058263974184]]\";\n",
       "                var nbb_formatted_code = \"# Write a function 'kalman_filter' that implements a multi-\\n# dimensional Kalman Filter for the example given\\n\\nfrom math import *\\n\\n\\nclass matrix:\\n\\n    # implements basic operations of a matrix class\\n\\n    def __init__(self, value):\\n        self.value = value\\n        self.dimx = len(value)\\n        self.dimy = len(value[0])\\n        if value == [[]]:\\n            self.dimx = 0\\n\\n    def zero(self, dimx, dimy):\\n        # check if valid dimensions\\n        if dimx < 1 or dimy < 1:\\n            raise ValueError(\\\"Invalid size of matrix\\\")\\n        else:\\n            self.dimx = dimx\\n            self.dimy = dimy\\n            self.value = [[0 for row in range(dimy)] for col in range(dimx)]\\n\\n    def identity(self, dim):\\n        # check if valid dimension\\n        if dim < 1:\\n            raise ValueError(\\\"Invalid size of matrix\\\")\\n        else:\\n            self.dimx = dim\\n            self.dimy = dim\\n            self.value = [[0 for row in range(dim)] for col in range(dim)]\\n            for i in range(dim):\\n                self.value[i][i] = 1\\n\\n    def show(self):\\n        for i in range(self.dimx):\\n            print(self.value[i])\\n        print(\\\" \\\")\\n\\n    def __add__(self, other):\\n        # check if correct dimensions\\n        if self.dimx != other.dimx or self.dimy != other.dimy:\\n            raise ValueError(\\\"Matrices must be of equal dimensions to add\\\")\\n        else:\\n            # add if correct dimensions\\n            res = matrix([[]])\\n            res.zero(self.dimx, self.dimy)\\n            for i in range(self.dimx):\\n                for j in range(self.dimy):\\n                    res.value[i][j] = self.value[i][j] + other.value[i][j]\\n            return res\\n\\n    def __sub__(self, other):\\n        # check if correct dimensions\\n        if self.dimx != other.dimx or self.dimy != other.dimy:\\n            raise ValueError(\\\"Matrices must be of equal dimensions to subtract\\\")\\n        else:\\n            # subtract if correct dimensions\\n            res = matrix([[]])\\n            res.zero(self.dimx, self.dimy)\\n            for i in range(self.dimx):\\n                for j in range(self.dimy):\\n                    res.value[i][j] = self.value[i][j] - other.value[i][j]\\n            return res\\n\\n    def __mul__(self, other):\\n        # check if correct dimensions\\n        if self.dimy != other.dimx:\\n            raise ValueError(\\\"Matrices must be m*n and n*p to multiply\\\")\\n        else:\\n            # multiply if correct dimensions\\n            res = matrix([[]])\\n            res.zero(self.dimx, other.dimy)\\n            for i in range(self.dimx):\\n                for j in range(other.dimy):\\n                    for k in range(self.dimy):\\n                        res.value[i][j] += self.value[i][k] * other.value[k][j]\\n            return res\\n\\n    def transpose(self):\\n        # compute transpose\\n        res = matrix([[]])\\n        res.zero(self.dimy, self.dimx)\\n        for i in range(self.dimx):\\n            for j in range(self.dimy):\\n                res.value[j][i] = self.value[i][j]\\n        return res\\n\\n    # Thanks to Ernesto P. Adorio for use of Cholesky and CholeskyInverse functions\\n\\n    def Cholesky(self, ztol=1.0e-5):\\n        # Computes the upper triangular Cholesky factorization of\\n        # a positive definite matrix.\\n        res = matrix([[]])\\n        res.zero(self.dimx, self.dimx)\\n\\n        for i in range(self.dimx):\\n            S = sum([(res.value[k][i]) ** 2 for k in range(i)])\\n            d = self.value[i][i] - S\\n            if abs(d) < ztol:\\n                res.value[i][i] = 0.0\\n            else:\\n                if d < 0.0:\\n                    raise ValueError(\\\"Matrix not positive-definite\\\")\\n                res.value[i][i] = sqrt(d)\\n            for j in range(i + 1, self.dimx):\\n                S = sum([res.value[k][i] * res.value[k][j] for k in range(self.dimx)])\\n                if abs(S) < ztol:\\n                    S = 0.0\\n                try:\\n                    res.value[i][j] = (self.value[i][j] - S) / res.value[i][i]\\n                except:\\n                    raise ValueError(\\\"Zero diagonal\\\")\\n        return res\\n\\n    def CholeskyInverse(self):\\n        # Computes inverse of matrix given its Cholesky upper Triangular\\n        # decomposition of matrix.\\n        res = matrix([[]])\\n        res.zero(self.dimx, self.dimx)\\n\\n        # Backward step for inverse.\\n        for j in reversed(range(self.dimx)):\\n            tjj = self.value[j][j]\\n            S = sum(\\n                [self.value[j][k] * res.value[j][k] for k in range(j + 1, self.dimx)]\\n            )\\n            res.value[j][j] = 1.0 / tjj ** 2 - S / tjj\\n            for i in reversed(range(j)):\\n                res.value[j][i] = res.value[i][j] = (\\n                    -sum(\\n                        [\\n                            self.value[i][k] * res.value[k][j]\\n                            for k in range(i + 1, self.dimx)\\n                        ]\\n                    )\\n                    / self.value[i][i]\\n                )\\n        return res\\n\\n    def inverse(self):\\n        aux = self.Cholesky()\\n        res = aux.CholeskyInverse()\\n        return res\\n\\n    def __repr__(self):\\n        return repr(self.value)\\n\\n\\n########################################\\n\\n# Implement the filter function below\\n\\n\\ndef kalman_filter(x, P):\\n    for n in range(len(measurements)):\\n\\n        # measurement update\\n        Z = matrix([[measurements[n]]])\\n        y = Z - (H * x)\\n        S = H * P * H.transpose() + R\\n        K = P * H.transpose() * S.inverse()\\n        x = x + (K * y)\\n\\n        P = (I - (K * H)) * P\\n\\n        # prediction\\n        x = (F * x) + u\\n        P = F * P * F.transpose()\\n\\n    return x, P\\n\\n\\n############################################\\n### use the code below to test your filter!\\n############################################\\n\\nmeasurements = [1, 2, 3]  # filter with these measurements\\n\\nx = matrix([[0.0], [0.0]])  # initial state (location and velocity)\\nP = matrix([[1000.0, 0.0], [0.0, 1000.0]])  # initial uncertainty\\nu = matrix([[0.0], [0.0]])  # external motion\\nF = matrix([[1.0, 1.0], [0, 1.0]])  # next state function\\nH = matrix([[1.0, 0.0]])  # measurement function\\nR = matrix([[1.0]])  # measurement uncertainty\\nI = matrix([[1.0, 0.0], [0.0, 1.0]])  # identity matrix\\n\\nprint(kalman_filter(x, P))\\n# output should be:\\n# x: [[3.9996664447958645], [0.9999998335552873]]\\n# P: [[2.3318904241194827, 0.9991676099921091], [0.9991676099921067, 0.49950058263974184]]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a function 'kalman_filter' that implements a multi-\n",
    "# dimensional Kalman Filter for the example given\n",
    "\n",
    "from math import *\n",
    "\n",
    "\n",
    "class matrix:\n",
    "\n",
    "    # implements basic operations of a matrix class\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.dimx = len(value)\n",
    "        self.dimy = len(value[0])\n",
    "        if value == [[]]:\n",
    "            self.dimx = 0\n",
    "\n",
    "    def zero(self, dimx, dimy):\n",
    "        # check if valid dimensions\n",
    "        if dimx < 1 or dimy < 1:\n",
    "            raise ValueError(\"Invalid size of matrix\")\n",
    "        else:\n",
    "            self.dimx = dimx\n",
    "            self.dimy = dimy\n",
    "            self.value = [[0 for row in range(dimy)] for col in range(dimx)]\n",
    "\n",
    "    def identity(self, dim):\n",
    "        # check if valid dimension\n",
    "        if dim < 1:\n",
    "            raise ValueError(\"Invalid size of matrix\")\n",
    "        else:\n",
    "            self.dimx = dim\n",
    "            self.dimy = dim\n",
    "            self.value = [[0 for row in range(dim)] for col in range(dim)]\n",
    "            for i in range(dim):\n",
    "                self.value[i][i] = 1\n",
    "\n",
    "    def show(self):\n",
    "        for i in range(self.dimx):\n",
    "            print(self.value[i])\n",
    "        print(\" \")\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # check if correct dimensions\n",
    "        if self.dimx != other.dimx or self.dimy != other.dimy:\n",
    "            raise ValueError(\"Matrices must be of equal dimensions to add\")\n",
    "        else:\n",
    "            # add if correct dimensions\n",
    "            res = matrix([[]])\n",
    "            res.zero(self.dimx, self.dimy)\n",
    "            for i in range(self.dimx):\n",
    "                for j in range(self.dimy):\n",
    "                    res.value[i][j] = self.value[i][j] + other.value[i][j]\n",
    "            return res\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # check if correct dimensions\n",
    "        if self.dimx != other.dimx or self.dimy != other.dimy:\n",
    "            raise ValueError(\"Matrices must be of equal dimensions to subtract\")\n",
    "        else:\n",
    "            # subtract if correct dimensions\n",
    "            res = matrix([[]])\n",
    "            res.zero(self.dimx, self.dimy)\n",
    "            for i in range(self.dimx):\n",
    "                for j in range(self.dimy):\n",
    "                    res.value[i][j] = self.value[i][j] - other.value[i][j]\n",
    "            return res\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # check if correct dimensions\n",
    "        if self.dimy != other.dimx:\n",
    "            raise ValueError(\"Matrices must be m*n and n*p to multiply\")\n",
    "        else:\n",
    "            # multiply if correct dimensions\n",
    "            res = matrix([[]])\n",
    "            res.zero(self.dimx, other.dimy)\n",
    "            for i in range(self.dimx):\n",
    "                for j in range(other.dimy):\n",
    "                    for k in range(self.dimy):\n",
    "                        res.value[i][j] += self.value[i][k] * other.value[k][j]\n",
    "            return res\n",
    "\n",
    "    def transpose(self):\n",
    "        # compute transpose\n",
    "        res = matrix([[]])\n",
    "        res.zero(self.dimy, self.dimx)\n",
    "        for i in range(self.dimx):\n",
    "            for j in range(self.dimy):\n",
    "                res.value[j][i] = self.value[i][j]\n",
    "        return res\n",
    "\n",
    "    # Thanks to Ernesto P. Adorio for use of Cholesky and CholeskyInverse functions\n",
    "\n",
    "    def Cholesky(self, ztol=1.0e-5):\n",
    "        # Computes the upper triangular Cholesky factorization of\n",
    "        # a positive definite matrix.\n",
    "        res = matrix([[]])\n",
    "        res.zero(self.dimx, self.dimx)\n",
    "\n",
    "        for i in range(self.dimx):\n",
    "            S = sum([(res.value[k][i]) ** 2 for k in range(i)])\n",
    "            d = self.value[i][i] - S\n",
    "            if abs(d) < ztol:\n",
    "                res.value[i][i] = 0.0\n",
    "            else:\n",
    "                if d < 0.0:\n",
    "                    raise ValueError(\"Matrix not positive-definite\")\n",
    "                res.value[i][i] = sqrt(d)\n",
    "            for j in range(i + 1, self.dimx):\n",
    "                S = sum([res.value[k][i] * res.value[k][j] for k in range(self.dimx)])\n",
    "                if abs(S) < ztol:\n",
    "                    S = 0.0\n",
    "                try:\n",
    "                    res.value[i][j] = (self.value[i][j] - S) / res.value[i][i]\n",
    "                except:\n",
    "                    raise ValueError(\"Zero diagonal\")\n",
    "        return res\n",
    "\n",
    "    def CholeskyInverse(self):\n",
    "        # Computes inverse of matrix given its Cholesky upper Triangular\n",
    "        # decomposition of matrix.\n",
    "        res = matrix([[]])\n",
    "        res.zero(self.dimx, self.dimx)\n",
    "\n",
    "        # Backward step for inverse.\n",
    "        for j in reversed(range(self.dimx)):\n",
    "            tjj = self.value[j][j]\n",
    "            S = sum(\n",
    "                [self.value[j][k] * res.value[j][k] for k in range(j + 1, self.dimx)]\n",
    "            )\n",
    "            res.value[j][j] = 1.0 / tjj ** 2 - S / tjj\n",
    "            for i in reversed(range(j)):\n",
    "                res.value[j][i] = res.value[i][j] = (\n",
    "                    -sum(\n",
    "                        [\n",
    "                            self.value[i][k] * res.value[k][j]\n",
    "                            for k in range(i + 1, self.dimx)\n",
    "                        ]\n",
    "                    )\n",
    "                    / self.value[i][i]\n",
    "                )\n",
    "        return res\n",
    "\n",
    "    def inverse(self):\n",
    "        aux = self.Cholesky()\n",
    "        res = aux.CholeskyInverse()\n",
    "        return res\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "# Implement the filter function below\n",
    "\n",
    "\n",
    "def kalman_filter(x, P):\n",
    "    for n in range(len(measurements)):\n",
    "\n",
    "        # measurement update\n",
    "        Z = matrix([[measurements[n]]])\n",
    "        y = Z - (H * x)\n",
    "        S = H * P * H.transpose() + R\n",
    "        K = P * H.transpose() * S.inverse()\n",
    "        x = x + (K * y)\n",
    "\n",
    "        P = (I - (K * H)) * P\n",
    "\n",
    "        # prediction\n",
    "        x = (F * x) + u\n",
    "        P = F * P * F.transpose()\n",
    "\n",
    "    return x, P\n",
    "\n",
    "\n",
    "############################################\n",
    "### use the code below to test your filter!\n",
    "############################################\n",
    "\n",
    "measurements = [1, 2, 3]  # filter with these measurements\n",
    "\n",
    "x = matrix([[0.0], [0.0]])  # initial state (location and velocity)\n",
    "P = matrix([[1000.0, 0.0], [0.0, 1000.0]])  # initial uncertainty\n",
    "u = matrix([[0.0], [0.0]])  # external motion\n",
    "F = matrix([[1.0, 1.0], [0, 1.0]])  # next state function\n",
    "H = matrix([[1.0, 0.0]])  # measurement function\n",
    "R = matrix([[1.0]])  # measurement uncertainty\n",
    "I = matrix([[1.0, 0.0], [0.0, 1.0]])  # identity matrix\n",
    "\n",
    "print(kalman_filter(x, P))\n",
    "# output should be:\n",
    "# x: [[3.9996664447958645], [0.9999998335552873]]\n",
    "# P: [[2.3318904241194827, 0.9991676099921091], [0.9991676099921067, 0.49950058263974184]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you really understood something fundamental here that I believe is really essential to artificial intelligence and to building self-driving cars\n",
    "- you implemented effectively our method for finding other cars\n",
    "- let me put this in context\n",
    "  - here's a Google self-driving car and here's another car\n",
    "  - our Google self-driving car uses radar on the front bumper that measures the distance to vehicles and also gives a noisy estimate of the velocity\n",
    "  - and it also uses its lasers, and again, it measures the distance to other cars but no velocities\n",
    "- if you take the same situation from above, here is the Google car; it is localized on a map and here are other vehicles\n",
    "  - using radars and lasers, the Google car estimates the distance and the velocity of all these vehicles, and it does so using a Kalman filter \n",
    "  - it feeds in range data from the laser, and it uses state spaces like this one of the relative distance in x and y and the relative velocity in x and y to get state transition matrices of the type I've just shown you to find out where these other cars are\n",
    "    - this is exactly what you've just learned and programmed yourself\n",
    "\n",
    "<img src=\"resources/kalman_filters_example_conclusion.png\"/>\n",
    "\n",
    "- I didn't tell you how to extract the location of other cars from radar and laser\n",
    "  - there's something called a correspondence problem\n",
    "  - sometimes you don't know which one each car is, and I won't talk in much depth about it\n",
    "- but you understand the gist of the solution now, and you've been able to program it\n",
    "\n",
    "\n",
    "- if you were in a situation like this, you can use range data like laser data and radar data and come up with a rational algorithm that takes momentary measurements of other cars and not just estimates where they are but also how fast they're moving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this completes my unit on Kalman filters\n",
    "- you learned about Gaussians, how to do measurement updates using multiplication, how to do prediction or state transitions using convolution, and you even implemented your first Kalman filter\n",
    "- you've implemented it in the context of vehicle tracking, and you used this to estimate a nonobservable velocity for measurement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
