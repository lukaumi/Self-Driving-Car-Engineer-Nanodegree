{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you're going to learn about behavioral cloning, but more importantly, you're going to play with simulation\n",
    "- you're going to drive a car through simulating yourself, and then you make a neural network copy you\n",
    "- the neural network will watch you--your behavior\n",
    "- the task for the neural network is to pick up your driving skill, and copy it and do the same\n",
    "- there's two ways to fail\n",
    "  - first is you know a network fails\n",
    "  - second is that you are a lousy driver--then find somebody else to drive for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  we encourage you to drive the vehicle in training mode and collect your own training data, but we have also included [sample driving data](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip) for the first track, which you can optionally use to train your network\n",
    "- you may need to collect additional data in order to get the vehicle to stay on the road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here are the latest updates to the simulator:\n",
    "  - steering is controlled via position mouse instead of keyboard\n",
    "    - this creates better angles for training--note the angle is based on the mouse distance\n",
    "    - to steer, hold the left mouse button and move left or right\n",
    "    - to reset the angle to 0 simply lift your finger off the left mouse button\n",
    "  - you can toggle record by pressing R, previously you had to click the record button (you can still do that)\n",
    "  - when recording is finished, saves all the captured images to disk at the same time instead of trying to save them while the car is still driving periodically\n",
    "    - you can see a save status and play back of the captured data\n",
    "  - you can takeover in autonomous mode\n",
    "    - while W or S are held down you can control the car the same way you would in training mode\n",
    "    - this can be helpful for debugging\n",
    "    - as soon as W or S are let go, autonomous takes over again\n",
    "  - pressing the spacebar in training mode toggles on and off cruise control (effectively presses W for you)\n",
    "  - added a Control screen\n",
    "  - track 2 was replaced from a mountain theme to Jungle with free assets; note the track is challenging\n",
    "  - you can use brake input in `drive.py` by issuing negative throttle values\n",
    "\n",
    "\n",
    "- if you are interested here is the source code for the [simulator repository](https://github.com/udacity/self-driving-car-sim)\n",
    "\n",
    "\n",
    "- when you first run the simulator, you’ll see a configuration screen asking what size and graphical quality you would like\n",
    "- we suggest running at the smallest size and the fastest graphical quality\n",
    "- we also suggest closing most other applications (especially graphically intensive applications) on your computer, so that your machine can devote its resources to running the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the next screen gives you two options: Training Mode and Autonomous Mode\n",
    "- select Training Mode\n",
    "  - the simulator will load and you will be able to drive the car like it’s a video game\n",
    "- you'll use autonomous mode in a later step, after you've used the data you collect here to train your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for the behavioral cloning project, you only need to work with this first track\n",
    "- but if you want to give yourself an extra challenge, you can also train your vehicle to drive on the second track\n",
    "- steering with the mouse actually outputs the best steering measurements and the training data\n",
    "\n",
    "\n",
    "- in order to start collecting training data, you'll need to do the following:\n",
    "  - enter Training Mode in the simulator\n",
    "  - start driving the car to get a feel for the controls\n",
    "  - when you are ready, hit the record button in the top right to start recording\n",
    "  - continue driving for a few laps or till you feel like you have enough data\n",
    "  - hit the record button in the top right again to stop recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- some of the interesting features of the track will also present challenges for training an end-to-end driving model\n",
    "  - for example, the model will have to learn how to handle these sharp turns\n",
    "  - this bridge has a different texture and different borders than the other parts of the road\n",
    "  - the dirt border on the edge of the track here is different than the curbs that lined the rest of the track--that could be tricky for the model to learn\n",
    "  - this sequence of sharp turns is challenging to drive even by hand--the neural network could easily lose control here and drive the car into the water\n",
    "- once we've gotten past this second dirt border, we've completed about one lap of driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so that the car drives down the center of the road, it's essential to capture center lane driving\n",
    "- try driving around the track various times while staying as close to the middle of the track as possible even when making turns\n",
    "- in the real world, the car would need to stay in a lane rather than driving down the center but for the purposes of this project, aim for center of the road driving\n",
    "\n",
    "\n",
    "- now that you have driven the simulator and know how to record data, it's time to think about collecting data that will ensure a successful model\n",
    "- there are a few general concepts to think about that we will later discuss in more detail:\n",
    "  - the car should stay in the center of the road as much as possible\n",
    "  - if the car veers off to the side, it should recover back to center\n",
    "  - driving counter-clockwise can help the model generalize\n",
    "  - flipping the images is a quick way to augment the data\n",
    "  - collecting data from the second track can also help generalize the model\n",
    "  - we want to avoid overfitting or underfitting when training the model\n",
    "  - knowing when to stop collecting more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the simulator data consists primarily of images\n",
    "- before I start using this data to train a model, I'd like to look at those images\n",
    "- I'll start by opening `driving_log.csv`\n",
    "  - each line in this file represents a point in time during my training lap, and each line has seven tokens\n",
    "    - the first three tokens in each line are paths to images, one each for cameras mounted on the center, left, and right of the vehicle's windshield\n",
    "    - the next four tokens are measurements for steering, throttle, brake, and speed\n",
    "      - the steering measurements range between $-1$ and $1$\n",
    "      - the throttle measurements range between $0$ and $1$\n",
    "      - the brake measurements seem to all be $0$,\n",
    "      - the speed measurements range from $0$ to $30$\n",
    "  - the throttle, brake, and speed data could all be useful for training the network, but for now, I'm going to ignore them\n",
    "\n",
    "\n",
    "- I'll start by using the images as the feature set and the steering measurements as the label set\n",
    "- then I'll use the images to train the network to predict the steering measurements\n",
    "\n",
    "\n",
    "- you might wonder why the simulator gives us three camera shots for each point in time\n",
    "  - the reason is that we can use these camera shots to help us generalize the model\n",
    "- effectively, we can pass these images to the network to teach the network what it looks like to be off the center of the road and how to steer to get back to the center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the image data from the simulator is going to be much easier to work with on a GPU than on a CPU\n",
    "- I'll use Python CSV library to read and store the lines from the `drivinglog.csv` file\n",
    "- once I have the current path, I can use OpenCV to load the image and once I've loaded the image, I can append it to my list of images\n",
    "\n",
    "\n",
    "- I can do something similar for the steering measurements, which will serve as my output labels\n",
    "- it's actually going to be easier to load this steering measurements, because there are no paths or images to handle\n",
    "- I simply extract the fourth token from the CSV line, and then cast it as a float\n",
    "- that gives me the steering measurement for this point in time\n",
    "- then, I append that measurement to the larger measurements array, just like I did for the image\n",
    "\n",
    "\n",
    "- now that I've loaded the images and steering measurements, I'm going to convert them to NumPy arrays, since that's the format Keras requires\n",
    "\n",
    "\n",
    "- for the loss function, I'll use mean squared error, or MSE\n",
    "- this is different than the cross-entropy function we've used in the past, again because this is a regression network instead of a classification network\n",
    "- what I want to do is minimize the error between the steering measurement that the network predicts and the ground truth steering measurement\n",
    "- mean squared error is a good loss function for this\n",
    "\n",
    "\n",
    "- once the model is compiled, I'll train it with the feature and label arrays I just built\n",
    "- I'll also shuffle the data and split off 20% of the data to use for a validation set\n",
    "- by default, Keras trains for 10 epoches\n",
    "\n",
    "\n",
    "- finally, I'm going to save the trained model, so that later I can download it onto my local machine, and see if it works for driving the simulator\n",
    "- next, I'll download this model to my local machine, and see how well it drives the car in the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note: `cv2.imread` will get images in BGR format, while `drive.py` uses RGB\n",
    "- in the video above one way you could keep the same image formatting is to do `image = ndimage.imread(current_path)` with `from scipy import ndimage` instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that you have training data, it’s time to build and train your network!\n",
    "- use Keras to train a network to do the following:\n",
    "  - take in an image from the center camera of the car--this is the input to your neural network\n",
    "  - output a new steering angle for the car\n",
    "- you don’t have to worry about the throttle for this project, that will be set for you\n",
    "\n",
    "\n",
    "- [save your trained model](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) architecture as `model.h5` using `model.save('model.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in order to run the simulator in autonomous mode on my local machine, I'm going to first clone the GitHub repo for the project\n",
    "- this repo provides the `drive.py` file that will load my saved model and use it to make steering predictions\n",
    "- I'll run `drive.py` and pass my model file as an argument\n",
    "  - `drive.py` is running now, waiting for the simulator to start in autonomous mode\n",
    "\n",
    "\n",
    "- when I launch the simulator this time, I'll select autonomous mode\n",
    "- I can see from the output in the terminal that `drive.py` is successfully predicting steering measurements based on my model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in order to validate your network, you'll want to compare model performance on the training set and a validation set\n",
    "- the validation set should contain image and steering data that was not used for training\n",
    "- a rule of thumb could be to use $80\\%$ of your data for training and $20\\%$ for validation or $70\\%$ and $30\\%$\n",
    "- be sure to randomly shuffle the data before splitting into training and validation sets\n",
    "\n",
    "\n",
    "- if model predictions are poor on both the training and validation set (for example, mean squared error is high on both), then this is evidence of underfitting\n",
    "- possible solutions could be to\n",
    "  - increase the number of epochs\n",
    "  - add more convolutions to the network\n",
    "\n",
    "\n",
    "- when the model predicts well on the training set but poorly on the validation set (for example, low mean squared error for training set, high mean squared error for validation set), this is evidence of overfitting\n",
    "- if the model is overfitting, a few ideas could be to\n",
    "  - use dropout or pooling layers\n",
    "  - use fewer convolution or fewer fully connected layers\n",
    "  - collect more data or further augment the data set\n",
    "\n",
    "\n",
    "- ideally, the model will make good predictions on both the training and validation sets\n",
    "- the implication is that when the network sees an image, it can successfully predict what angle was being driven at that moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- once you're satisfied that the model is making good predictions on the training and validation sets, you can test your model by launching the simulator and entering autonomous mode\n",
    "- the car will just sit there until your Python server connects to it and provides it steering angles\n",
    "\n",
    "\n",
    "- here’s how you start your Python server: `python drive.py model.h5`\n",
    "\n",
    "\n",
    "- once the model is up and running in `drive.py`, you should see the car move around (and hopefully not off) the track!\n",
    "- if your model has low mean squared error on the training and validation sets but is driving off the track, this could be because of the data collection process\n",
    "- it's important to feed the network examples of good driving behavior so that the vehicle stays in the center and recovers when getting too close to the sides of the road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the first obvious step to improve the model is to preprocess the data\n",
    "- I'm going to add two preprocessing steps, *normalizing the data* and *mean centering the data*\n",
    "\n",
    "\n",
    "- for normalization, I'll add a Lambda layer to my model\n",
    "- within this Lambda layer, I'll normalize the image by dividing each element by $255$, which is the maximum value of an image pixel\n",
    "- once the image is normalized to a range between $0$ and $1$, I'll mean center the image by subtracting $0.5$ from each element, which will shift the element mean down from $0.5$ to $0$\n",
    "\n",
    "\n",
    "- when I train this model, now I can see that the training loss and the validation loss are both much smaller, which is a good sign\n",
    "- however, if the validation loss only decreases for the first few epoches and then starts oscillating up and down, that's a sign that the model may be overfitting the training data\n",
    "  - I'll reduce the number of epoches and train the model again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in Keras, [lambda layers](https://keras.io/layers/core/#lambda) can be used to create arbitrary functions that operate on each image as it passes through the layer\n",
    "\n",
    "\n",
    "- in this project, a lambda layer is a convenient way to parallelize image normalization\n",
    "- the lambda layer will also ensure that the model will normalize input images when making predictions in `drive.py`\n",
    "- that lambda layer could take each pixel in an image and run it through the formulas:\n",
    "  - `pixel_normalized = pixel / 255`\n",
    "  - `pixel_mean_centered = pixel_normalized - 0.5`\n",
    "- a lambda layer will look something like: \n",
    "  - `Lambda(lambda x: (x / 255.0) - 0.5)`\n",
    "\n",
    "\n",
    "- below is some example code for how a lambda layer can be used\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Lambda\n",
    "\n",
    "# set up lambda layer\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have this problem that the car seems to pull too hard to the left\n",
    "- this actually makes sense--the training track is a loop, and the car drives counter-clockwise\n",
    "- so most of the time, the model is learning to steer to the left--then in autonomous mode, the model does steer to the left even in situations when staying straight might be best\n",
    "- one approach to mitigate this problem is data augmentation\n",
    "- there are many ways to augment data to expand the training set and help the model generalize better\n",
    "  - I could change the brightness on the images or I could shift them horizontally or vertically\n",
    "  - in this case, though, I'm going to keep things pretty simple--I'm just going to flip the images horizontally like a mirror\n",
    "  - then I'll invert the steering angles, and I should wind up with a balanced data set that teaches the car to steer clockwise, as well as counter-clockwise\n",
    "\n",
    "\n",
    "- just like with using side camera data, using data augmentation carries two benefits\n",
    "  - we have more data to use for training the network\n",
    "  - the data we use for training the network is more comprehensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping Images And Steering Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an effective technique for helping with the left turn bias involves flipping images and taking the opposite sign of the steering measurement\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "image_flipped = np.fliplr(image)\n",
    "measurement_flipped = -measurement\n",
    "```\n",
    "\n",
    "- the cv2 library also has similar functionality with the [flip method](http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Multiple Cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using the side camera images carries two benefits\n",
    "  - it's simply more data--in fact, it's three times as much data\n",
    "  - using these images will help teach the network how to steer back to the center if the vehicle starts drifting off to the side\n",
    "\n",
    "\n",
    "- let's look at the left image--this image is a little off to the left side of the road, but it's paired with a steering measurement of zero\n",
    "- that's because when this image was taken, the center of the vehicle really was in the center of the road\n",
    "\n",
    "\n",
    "- basically, I want to train the network to steer a little harder to the right whenever the network sees an image like this\n",
    "  - I'll do this by taking the actual steering measurement, which in this case is zero, and adding a small correction factor to it\n",
    "  - I could actually draw out a model and use trigonometry and physics to calculate an optimal correction factor, but I'm just going to be lazy and try a correction factor of $0.2$\n",
    "\n",
    "\n",
    "- similarly, for the right camera image, I'm going to train the network to steer a little harder to the left\n",
    "- I'll do that by subtracting the correction factor from the actual steering measurement whenever I use a right camera image\n",
    "  - I implement this by looping through the first three tokens of each line in the CSV file, and I use these to load each camera image into the images array\n",
    "  - then I add three measurements to the measurements array corresponding to each image I just added\n",
    "\n",
    "\n",
    "- now I should have three times as much data, and my model should perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the simulator captures images from three cameras mounted on the car: a center, right and left camera\n",
    "- that’s because of the issue of recovering from being off-center\n",
    "- in the simulator, you can weave all over the road and turn recording on and off to record recovery driving\n",
    "- in a real car, however, that’s not really possible--at least not legally\n",
    "\n",
    "\n",
    "- so in a real car, we’ll have multiple cameras on the vehicle, and we’ll map recovery paths from each camera\n",
    "  - for example, if you train the model to associate a given image from the center camera with a left turn, then you could also train the model to associate the corresponding image from the left camera with a somewhat softer left turn\n",
    "  - and you could train the model to associate the corresponding image from the right camera with an even harder left turn\n",
    "- in that way, you can simulate your vehicle being in different positions, somewhat further off the center line\n",
    "\n",
    "\n",
    "- to read more about this approach, see [this paper](http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) by our friends at NVIDIA that makes use of this technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of How Multiple Cameras Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the image below gives a sense for how multiple cameras are used to train a self-driving car\n",
    "- this image shows a bird's-eye perspective of the car\n",
    "- the driver is moving forward but wants to turn towards a destination on the left\n",
    "\n",
    "\n",
    "- from the perspective of the left camera, the steering angle would be less than the steering angle from the center camera\n",
    "- from the right camera's perspective, the steering angle would be larger than the angle from the center camera\n",
    "- the next section will discuss how this can be implemented in your project although there is no requirement to use the left and right camera images\n",
    "\n",
    "<img src=\"resources/using_multiple_cameras.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Cameras in This Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for this project, recording recoveries from the sides of the road back to center is effective\n",
    "- but it is also possible to use all three camera images to train the model\n",
    "- when recording, the simulator will simultaneously save an image for the left, center and right cameras\n",
    "- each row of the csv log file, `driving_log.csv`, contains the file path for each camera as well as information about the steering measurement, throttle, brake and speed of the vehicle\n",
    "\n",
    "\n",
    "- here is some example code to give an idea of how all three images can be used:\n",
    "\n",
    "```python\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            steering_center = float(row[3])\n",
    "\n",
    "            # create adjusted steering measurements for the side camera images\n",
    "            correction = 0.2 # this is a parameter to tune\n",
    "            steering_left = steering_center + correction\n",
    "            steering_right = steering_center - correction\n",
    "\n",
    "            # read in images from center, left and right cameras\n",
    "            path = \"...\" # fill in the path to your training IMG directory\n",
    "            img_center = process_image(np.asarray(Image.open(path + row[0])))\n",
    "            img_left = process_image(np.asarray(Image.open(path + row[1])))\n",
    "            img_right = process_image(np.asarray(Image.open(path + row[2])))\n",
    "\n",
    "            # add images and angles to data set\n",
    "            car_images.extend(img_center, img_left, img_right)\n",
    "            steering_angles.extend(steering_center, steering_left, steering_right)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- during training, you want to feed the left and right camera images to your model as if they were coming from the center camera\n",
    "  - this way, you can teach your model how to steer if the car drifts off to the left or the right\n",
    "- figuring out how much to add or subtract from the center angle will involve some experimentation\n",
    "- during prediction (i.e. \"autonomous mode\"), you only need to predict with the center camera image\n",
    "- it is not necessary to use the left and right images to derive a successful model\n",
    "- recording recovery driving from the sides of the road is also effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping Images in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'd like to return to something I saw a while back, which is that the top pixels of the image mostly capture sky and trees and hills and other elements that might be more distracting for the model than helpful\n",
    "  - it looks like this is about 70 rows of pixels\n",
    "- I also notice that the bottom 25 pixels largely consist of the hood of the car\n",
    "- I'm going to crop out those portions of the images.\n",
    "\n",
    "\n",
    "- just like with the Lambda layer, I'm going to use a built-in Keras layer to perform the cropping inside of the model\n",
    "- in this case I'll use the Cropping 2D layer to remove the top 70 pixels and the bottom 25 pixels\n",
    "\n",
    "- your model might train faster if you crop each image to focus on only the portion of the image that is useful for predicting a steering angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping2D Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras provides the [Cropping2D layer](https://keras.io/layers/convolutional/#cropping2d) for image cropping within the model\n",
    "- this is relatively fast, because the model is parallelized on the GPU, so many images are cropped simultaneously\n",
    "- by contrast, image cropping outside the model on the CPU is relatively slow\n",
    "- also, by adding the cropping layer, the model will automatically crop the input images when making predictions in `drive.py`\n",
    "- the Cropping2D layer might be useful for choosing an area of interest that excludes the sky and/or the hood of the car\n",
    "\n",
    "\n",
    "- cropping Layer Code example:\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Cropping2D\n",
    "import cv2\n",
    "\n",
    "# set up cropping2D layer\n",
    "model = Sequential()\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))\n",
    "...\n",
    "```\n",
    "- the example above crops:\n",
    "  - 50 rows pixels from the top of the image\n",
    "  - 20 rows pixels from the bottom of the image\n",
    "  - 0 columns of pixels from the left of the image\n",
    "  - 0 columns of pixels from the right of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one obvious approach to improve the network that I haven't tried yet--maybe THE obvious approach, is to gather more data\n",
    "- I could also try to be more clever about data collection\n",
    "- for example, I could drive around the track in the opposite direction\n",
    "  - training in the opposite direction is kind of like gathering data from a whole new track, and this data will help my model generalize better\n",
    "  - so when I run in autonomous mode, if the vehicle gets into a weird place on the road, the model will have a better chance of making a good decision\n",
    "- in the same vein, I could gather training data from multiple tracks and mix them together\n",
    "  - this could produce a much more generalized model that can drive either track successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovery Laps\n",
    "\n",
    "- if you drive and record normal laps around the track, even if you record a lot of them, it might not be enough to train your model to drive properly\n",
    "- here’s the problem: if your training data is all focused on driving down the middle of the road, your model won’t ever learn what to do if it gets off to the side of the road\n",
    "- and probably when you run your model to predict steering measurements, things won’t go perfectly and the car will wander off to the side of the road at some point\n",
    "\n",
    "\n",
    "- so you need to teach the car what to do when it’s off on the side of the road\n",
    "  - one approach might be to constantly wander off to the side of the road and then steer back to the middle\n",
    "  - a better approach is to only record data when the car is driving from the side of the road back toward the center line\n",
    "  - so as the human driver, you’re still weaving back and forth between the middle of the road and the shoulder, but you need to turn off data recording when you weave out to the side, and turn it back on when you steer back to the middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driving Counter-Clockwise\n",
    "\n",
    "- track one has a left turn bias\n",
    "- if you only drive around the first track in a clock-wise direction, the data will be biased towards left turns\n",
    "- one way to combat the bias is to turn the car around and record counter-clockwise laps around the track\n",
    "- driving counter-clockwise is also like giving the model a new track to learn from, so the model will generalize better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Both Tracks\n",
    "\n",
    "- if you end up using data from only track one, the convolutional neural network could essentially memorize the track\n",
    "- consider using data from both track one and track two to make a more generalized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Enough Data\n",
    "\n",
    "- how do you know when you have collected enough data?\n",
    "- machine learning involves trying out ideas and testing them to see if they work\n",
    "- if the model is over or underfitting, then try to figure out why and adjust accordingly\n",
    "- since this model outputs a single continuous numeric value, one appropriate error metric would be mean squared error\n",
    "  - if the mean squared error is high on both a training and validation set, the model is underfitting\n",
    "  - if the mean squared error is low on a training set but high on a validation set, the model is overfitting\n",
    "    - collecting more data can help improve a model when the model is overfitting\n",
    "\n",
    "\n",
    "- what if the model has a low mean squared error on both the training and validation sets, but the car is falling off the track?\n",
    "  - try to figure out the cases where the vehicle is falling off the track\n",
    "  - does it occur only on turns?\n",
    "    - then maybe it's important to collect more turning data\n",
    "  - the vehicle's driving behavior is only as good as the behavior of the driver who provided the data\n",
    "\n",
    "\n",
    "- here are some general guidelines for data collection:\n",
    "  - two or three laps of center lane driving\n",
    "  - one lap of recovery driving from the sides\n",
    "  - one lap focusing on driving smoothly around curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting Training and Validation Loss Metrics\n",
    "\n",
    "- in Keras, the `model.fit()` and `model.fit_generator()` methods have a verbose parameter that tells Keras to output loss metrics as the model trains\n",
    "- the `verbose` parameter can optionally be set to `verbose = 1` or `verbose = 2`\n",
    "- setting `model.fit(verbose = 1)` will:\n",
    "  - output a progress bar in the terminal as the model trains.\n",
    "  - output the loss metric on the training set as the model trains.\n",
    "  - output the loss on the training and validation sets after each epoch\n",
    "- with `model.fit(verbose = 2)`, Keras will only output the loss on the training set and validation set after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model History Object\n",
    "\n",
    "- when calling `model.fit()` or `model.fit_generator()`, Keras outputs a history object that contains the training and validation loss for each epoch\n",
    "- the following code shows how to use the `model.fit()` history object to produce the visualization\n",
    "\n",
    "```python\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch =\n",
    "    len(train_samples), validation_data = \n",
    "    validation_generator,\n",
    "    nb_val_samples = len(validation_samples), \n",
    "    nb_epoch=5, verbose=1)\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the images captured in the car simulator are much larger than the images encountered in the Traffic Sign Classifier Project, a size of $160 x 320 x 3$ compared to $32 x 32 x 3$\n",
    "- storing $10 000$ traffic sign images would take about $30$ MB but storing $10 000$ simulator images would take over $1.5$ GB\n",
    "- that's a lot of memory!--not to mention that preprocessing data can change data types from an int to a float, which can increase the size of the data by a factor of $4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generators can be a great way to work with large amounts of data\n",
    "- instead of storing the preprocessed data in memory all at once, using a generator you can pull pieces of the data and process them on the fly only when you need them, which is much more memory-efficient\n",
    "- a generator is like a [coroutine](https://en.wikipedia.org/wiki/Coroutine), a process that can run separately from another main routine, which makes it a useful Python function\n",
    "\n",
    "\n",
    "- instead of using return, the generator uses yield, which still returns the desired output values but saves the current values of all the generator's variables\n",
    "- when the generator is called a second time it re-starts right after the yield statement, with all its variables set to the same values as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- below is a short quiz using a generator\n",
    "- this generator appends a new Fibonacci number to its list every time it is called\n",
    "- the result will be we can get the first $10$ Fibonacci numbers simply by calling our generator $10$ times\n",
    "- if we need to go do something else besides generate Fibonacci numbers for a while we can do that and then always just call the generator again whenever we need more Fibonacci numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n"
     ]
    }
   ],
   "source": [
    "def fibonacci():\n",
    "    numbers_list = []\n",
    "    while 1:\n",
    "        if(len(numbers_list) < 2):\n",
    "            numbers_list.append(1)\n",
    "        else:\n",
    "            numbers_list.append(numbers_list[-1] + numbers_list[-2])\n",
    "        yield numbers_list\n",
    "\n",
    "our_generator = fibonacci()\n",
    "my_output = []\n",
    "\n",
    "for i in range(10):\n",
    "    my_output = (next(our_generator))\n",
    "    \n",
    "print(my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here is an example of how you could use a generator to load data and preprocess it on the fly, in batch size portions to feed into your Behavioral Cloning model\n",
    "\n",
    "```python\n",
    "import os\n",
    "import csv\n",
    "\n",
    "samples = []\n",
    "with open('./driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = './IMG/'+batch_sample[0].split('/')[-1]\n",
    "                center_image = cv2.imread(name)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "# Set our batch size\n",
    "batch_size=32\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "ch, row, col = 3, 80, 320  # Trimmed image format\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "        input_shape=(ch, row, col),\n",
    "        output_shape=(ch, row, col)))\n",
    "model.add(... finish defining the rest of your model architecture here ...)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit_generator(train_generator, /\n",
    "            steps_per_epoch=ceil(len(train_samples)/batch_size), /\n",
    "            validation_data=validation_generator, /\n",
    "            validation_steps=ceil(len(validation_samples)/batch_size), /\n",
    "            epochs=5, verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Video in Autonomous Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- because your hardware setup might be different from a reviewer's hardware setup, driving behavior could be different on your machine than on the reviewer's\n",
    "- to help with reviewing your submission, we require that you submit a video recording of your vehicle driving autonomously around the track\n",
    "- the video should include at least one full lap around the track. Keep in mind the rubric specifications:\n",
    "  - \"No tire may leave the drivable portion of the track surface. The car may not pop up onto ledges or roll over any surfaces that would otherwise be considered unsafe (if humans were in the vehicle).\"\n",
    "\n",
    "\n",
    "- in the [GitHub repo](https://github.com/udacity/CarND-Behavioral-Cloning-P3), we have included a file called `video.py`, which can be used to create the video recording when in autonomous mode\n",
    "- the README file in the GitHub repo contains instructions about how to make the video recording\n",
    "  - here are the instructions as well: `python drive.py model.h5 run1`\n",
    "  - the fourth argument, `run1`, is the directory in which to save the images seen by the agent\n",
    "    - if the directory already exists, it'll be overwritten\n",
    "\n",
    "\n",
    "- the image file name is a timestamp of when the image was seen\n",
    "- this information is used by `video.py` to create a chronological video of the agent driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using video.py\n",
    "\n",
    "- `python video.py run1` creates a video based on images found in the run1 directory\n",
    "- the name of the video will be the name of the directory followed by `'.mp4'`, so, in this case the video will be `run1.mp4`\n",
    "- optionally, one can specify the FPS (frames per second) of the video: `python video.py run1 --fps 48`\n",
    "  - the video will run at 48 FPS\n",
    "  - the default FPS is 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a former student has written [a helpful guide](https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/Behavioral+Cloning+Cheatsheet+-+CarND.pdf) for those of you looking for some hints and advice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
