{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import cv2\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport os\\nimport pathlib\\nimport pickle\\n\\nfrom moviepy.editor import VideoFileClip\";\n",
       "                var nbb_formatted_code = \"import cv2\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport os\\nimport pathlib\\nimport pickle\\n\\nfrom moviepy.editor import VideoFileClip\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def create_output_directory(fpath):\\n    \\\"\\\"\\\"\\n    Creates output directory for images, if it doesn't already exist\\n    \\\"\\\"\\\"\\n\\n    pathlib.Path(fpath).mkdir(parents=True, exist_ok=True)\";\n",
       "                var nbb_formatted_code = \"def create_output_directory(fpath):\\n    \\\"\\\"\\\"\\n    Creates output directory for images, if it doesn't already exist\\n    \\\"\\\"\\\"\\n\\n    pathlib.Path(fpath).mkdir(parents=True, exist_ok=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_output_directory(fpath):\n",
    "    \"\"\"\n",
    "    Creates output directory for images, if it doesn't already exist\n",
    "    \"\"\"\n",
    "\n",
    "    pathlib.Path(fpath).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"test_images_input_dir = \\\"./test_images/\\\"\\ncamera_calibration_input_dir = \\\"./camera_cal/\\\"\\n\\noutput_images_dir = \\\"./output_images/\\\"\\noutput_videos_dir = \\\"./output_videos/\\\"\\noutput_data_dir = \\\"./output_data/\\\"\\n\\ncamera_calibration_chessboard_corners_output_dir = (\\n    output_images_dir + \\\"1_camera_calibration_output_images/chessboard_corners/\\\"\\n)\\ncamera_calibration_undistorted_output_dir = (\\n    output_images_dir + \\\"1_camera_calibration_output_images/undistorted/\\\"\\n)\\nperspective_transform_output_dir = (\\n    output_images_dir + \\\"2_perspective_transform_output_images/\\\"\\n)\\ncolor_threshold_output_dir = output_images_dir + \\\"3_color_threshold_output_images/\\\"\\nlane_lines_img_output_dir = output_images_dir + \\\"4_lane_lines_output_images/\\\"\";\n",
       "                var nbb_formatted_code = \"test_images_input_dir = \\\"./test_images/\\\"\\ncamera_calibration_input_dir = \\\"./camera_cal/\\\"\\n\\noutput_images_dir = \\\"./output_images/\\\"\\noutput_videos_dir = \\\"./output_videos/\\\"\\noutput_data_dir = \\\"./output_data/\\\"\\n\\ncamera_calibration_chessboard_corners_output_dir = (\\n    output_images_dir + \\\"1_camera_calibration_output_images/chessboard_corners/\\\"\\n)\\ncamera_calibration_undistorted_output_dir = (\\n    output_images_dir + \\\"1_camera_calibration_output_images/undistorted/\\\"\\n)\\nperspective_transform_output_dir = (\\n    output_images_dir + \\\"2_perspective_transform_output_images/\\\"\\n)\\ncolor_threshold_output_dir = output_images_dir + \\\"3_color_threshold_output_images/\\\"\\nlane_lines_img_output_dir = output_images_dir + \\\"4_lane_lines_output_images/\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images_input_dir = \"./test_images/\"\n",
    "camera_calibration_input_dir = \"./camera_cal/\"\n",
    "\n",
    "output_images_dir = \"./output_images/\"\n",
    "output_videos_dir = \"./output_videos/\"\n",
    "output_data_dir = \"./output_data/\"\n",
    "\n",
    "camera_calibration_chessboard_corners_output_dir = (\n",
    "    output_images_dir + \"1_camera_calibration_output_images/chessboard_corners/\"\n",
    ")\n",
    "camera_calibration_undistorted_output_dir = (\n",
    "    output_images_dir + \"1_camera_calibration_output_images/undistorted/\"\n",
    ")\n",
    "perspective_transform_output_dir = (\n",
    "    output_images_dir + \"2_perspective_transform_output_images/\"\n",
    ")\n",
    "color_threshold_output_dir = output_images_dir + \"3_color_threshold_output_images/\"\n",
    "lane_lines_img_output_dir = output_images_dir + \"4_lane_lines_output_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"create_output_directory(output_data_dir)\\ncreate_output_directory(camera_calibration_chessboard_corners_output_dir)\\ncreate_output_directory(camera_calibration_undistorted_output_dir)\\ncreate_output_directory(perspective_transform_output_dir)\\ncreate_output_directory(color_threshold_output_dir)\\ncreate_output_directory(lane_lines_img_output_dir)\";\n",
       "                var nbb_formatted_code = \"create_output_directory(output_data_dir)\\ncreate_output_directory(camera_calibration_chessboard_corners_output_dir)\\ncreate_output_directory(camera_calibration_undistorted_output_dir)\\ncreate_output_directory(perspective_transform_output_dir)\\ncreate_output_directory(color_threshold_output_dir)\\ncreate_output_directory(lane_lines_img_output_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_output_directory(output_data_dir)\n",
    "create_output_directory(camera_calibration_chessboard_corners_output_dir)\n",
    "create_output_directory(camera_calibration_undistorted_output_dir)\n",
    "create_output_directory(perspective_transform_output_dir)\n",
    "create_output_directory(color_threshold_output_dir)\n",
    "create_output_directory(lane_lines_img_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate Camera\n",
    "\n",
    "- https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html\n",
    "- images in `camera_cal/` have chessboard size $9x6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"chess_w = 9\\nchess_h = 6\\n\\n# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ... (6, 4, 0)\\n# by creating six by eight points in an array\\n# each with three columns for the x, y, and z coordinates of each corner\\n# initialize these all as zeros using Numpy's zero function\\nobjp = np.zeros((chess_w * chess_h, 3), np.float32)\\n\\n# z coordinate will stay zero but for first two columns, x and y\\n# use numpy's mgrid function to generate the coordinates that we want\\n# mgrid returns the coordinate values for a given grid size\\n# shape those coordinates back into two columns, one for x and one for y\\nobjp[:, :2] = np.mgrid[0:chess_w, 0:chess_h].T.reshape(-1, 2)  # x, y coordinates\\n\\n# Arrays to store object points and image points from all the images\\nobjpoints = []  # 3D points in real world space\\nimgpoints = []  # 2D points in image plane\\n\\n# termination criteria\\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\\n\\nfor fname in os.listdir(camera_calibration_input_dir):\\n    # Read in each image\\n    cam_cal_img = cv2.imread(camera_calibration_input_dir + fname)\\n\\n    # Convert to grayscale\\n    cam_cal_gray_img = cv2.cvtColor(cam_cal_img, cv2.COLOR_BGR2GRAY)\\n\\n    # Find the chessboard corners\\n    ret, corners = cv2.findChessboardCorners(cam_cal_gray_img, (chess_w, chess_h), None)\\n\\n    # If corners are found, add object points, image points\\n    if ret == True:\\n        objpoints.append(\\n            objp\\n        )  # these object points will be the same for all of the calibration images, since they represent a real chessboard\\n\\n        # increase corner accuracy\\n        corners2 = cv2.cornerSubPix(\\n            cam_cal_gray_img, corners, (11, 11), (-1, -1), criteria\\n        )\\n        imgpoints.append(corners2)\\n\\n        # Draw and display the corners\\n        cam_cal_img = cv2.drawChessboardCorners(\\n            cam_cal_img, (chess_w, chess_h), corners, ret\\n        )\\n\\n        # plt.figure()\\n        # plt.imshow(cam_cal_img)\\n\\n        # save image\\n        cv2.imwrite(\\n            camera_calibration_chessboard_corners_output_dir + fname, cam_cal_img\\n        )\";\n",
       "                var nbb_formatted_code = \"chess_w = 9\\nchess_h = 6\\n\\n# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ... (6, 4, 0)\\n# by creating six by eight points in an array\\n# each with three columns for the x, y, and z coordinates of each corner\\n# initialize these all as zeros using Numpy's zero function\\nobjp = np.zeros((chess_w * chess_h, 3), np.float32)\\n\\n# z coordinate will stay zero but for first two columns, x and y\\n# use numpy's mgrid function to generate the coordinates that we want\\n# mgrid returns the coordinate values for a given grid size\\n# shape those coordinates back into two columns, one for x and one for y\\nobjp[:, :2] = np.mgrid[0:chess_w, 0:chess_h].T.reshape(-1, 2)  # x, y coordinates\\n\\n# Arrays to store object points and image points from all the images\\nobjpoints = []  # 3D points in real world space\\nimgpoints = []  # 2D points in image plane\\n\\n# termination criteria\\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\\n\\nfor fname in os.listdir(camera_calibration_input_dir):\\n    # Read in each image\\n    cam_cal_img = cv2.imread(camera_calibration_input_dir + fname)\\n\\n    # Convert to grayscale\\n    cam_cal_gray_img = cv2.cvtColor(cam_cal_img, cv2.COLOR_BGR2GRAY)\\n\\n    # Find the chessboard corners\\n    ret, corners = cv2.findChessboardCorners(cam_cal_gray_img, (chess_w, chess_h), None)\\n\\n    # If corners are found, add object points, image points\\n    if ret == True:\\n        objpoints.append(\\n            objp\\n        )  # these object points will be the same for all of the calibration images, since they represent a real chessboard\\n\\n        # increase corner accuracy\\n        corners2 = cv2.cornerSubPix(\\n            cam_cal_gray_img, corners, (11, 11), (-1, -1), criteria\\n        )\\n        imgpoints.append(corners2)\\n\\n        # Draw and display the corners\\n        cam_cal_img = cv2.drawChessboardCorners(\\n            cam_cal_img, (chess_w, chess_h), corners, ret\\n        )\\n\\n        # plt.figure()\\n        # plt.imshow(cam_cal_img)\\n\\n        # save image\\n        cv2.imwrite(\\n            camera_calibration_chessboard_corners_output_dir + fname, cam_cal_img\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chess_w = 9\n",
    "chess_h = 6\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ... (6, 4, 0)\n",
    "# by creating six by eight points in an array\n",
    "# each with three columns for the x, y, and z coordinates of each corner\n",
    "# initialize these all as zeros using Numpy's zero function\n",
    "objp = np.zeros((chess_w * chess_h, 3), np.float32)\n",
    "\n",
    "# z coordinate will stay zero but for first two columns, x and y\n",
    "# use numpy's mgrid function to generate the coordinates that we want\n",
    "# mgrid returns the coordinate values for a given grid size\n",
    "# shape those coordinates back into two columns, one for x and one for y\n",
    "objp[:, :2] = np.mgrid[0:chess_w, 0:chess_h].T.reshape(-1, 2)  # x, y coordinates\n",
    "\n",
    "# Arrays to store object points and image points from all the images\n",
    "objpoints = []  # 3D points in real world space\n",
    "imgpoints = []  # 2D points in image plane\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "for fname in os.listdir(camera_calibration_input_dir):\n",
    "    # Read in each image\n",
    "    cam_cal_img = cv2.imread(camera_calibration_input_dir + fname)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    cam_cal_gray_img = cv2.cvtColor(cam_cal_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(cam_cal_gray_img, (chess_w, chess_h), None)\n",
    "\n",
    "    # If corners are found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(\n",
    "            objp\n",
    "        )  # these object points will be the same for all of the calibration images, since they represent a real chessboard\n",
    "\n",
    "        # increase corner accuracy\n",
    "        corners2 = cv2.cornerSubPix(\n",
    "            cam_cal_gray_img, corners, (11, 11), (-1, -1), criteria\n",
    "        )\n",
    "        imgpoints.append(corners2)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        cam_cal_img = cv2.drawChessboardCorners(\n",
    "            cam_cal_img, (chess_w, chess_h), corners, ret\n",
    "        )\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.imshow(cam_cal_img)\n",
    "\n",
    "        # save image\n",
    "        cv2.imwrite(\n",
    "            camera_calibration_chessboard_corners_output_dir + fname, cam_cal_img\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# Load camera calibration example image\\ncam_cal_example_img = cv2.imread(\\\"./camera_cal/calibration1.jpg\\\")\\ncam_cal_example_img_size = (cam_cal_example_img.shape[1], cam_cal_example_img.shape[0])\\n\\n# Do camera calibration given object points and image points\\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\\n    objpoints, imgpoints, cam_cal_example_img_size, None, None\\n)\";\n",
       "                var nbb_formatted_code = \"# Load camera calibration example image\\ncam_cal_example_img = cv2.imread(\\\"./camera_cal/calibration1.jpg\\\")\\ncam_cal_example_img_size = (cam_cal_example_img.shape[1], cam_cal_example_img.shape[0])\\n\\n# Do camera calibration given object points and image points\\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\\n    objpoints, imgpoints, cam_cal_example_img_size, None, None\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load camera calibration example image\n",
    "cam_cal_example_img = cv2.imread(\"./camera_cal/calibration1.jpg\")\n",
    "cam_cal_example_img_size = (cam_cal_example_img.shape[1], cam_cal_example_img.shape[0])\n",
    "\n",
    "# Do camera calibration given object points and image points\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
    "    objpoints, imgpoints, cam_cal_example_img_size, None, None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# save calculated camera matrix and distortion coefficients\\ncalibrate_camera_data = {\\\"mtx\\\": mtx, \\\"dist\\\": dist}\\n\\n# store data (serialize)\\nwith open(output_data_dir + \\\"calibrate_camera_data.pickle\\\", \\\"wb\\\") as handle:\\n    pickle.dump(calibrate_camera_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\";\n",
       "                var nbb_formatted_code = \"# save calculated camera matrix and distortion coefficients\\ncalibrate_camera_data = {\\\"mtx\\\": mtx, \\\"dist\\\": dist}\\n\\n# store data (serialize)\\nwith open(output_data_dir + \\\"calibrate_camera_data.pickle\\\", \\\"wb\\\") as handle:\\n    pickle.dump(calibrate_camera_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save calculated camera matrix and distortion coefficients\n",
    "calibrate_camera_data = {\"mtx\": mtx, \"dist\": dist}\n",
    "\n",
    "# store data (serialize)\n",
    "with open(output_data_dir + \"calibrate_camera_data.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(calibrate_camera_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# Four source coordinates (region of interest)\\nperspective_src = np.float32(\\n    [\\n        (575, 465),  # top left\\n        (710, 465),  # top right\\n        (255, 685),  # bottom left\\n        (1050, 685),  # bottom right\\n    ]\\n)\\n\\ny_size = 720  # height\\nx_size = 1280  # width\\n\\n# Four desired coordinates (region of interest)\\nperspective_dst = np.float32(\\n    [\\n        (450, 0),  # top left\\n        (x_size - 450, 0),  # top right\\n        (450, y_size),  # bottom left\\n        (x_size - 450, y_size),  # bottom right\\n    ]\\n)\";\n",
       "                var nbb_formatted_code = \"# Four source coordinates (region of interest)\\nperspective_src = np.float32(\\n    [\\n        (575, 465),  # top left\\n        (710, 465),  # top right\\n        (255, 685),  # bottom left\\n        (1050, 685),  # bottom right\\n    ]\\n)\\n\\ny_size = 720  # height\\nx_size = 1280  # width\\n\\n# Four desired coordinates (region of interest)\\nperspective_dst = np.float32(\\n    [\\n        (450, 0),  # top left\\n        (x_size - 450, 0),  # top right\\n        (450, y_size),  # bottom left\\n        (x_size - 450, y_size),  # bottom right\\n    ]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Four source coordinates (region of interest)\n",
    "perspective_src = np.float32(\n",
    "    [\n",
    "        (575, 465),  # top left\n",
    "        (710, 465),  # top right\n",
    "        (255, 685),  # bottom left\n",
    "        (1050, 685),  # bottom right\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_size = 720  # height\n",
    "x_size = 1280  # width\n",
    "\n",
    "# Four desired coordinates (region of interest)\n",
    "perspective_dst = np.float32(\n",
    "    [\n",
    "        (450, 0),  # top left\n",
    "        (x_size - 450, 0),  # top right\n",
    "        (450, y_size),  # bottom left\n",
    "        (x_size - 450, y_size),  # bottom right\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# Define perspective transform function\\ndef perspective_transform(img_input, src, dst):\\n    \\\"\\\"\\\"\\n    Applies perspective transform on input image based on src and dst parameters.\\n    \\\"\\\"\\\"\\n\\n    # Define calibration box in source (original) and destination (desired or warped) coordinates\\n    img_size = (img_input.shape[1], img_input.shape[0])\\n\\n    # Compute the perspective transform, M (matrix)\\n    M = cv2.getPerspectiveTransform(src, dst)\\n\\n    # Compute the inverse also by swapping the input parameters\\n    M_inv = cv2.getPerspectiveTransform(dst, src)\\n\\n    # Create warped image using the perspective transform, M - uses linear interpolation\\n    img_warped = cv2.warpPerspective(img_input, M, img_size, flags=cv2.INTER_LINEAR)\\n\\n    return img_warped, M_inv\";\n",
       "                var nbb_formatted_code = \"# Define perspective transform function\\ndef perspective_transform(img_input, src, dst):\\n    \\\"\\\"\\\"\\n    Applies perspective transform on input image based on src and dst parameters.\\n    \\\"\\\"\\\"\\n\\n    # Define calibration box in source (original) and destination (desired or warped) coordinates\\n    img_size = (img_input.shape[1], img_input.shape[0])\\n\\n    # Compute the perspective transform, M (matrix)\\n    M = cv2.getPerspectiveTransform(src, dst)\\n\\n    # Compute the inverse also by swapping the input parameters\\n    M_inv = cv2.getPerspectiveTransform(dst, src)\\n\\n    # Create warped image using the perspective transform, M - uses linear interpolation\\n    img_warped = cv2.warpPerspective(img_input, M, img_size, flags=cv2.INTER_LINEAR)\\n\\n    return img_warped, M_inv\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define perspective transform function\n",
    "def perspective_transform(img_input, src, dst):\n",
    "    \"\"\"\n",
    "    Applies perspective transform on input image based on src and dst parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define calibration box in source (original) and destination (desired or warped) coordinates\n",
    "    img_size = (img_input.shape[1], img_input.shape[0])\n",
    "\n",
    "    # Compute the perspective transform, M (matrix)\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "    # Compute the inverse also by swapping the input parameters\n",
    "    M_inv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "    # Create warped image using the perspective transform, M - uses linear interpolation\n",
    "    img_warped = cv2.warpPerspective(img_input, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return img_warped, M_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Spaces and Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# threshold the L-channel of HLS\\ndef hls_l_threshold(img_input, thresh=(220, 255)):\\n    \\\"\\\"\\\"\\n    Extracts L channel of HLS colorspace.\\n    Uses default threshold values if not specified explicitly.    \\n    \\\"\\\"\\\"\\n\\n    # convert to HLS color space\\n    hls_l_channel = cv2.cvtColor(img_input, cv2.COLOR_RGB2HLS)[:, :, 1]\\n    hls_l_channel = hls_l_channel * (255 / np.max(hls_l_channel))\\n\\n    # apply a threshold to the L channel\\n    hls_l_binary = np.zeros_like(hls_l_channel)\\n    hls_l_binary[(hls_l_channel > thresh[0]) & (hls_l_channel <= thresh[1])] = 1\\n\\n    return hls_l_binary\\n\\n\\n# threshold the B-channel of LAB\\ndef lab_b_threshold(img_input, thresh=(190, 255)):\\n    \\\"\\\"\\\"\\n    Extracts B channel of LAB colorspace.\\n    Uses default threshold values if not specified explicitly.    \\n    \\\"\\\"\\\"\\n\\n    # convert to LAB color space\\n    lab_b_channel = cv2.cvtColor(img_input, cv2.COLOR_RGB2Lab)[:, :, 2]\\n    # don't normalize if there are no yellows in the image\\n    if np.max(lab_b_channel) > 175:\\n        lab_b_channel = lab_b_channel * (255 / np.max(lab_b_channel))\\n\\n    # apply a threshold to the B channel\\n    lab_b_binary = np.zeros_like(lab_b_channel)\\n    lab_b_binary[((lab_b_channel > thresh[0]) & (lab_b_channel <= thresh[1]))] = 1\\n\\n    return lab_b_binary\";\n",
       "                var nbb_formatted_code = \"# threshold the L-channel of HLS\\ndef hls_l_threshold(img_input, thresh=(220, 255)):\\n    \\\"\\\"\\\"\\n    Extracts L channel of HLS colorspace.\\n    Uses default threshold values if not specified explicitly.    \\n    \\\"\\\"\\\"\\n\\n    # convert to HLS color space\\n    hls_l_channel = cv2.cvtColor(img_input, cv2.COLOR_RGB2HLS)[:, :, 1]\\n    hls_l_channel = hls_l_channel * (255 / np.max(hls_l_channel))\\n\\n    # apply a threshold to the L channel\\n    hls_l_binary = np.zeros_like(hls_l_channel)\\n    hls_l_binary[(hls_l_channel > thresh[0]) & (hls_l_channel <= thresh[1])] = 1\\n\\n    return hls_l_binary\\n\\n\\n# threshold the B-channel of LAB\\ndef lab_b_threshold(img_input, thresh=(190, 255)):\\n    \\\"\\\"\\\"\\n    Extracts B channel of LAB colorspace.\\n    Uses default threshold values if not specified explicitly.    \\n    \\\"\\\"\\\"\\n\\n    # convert to LAB color space\\n    lab_b_channel = cv2.cvtColor(img_input, cv2.COLOR_RGB2Lab)[:, :, 2]\\n    # don't normalize if there are no yellows in the image\\n    if np.max(lab_b_channel) > 175:\\n        lab_b_channel = lab_b_channel * (255 / np.max(lab_b_channel))\\n\\n    # apply a threshold to the B channel\\n    lab_b_binary = np.zeros_like(lab_b_channel)\\n    lab_b_binary[((lab_b_channel > thresh[0]) & (lab_b_channel <= thresh[1]))] = 1\\n\\n    return lab_b_binary\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# threshold the L-channel of HLS\n",
    "def hls_l_threshold(img_input, thresh=(220, 255)):\n",
    "    \"\"\"\n",
    "    Extracts L channel of HLS colorspace.\n",
    "    Uses default threshold values if not specified explicitly.    \n",
    "    \"\"\"\n",
    "\n",
    "    # convert to HLS color space\n",
    "    hls_l_channel = cv2.cvtColor(img_input, cv2.COLOR_RGB2HLS)[:, :, 1]\n",
    "    hls_l_channel = hls_l_channel * (255 / np.max(hls_l_channel))\n",
    "\n",
    "    # apply a threshold to the L channel\n",
    "    hls_l_binary = np.zeros_like(hls_l_channel)\n",
    "    hls_l_binary[(hls_l_channel > thresh[0]) & (hls_l_channel <= thresh[1])] = 1\n",
    "\n",
    "    return hls_l_binary\n",
    "\n",
    "\n",
    "# threshold the B-channel of LAB\n",
    "def lab_b_threshold(img_input, thresh=(190, 255)):\n",
    "    \"\"\"\n",
    "    Extracts B channel of LAB colorspace.\n",
    "    Uses default threshold values if not specified explicitly.    \n",
    "    \"\"\"\n",
    "\n",
    "    # convert to LAB color space\n",
    "    lab_b_channel = cv2.cvtColor(img_input, cv2.COLOR_RGB2Lab)[:, :, 2]\n",
    "    # don't normalize if there are no yellows in the image\n",
    "    if np.max(lab_b_channel) > 175:\n",
    "        lab_b_channel = lab_b_channel * (255 / np.max(lab_b_channel))\n",
    "\n",
    "    # apply a threshold to the B channel\n",
    "    lab_b_binary = np.zeros_like(lab_b_channel)\n",
    "    lab_b_binary[((lab_b_channel > thresh[0]) & (lab_b_channel <= thresh[1]))] = 1\n",
    "\n",
    "    return lab_b_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"def sliding_window_polyfit(img_input):\\n    \\\"\\\"\\\"\\n    Applies sliding window algorithm on input image to extract lane lines.\\n    \\\"\\\"\\\"\\n\\n    # Take a histogram of the bottom half of the image\\n    histogram = np.sum(img_input[img_input.shape[0] // 2 :, :], axis=0)\\n\\n    # Find the peak of the left and right halves of the histogram\\n    # These will be the starting point for the left and right lines\\n    midpoint = np.int(histogram.shape[0] // 2)\\n    quarter_point = np.int(midpoint // 2)\\n    leftx_base = np.argmax(histogram[quarter_point:midpoint]) + quarter_point\\n    rightx_base = np.argmax(histogram[midpoint : (midpoint + quarter_point)]) + midpoint\\n\\n    # HYPERPARAMETERS - related to our sliding windows\\n    # Choose the number of sliding windows\\n    nwindows = 10\\n    # Set the width of the windows +/- margin\\n    margin = 90\\n    # Set minimum number of pixels found to recenter window\\n    minpix = 50\\n\\n    # Set height of windows - based on nwindows above and image shape\\n    window_height = np.int(img_input.shape[0] // nwindows)\\n\\n    # Identify the x and y positions of all nonzero pixels in the image\\n    nonzero = img_input.nonzero()\\n    nonzeroy = np.array(nonzero[0])\\n    nonzerox = np.array(nonzero[1])\\n\\n    # Current positions to be updated later for each window in nwindows\\n    leftx_current = leftx_base\\n    rightx_current = rightx_base\\n\\n    # Create empty lists to receive left and right lane pixel indices\\n    left_lane_inds = []\\n    right_lane_inds = []\\n\\n    # Rectangles data for sliding windows visualization\\n    rectangles_visualization_data = []\\n\\n    # 1. Loop through each window in nwindows\\n\\n    # 2. Find the boundaries of our current window.\\n    #    This is based on a combination of the current window's starting point\\n    #    (leftx_current and rightx_current), as well as the margin you set in the hyperparameters.\\n\\n    # 3. Use cv2.rectangle to draw these window boundaries onto our visualization image out_img.\\n    #    This is required for the quiz, but you can skip this step in practice if you don't need\\n    #    to visualize where the windows are.\\n\\n    # 4. Now that we know the boundaries of our window, find out which activated pixels from\\n    #    nonzeroy and nonzerox above actually fall into the window.\\n\\n    # 5. Append these to our lists left_lane_inds and right_lane_inds.\\n\\n    # 6. If the number of pixels you found in Step 4 are greater than your hyperparameter minpix,\\n    #    re-center our window (i.e. leftx_current or rightx_current) based on the mean position of these pixels.\\n\\n    # Step through the windows one by one\\n    for window in range(nwindows):\\n        # Identify window boundaries in x and y (and right and left)\\n        win_y_low = img_input.shape[0] - (window + 1) * window_height\\n        win_y_high = img_input.shape[0] - window * window_height\\n        win_xleft_low = leftx_current - margin\\n        win_xleft_high = leftx_current + margin\\n        win_xright_low = rightx_current - margin\\n        win_xright_high = rightx_current + margin\\n\\n        # Identify the nonzero pixels in x and y within the window #\\n        good_left_inds = (\\n            (nonzeroy >= win_y_low)\\n            & (nonzeroy < win_y_high)\\n            & (nonzerox >= win_xleft_low)\\n            & (nonzerox < win_xleft_high)\\n        ).nonzero()[0]\\n        good_right_inds = (\\n            (nonzeroy >= win_y_low)\\n            & (nonzeroy < win_y_high)\\n            & (nonzerox >= win_xright_low)\\n            & (nonzerox < win_xright_high)\\n        ).nonzero()[0]\\n\\n        # Append these indices to the lists\\n        left_lane_inds.append(good_left_inds)\\n        right_lane_inds.append(good_right_inds)\\n\\n        # If you found > minpix pixels, recenter next window on their mean position\\n        if len(good_left_inds) > minpix:\\n            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\\n        if len(good_right_inds) > minpix:\\n            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\\n\\n    # Concatenate the arrays of indices\\n    left_lane_inds = np.concatenate(left_lane_inds)\\n    right_lane_inds = np.concatenate(right_lane_inds)\\n\\n    # Extract left and right line pixel positions\\n    leftx = nonzerox[left_lane_inds]\\n    lefty = nonzeroy[left_lane_inds]\\n    rightx = nonzerox[right_lane_inds]\\n    righty = nonzeroy[right_lane_inds]\\n\\n    left_fit, right_fit = (None, None)\\n    # Fit a second order polynomial to each side/lane\\n    if len(leftx) != 0:\\n        left_fit = np.polyfit(lefty, leftx, 2)\\n    if len(rightx) != 0:\\n        right_fit = np.polyfit(righty, rightx, 2)\\n\\n    return (left_fit, right_fit, left_lane_inds, right_lane_inds)\";\n",
       "                var nbb_formatted_code = \"def sliding_window_polyfit(img_input):\\n    \\\"\\\"\\\"\\n    Applies sliding window algorithm on input image to extract lane lines.\\n    \\\"\\\"\\\"\\n\\n    # Take a histogram of the bottom half of the image\\n    histogram = np.sum(img_input[img_input.shape[0] // 2 :, :], axis=0)\\n\\n    # Find the peak of the left and right halves of the histogram\\n    # These will be the starting point for the left and right lines\\n    midpoint = np.int(histogram.shape[0] // 2)\\n    quarter_point = np.int(midpoint // 2)\\n    leftx_base = np.argmax(histogram[quarter_point:midpoint]) + quarter_point\\n    rightx_base = np.argmax(histogram[midpoint : (midpoint + quarter_point)]) + midpoint\\n\\n    # HYPERPARAMETERS - related to our sliding windows\\n    # Choose the number of sliding windows\\n    nwindows = 10\\n    # Set the width of the windows +/- margin\\n    margin = 90\\n    # Set minimum number of pixels found to recenter window\\n    minpix = 50\\n\\n    # Set height of windows - based on nwindows above and image shape\\n    window_height = np.int(img_input.shape[0] // nwindows)\\n\\n    # Identify the x and y positions of all nonzero pixels in the image\\n    nonzero = img_input.nonzero()\\n    nonzeroy = np.array(nonzero[0])\\n    nonzerox = np.array(nonzero[1])\\n\\n    # Current positions to be updated later for each window in nwindows\\n    leftx_current = leftx_base\\n    rightx_current = rightx_base\\n\\n    # Create empty lists to receive left and right lane pixel indices\\n    left_lane_inds = []\\n    right_lane_inds = []\\n\\n    # Rectangles data for sliding windows visualization\\n    rectangles_visualization_data = []\\n\\n    # 1. Loop through each window in nwindows\\n\\n    # 2. Find the boundaries of our current window.\\n    #    This is based on a combination of the current window's starting point\\n    #    (leftx_current and rightx_current), as well as the margin you set in the hyperparameters.\\n\\n    # 3. Use cv2.rectangle to draw these window boundaries onto our visualization image out_img.\\n    #    This is required for the quiz, but you can skip this step in practice if you don't need\\n    #    to visualize where the windows are.\\n\\n    # 4. Now that we know the boundaries of our window, find out which activated pixels from\\n    #    nonzeroy and nonzerox above actually fall into the window.\\n\\n    # 5. Append these to our lists left_lane_inds and right_lane_inds.\\n\\n    # 6. If the number of pixels you found in Step 4 are greater than your hyperparameter minpix,\\n    #    re-center our window (i.e. leftx_current or rightx_current) based on the mean position of these pixels.\\n\\n    # Step through the windows one by one\\n    for window in range(nwindows):\\n        # Identify window boundaries in x and y (and right and left)\\n        win_y_low = img_input.shape[0] - (window + 1) * window_height\\n        win_y_high = img_input.shape[0] - window * window_height\\n        win_xleft_low = leftx_current - margin\\n        win_xleft_high = leftx_current + margin\\n        win_xright_low = rightx_current - margin\\n        win_xright_high = rightx_current + margin\\n\\n        # Identify the nonzero pixels in x and y within the window #\\n        good_left_inds = (\\n            (nonzeroy >= win_y_low)\\n            & (nonzeroy < win_y_high)\\n            & (nonzerox >= win_xleft_low)\\n            & (nonzerox < win_xleft_high)\\n        ).nonzero()[0]\\n        good_right_inds = (\\n            (nonzeroy >= win_y_low)\\n            & (nonzeroy < win_y_high)\\n            & (nonzerox >= win_xright_low)\\n            & (nonzerox < win_xright_high)\\n        ).nonzero()[0]\\n\\n        # Append these indices to the lists\\n        left_lane_inds.append(good_left_inds)\\n        right_lane_inds.append(good_right_inds)\\n\\n        # If you found > minpix pixels, recenter next window on their mean position\\n        if len(good_left_inds) > minpix:\\n            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\\n        if len(good_right_inds) > minpix:\\n            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\\n\\n    # Concatenate the arrays of indices\\n    left_lane_inds = np.concatenate(left_lane_inds)\\n    right_lane_inds = np.concatenate(right_lane_inds)\\n\\n    # Extract left and right line pixel positions\\n    leftx = nonzerox[left_lane_inds]\\n    lefty = nonzeroy[left_lane_inds]\\n    rightx = nonzerox[right_lane_inds]\\n    righty = nonzeroy[right_lane_inds]\\n\\n    left_fit, right_fit = (None, None)\\n    # Fit a second order polynomial to each side/lane\\n    if len(leftx) != 0:\\n        left_fit = np.polyfit(lefty, leftx, 2)\\n    if len(rightx) != 0:\\n        right_fit = np.polyfit(righty, rightx, 2)\\n\\n    return (left_fit, right_fit, left_lane_inds, right_lane_inds)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sliding_window_polyfit(img_input):\n",
    "    \"\"\"\n",
    "    Applies sliding window algorithm on input image to extract lane lines.\n",
    "    \"\"\"\n",
    "\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(img_input[img_input.shape[0] // 2 :, :], axis=0)\n",
    "\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0] // 2)\n",
    "    quarter_point = np.int(midpoint // 2)\n",
    "    leftx_base = np.argmax(histogram[quarter_point:midpoint]) + quarter_point\n",
    "    rightx_base = np.argmax(histogram[midpoint : (midpoint + quarter_point)]) + midpoint\n",
    "\n",
    "    # HYPERPARAMETERS - related to our sliding windows\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 10\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 90\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(img_input.shape[0] // nwindows)\n",
    "\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img_input.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Rectangles data for sliding windows visualization\n",
    "    rectangles_visualization_data = []\n",
    "\n",
    "    # 1. Loop through each window in nwindows\n",
    "\n",
    "    # 2. Find the boundaries of our current window.\n",
    "    #    This is based on a combination of the current window's starting point\n",
    "    #    (leftx_current and rightx_current), as well as the margin you set in the hyperparameters.\n",
    "\n",
    "    # 3. Use cv2.rectangle to draw these window boundaries onto our visualization image out_img.\n",
    "    #    This is required for the quiz, but you can skip this step in practice if you don't need\n",
    "    #    to visualize where the windows are.\n",
    "\n",
    "    # 4. Now that we know the boundaries of our window, find out which activated pixels from\n",
    "    #    nonzeroy and nonzerox above actually fall into the window.\n",
    "\n",
    "    # 5. Append these to our lists left_lane_inds and right_lane_inds.\n",
    "\n",
    "    # 6. If the number of pixels you found in Step 4 are greater than your hyperparameter minpix,\n",
    "    #    re-center our window (i.e. leftx_current or rightx_current) based on the mean position of these pixels.\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img_input.shape[0] - (window + 1) * window_height\n",
    "        win_y_high = img_input.shape[0] - window * window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "\n",
    "        # Identify the nonzero pixels in x and y within the window #\n",
    "        good_left_inds = (\n",
    "            (nonzeroy >= win_y_low)\n",
    "            & (nonzeroy < win_y_high)\n",
    "            & (nonzerox >= win_xleft_low)\n",
    "            & (nonzerox < win_xleft_high)\n",
    "        ).nonzero()[0]\n",
    "        good_right_inds = (\n",
    "            (nonzeroy >= win_y_low)\n",
    "            & (nonzeroy < win_y_high)\n",
    "            & (nonzerox >= win_xright_low)\n",
    "            & (nonzerox < win_xright_high)\n",
    "        ).nonzero()[0]\n",
    "\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:\n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds]\n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    left_fit, right_fit = (None, None)\n",
    "    # Fit a second order polynomial to each side/lane\n",
    "    if len(leftx) != 0:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    if len(rightx) != 0:\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    return (left_fit, right_fit, left_lane_inds, right_lane_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search from Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"def search_around_poly(img_input, left_fit_prev, right_fit_prev, margin=80):\\n    \\\"\\\"\\\"\\n    Searches lane lines in a margin around the previous lane line position.\\n    \\\"\\\"\\\"\\n    # margin parameter is the width of the margin around the previous polynomial to search\\n\\n    # Grab activated pixels\\n    nonzero = img_input.nonzero()\\n    nonzeroy = np.array(nonzero[0])\\n    nonzerox = np.array(nonzero[1])\\n\\n    ### Set the area of search based on activated x-values ###\\n    ### within the +/- margin of our polynomial function ###\\n    ### Hint: consider the window areas for the similarly named variables ###\\n    ### in the previous quiz, but change the windows to our new search area ###\\n    left_lane_inds = (\\n        nonzerox\\n        > (\\n            left_fit_prev[0] * (nonzeroy ** 2)\\n            + left_fit_prev[1] * nonzeroy\\n            + left_fit_prev[2]\\n            - margin\\n        )\\n    ) & (\\n        nonzerox\\n        < (\\n            left_fit_prev[0] * (nonzeroy ** 2)\\n            + left_fit_prev[1] * nonzeroy\\n            + left_fit_prev[2]\\n            + margin\\n        )\\n    )\\n    right_lane_inds = (\\n        nonzerox\\n        > (\\n            right_fit_prev[0] * (nonzeroy ** 2)\\n            + right_fit_prev[1] * nonzeroy\\n            + right_fit_prev[2]\\n            - margin\\n        )\\n    ) & (\\n        nonzerox\\n        < (\\n            right_fit_prev[0] * (nonzeroy ** 2)\\n            + right_fit_prev[1] * nonzeroy\\n            + right_fit_prev[2]\\n            + margin\\n        )\\n    )\\n\\n    # Extract left and right line pixel positions\\n    leftx = nonzerox[left_lane_inds]\\n    lefty = nonzeroy[left_lane_inds]\\n    rightx = nonzerox[right_lane_inds]\\n    righty = nonzeroy[right_lane_inds]\\n\\n    left_fit, right_fit = (None, None)\\n    # Fit a second order polynomial to each side/lane\\n    if len(leftx) != 0:\\n        left_fit = np.polyfit(lefty, leftx, 2)\\n    if len(rightx) != 0:\\n        right_fit = np.polyfit(righty, rightx, 2)\\n\\n    return left_fit, right_fit, left_lane_inds, right_lane_inds\";\n",
       "                var nbb_formatted_code = \"def search_around_poly(img_input, left_fit_prev, right_fit_prev, margin=80):\\n    \\\"\\\"\\\"\\n    Searches lane lines in a margin around the previous lane line position.\\n    \\\"\\\"\\\"\\n    # margin parameter is the width of the margin around the previous polynomial to search\\n\\n    # Grab activated pixels\\n    nonzero = img_input.nonzero()\\n    nonzeroy = np.array(nonzero[0])\\n    nonzerox = np.array(nonzero[1])\\n\\n    ### Set the area of search based on activated x-values ###\\n    ### within the +/- margin of our polynomial function ###\\n    ### Hint: consider the window areas for the similarly named variables ###\\n    ### in the previous quiz, but change the windows to our new search area ###\\n    left_lane_inds = (\\n        nonzerox\\n        > (\\n            left_fit_prev[0] * (nonzeroy ** 2)\\n            + left_fit_prev[1] * nonzeroy\\n            + left_fit_prev[2]\\n            - margin\\n        )\\n    ) & (\\n        nonzerox\\n        < (\\n            left_fit_prev[0] * (nonzeroy ** 2)\\n            + left_fit_prev[1] * nonzeroy\\n            + left_fit_prev[2]\\n            + margin\\n        )\\n    )\\n    right_lane_inds = (\\n        nonzerox\\n        > (\\n            right_fit_prev[0] * (nonzeroy ** 2)\\n            + right_fit_prev[1] * nonzeroy\\n            + right_fit_prev[2]\\n            - margin\\n        )\\n    ) & (\\n        nonzerox\\n        < (\\n            right_fit_prev[0] * (nonzeroy ** 2)\\n            + right_fit_prev[1] * nonzeroy\\n            + right_fit_prev[2]\\n            + margin\\n        )\\n    )\\n\\n    # Extract left and right line pixel positions\\n    leftx = nonzerox[left_lane_inds]\\n    lefty = nonzeroy[left_lane_inds]\\n    rightx = nonzerox[right_lane_inds]\\n    righty = nonzeroy[right_lane_inds]\\n\\n    left_fit, right_fit = (None, None)\\n    # Fit a second order polynomial to each side/lane\\n    if len(leftx) != 0:\\n        left_fit = np.polyfit(lefty, leftx, 2)\\n    if len(rightx) != 0:\\n        right_fit = np.polyfit(righty, rightx, 2)\\n\\n    return left_fit, right_fit, left_lane_inds, right_lane_inds\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def search_around_poly(img_input, left_fit_prev, right_fit_prev, margin=80):\n",
    "    \"\"\"\n",
    "    Searches lane lines in a margin around the previous lane line position.\n",
    "    \"\"\"\n",
    "    # margin parameter is the width of the margin around the previous polynomial to search\n",
    "\n",
    "    # Grab activated pixels\n",
    "    nonzero = img_input.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    ### Set the area of search based on activated x-values ###\n",
    "    ### within the +/- margin of our polynomial function ###\n",
    "    ### Hint: consider the window areas for the similarly named variables ###\n",
    "    ### in the previous quiz, but change the windows to our new search area ###\n",
    "    left_lane_inds = (\n",
    "        nonzerox\n",
    "        > (\n",
    "            left_fit_prev[0] * (nonzeroy ** 2)\n",
    "            + left_fit_prev[1] * nonzeroy\n",
    "            + left_fit_prev[2]\n",
    "            - margin\n",
    "        )\n",
    "    ) & (\n",
    "        nonzerox\n",
    "        < (\n",
    "            left_fit_prev[0] * (nonzeroy ** 2)\n",
    "            + left_fit_prev[1] * nonzeroy\n",
    "            + left_fit_prev[2]\n",
    "            + margin\n",
    "        )\n",
    "    )\n",
    "    right_lane_inds = (\n",
    "        nonzerox\n",
    "        > (\n",
    "            right_fit_prev[0] * (nonzeroy ** 2)\n",
    "            + right_fit_prev[1] * nonzeroy\n",
    "            + right_fit_prev[2]\n",
    "            - margin\n",
    "        )\n",
    "    ) & (\n",
    "        nonzerox\n",
    "        < (\n",
    "            right_fit_prev[0] * (nonzeroy ** 2)\n",
    "            + right_fit_prev[1] * nonzeroy\n",
    "            + right_fit_prev[2]\n",
    "            + margin\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds]\n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    left_fit, right_fit = (None, None)\n",
    "    # Fit a second order polynomial to each side/lane\n",
    "    if len(leftx) != 0:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    if len(rightx) != 0:\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    return left_fit, right_fit, left_lane_inds, right_lane_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate lane curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"def calculate_lane_curvature(\\n    img_binary, left_lane_inds, right_lane_inds, ym_per_pix, xm_per_pix\\n):\\n    \\\"\\\"\\\"\\n    Calculates the curvature of polynomial functions in meters.\\n    \\\"\\\"\\\"\\n    # Define y-value where we want radius of curvature\\n    # We'll choose the maximum y-value, corresponding to the bottom of the image\\n    ploty = np.linspace(0, img_binary.shape[0] - 1, img_binary.shape[0])\\n    y_eval = np.max(ploty)\\n\\n    # Grab activated pixels\\n    nonzero = img_binary.nonzero()\\n    nonzeroy = np.array(nonzero[0])\\n    nonzerox = np.array(nonzero[1])\\n\\n    # Again, extract left and right line pixel positions\\n    leftx = nonzerox[left_lane_inds]\\n    lefty = nonzeroy[left_lane_inds]\\n    rightx = nonzerox[right_lane_inds]\\n    righty = nonzeroy[right_lane_inds]\\n\\n    left_curverad, right_curverad = 0, 0\\n\\n    if len(leftx) != 0 and len(rightx) != 0:\\n        # Fit new polynomials to x,y in world space\\n        left_fit_cr = np.polyfit(lefty * ym_per_pix, leftx * xm_per_pix, 2)\\n        right_fit_cr = np.polyfit(righty * ym_per_pix, rightx * xm_per_pix, 2)\\n\\n        # Calculate the new radius of curvature in meters\\n        left_curverad = (\\n            (1 + (2 * left_fit_cr[0] * y_eval * ym_per_pix + left_fit_cr[1]) ** 2)\\n            ** 1.5\\n        ) / np.absolute(2 * left_fit_cr[0])\\n        right_curverad = (\\n            (1 + (2 * right_fit_cr[0] * y_eval * ym_per_pix + right_fit_cr[1]) ** 2)\\n            ** 1.5\\n        ) / np.absolute(2 * right_fit_cr[0])\\n\\n    return left_curverad, right_curverad\";\n",
       "                var nbb_formatted_code = \"def calculate_lane_curvature(\\n    img_binary, left_lane_inds, right_lane_inds, ym_per_pix, xm_per_pix\\n):\\n    \\\"\\\"\\\"\\n    Calculates the curvature of polynomial functions in meters.\\n    \\\"\\\"\\\"\\n    # Define y-value where we want radius of curvature\\n    # We'll choose the maximum y-value, corresponding to the bottom of the image\\n    ploty = np.linspace(0, img_binary.shape[0] - 1, img_binary.shape[0])\\n    y_eval = np.max(ploty)\\n\\n    # Grab activated pixels\\n    nonzero = img_binary.nonzero()\\n    nonzeroy = np.array(nonzero[0])\\n    nonzerox = np.array(nonzero[1])\\n\\n    # Again, extract left and right line pixel positions\\n    leftx = nonzerox[left_lane_inds]\\n    lefty = nonzeroy[left_lane_inds]\\n    rightx = nonzerox[right_lane_inds]\\n    righty = nonzeroy[right_lane_inds]\\n\\n    left_curverad, right_curverad = 0, 0\\n\\n    if len(leftx) != 0 and len(rightx) != 0:\\n        # Fit new polynomials to x,y in world space\\n        left_fit_cr = np.polyfit(lefty * ym_per_pix, leftx * xm_per_pix, 2)\\n        right_fit_cr = np.polyfit(righty * ym_per_pix, rightx * xm_per_pix, 2)\\n\\n        # Calculate the new radius of curvature in meters\\n        left_curverad = (\\n            (1 + (2 * left_fit_cr[0] * y_eval * ym_per_pix + left_fit_cr[1]) ** 2)\\n            ** 1.5\\n        ) / np.absolute(2 * left_fit_cr[0])\\n        right_curverad = (\\n            (1 + (2 * right_fit_cr[0] * y_eval * ym_per_pix + right_fit_cr[1]) ** 2)\\n            ** 1.5\\n        ) / np.absolute(2 * right_fit_cr[0])\\n\\n    return left_curverad, right_curverad\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_lane_curvature(\n",
    "    img_binary, left_lane_inds, right_lane_inds, ym_per_pix, xm_per_pix\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the curvature of polynomial functions in meters.\n",
    "    \"\"\"\n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    ploty = np.linspace(0, img_binary.shape[0] - 1, img_binary.shape[0])\n",
    "    y_eval = np.max(ploty)\n",
    "\n",
    "    # Grab activated pixels\n",
    "    nonzero = img_binary.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds]\n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    left_curverad, right_curverad = 0, 0\n",
    "\n",
    "    if len(leftx) != 0 and len(rightx) != 0:\n",
    "        # Fit new polynomials to x,y in world space\n",
    "        left_fit_cr = np.polyfit(lefty * ym_per_pix, leftx * xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(righty * ym_per_pix, rightx * xm_per_pix, 2)\n",
    "\n",
    "        # Calculate the new radius of curvature in meters\n",
    "        left_curverad = (\n",
    "            (1 + (2 * left_fit_cr[0] * y_eval * ym_per_pix + left_fit_cr[1]) ** 2)\n",
    "            ** 1.5\n",
    "        ) / np.absolute(2 * left_fit_cr[0])\n",
    "        right_curverad = (\n",
    "            (1 + (2 * right_fit_cr[0] * y_eval * ym_per_pix + right_fit_cr[1]) ** 2)\n",
    "            ** 1.5\n",
    "        ) / np.absolute(2 * right_fit_cr[0])\n",
    "\n",
    "    return left_curverad, right_curverad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distance from center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"def calculate_distance_from_center(img_binary, left_fit, right_fit, xm_per_pix):\\n    \\\"\\\"\\\"\\n    Calculates vehicle's distance from lane center.\\n    \\\"\\\"\\\"\\n\\n    # Distance from center is image x-axis midpoint\\n    # we use mean of left_fit and right_fit intercepts\\n    if left_fit is not None and right_fit is not None:\\n        car_position = img_binary.shape[1] / 2\\n\\n        left_fit_x_int = (\\n            left_fit[0] * img_binary.shape[0] ** 2\\n            + left_fit[1] * img_binary.shape[0]\\n            + left_fit[2]\\n        )\\n        right_fit_x_int = (\\n            right_fit[0] * img_binary.shape[0] ** 2\\n            + right_fit[1] * img_binary.shape[0]\\n            + right_fit[2]\\n        )\\n        lane_center_position = (left_fit_x_int + right_fit_x_int) / 2\\n\\n    # return distance from center\\n    return (car_position - lane_center_position) * xm_per_pix\";\n",
       "                var nbb_formatted_code = \"def calculate_distance_from_center(img_binary, left_fit, right_fit, xm_per_pix):\\n    \\\"\\\"\\\"\\n    Calculates vehicle's distance from lane center.\\n    \\\"\\\"\\\"\\n\\n    # Distance from center is image x-axis midpoint\\n    # we use mean of left_fit and right_fit intercepts\\n    if left_fit is not None and right_fit is not None:\\n        car_position = img_binary.shape[1] / 2\\n\\n        left_fit_x_int = (\\n            left_fit[0] * img_binary.shape[0] ** 2\\n            + left_fit[1] * img_binary.shape[0]\\n            + left_fit[2]\\n        )\\n        right_fit_x_int = (\\n            right_fit[0] * img_binary.shape[0] ** 2\\n            + right_fit[1] * img_binary.shape[0]\\n            + right_fit[2]\\n        )\\n        lane_center_position = (left_fit_x_int + right_fit_x_int) / 2\\n\\n    # return distance from center\\n    return (car_position - lane_center_position) * xm_per_pix\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_distance_from_center(img_binary, left_fit, right_fit, xm_per_pix):\n",
    "    \"\"\"\n",
    "    Calculates vehicle's distance from lane center.\n",
    "    \"\"\"\n",
    "\n",
    "    # Distance from center is image x-axis midpoint\n",
    "    # we use mean of left_fit and right_fit intercepts\n",
    "    if left_fit is not None and right_fit is not None:\n",
    "        car_position = img_binary.shape[1] / 2\n",
    "\n",
    "        left_fit_x_int = (\n",
    "            left_fit[0] * img_binary.shape[0] ** 2\n",
    "            + left_fit[1] * img_binary.shape[0]\n",
    "            + left_fit[2]\n",
    "        )\n",
    "        right_fit_x_int = (\n",
    "            right_fit[0] * img_binary.shape[0] ** 2\n",
    "            + right_fit[1] * img_binary.shape[0]\n",
    "            + right_fit[2]\n",
    "        )\n",
    "        lane_center_position = (left_fit_x_int + right_fit_x_int) / 2\n",
    "\n",
    "    # return distance from center\n",
    "    return (car_position - lane_center_position) * xm_per_pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"def draw_lane(original_img, binary_img, left_fit, right_fit, Minv):\\n    new_img = np.copy(original_img)\\n    if left_fit is None or right_fit is None:\\n        return original_img\\n\\n    # create an image to draw the lines on\\n    warp_zero = np.zeros_like(binary_img).astype(np.uint8)\\n    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\\n\\n    ploty = np.linspace(0, binary_img.shape[0] - 1, num=binary_img.shape[0])\\n    left_fitx = left_fit[0] * ploty ** 2 + left_fit[1] * ploty + left_fit[2]\\n    right_fitx = right_fit[0] * ploty ** 2 + right_fit[1] * ploty + right_fit[2]\\n\\n    # Recast the x and y points into usable format for cv2.fillPoly()\\n    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\\n    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\\n    pts = np.hstack((pts_left, pts_right))\\n\\n    # Draw the lane onto the warped blank image\\n    cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0))\\n\\n    # Warp the blank back to original image space using inverse perspective matrix (Minv)\\n    newwarp = cv2.warpPerspective(\\n        color_warp, Minv, (binary_img.shape[1], binary_img.shape[0])\\n    )\\n    # Combine the result with the original image\\n    result = cv2.addWeighted(new_img, 1, newwarp, 0.5, 0)\\n\\n    return result\";\n",
       "                var nbb_formatted_code = \"def draw_lane(original_img, binary_img, left_fit, right_fit, Minv):\\n    new_img = np.copy(original_img)\\n    if left_fit is None or right_fit is None:\\n        return original_img\\n\\n    # create an image to draw the lines on\\n    warp_zero = np.zeros_like(binary_img).astype(np.uint8)\\n    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\\n\\n    ploty = np.linspace(0, binary_img.shape[0] - 1, num=binary_img.shape[0])\\n    left_fitx = left_fit[0] * ploty ** 2 + left_fit[1] * ploty + left_fit[2]\\n    right_fitx = right_fit[0] * ploty ** 2 + right_fit[1] * ploty + right_fit[2]\\n\\n    # Recast the x and y points into usable format for cv2.fillPoly()\\n    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\\n    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\\n    pts = np.hstack((pts_left, pts_right))\\n\\n    # Draw the lane onto the warped blank image\\n    cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0))\\n\\n    # Warp the blank back to original image space using inverse perspective matrix (Minv)\\n    newwarp = cv2.warpPerspective(\\n        color_warp, Minv, (binary_img.shape[1], binary_img.shape[0])\\n    )\\n    # Combine the result with the original image\\n    result = cv2.addWeighted(new_img, 1, newwarp, 0.5, 0)\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_lane(original_img, binary_img, left_fit, right_fit, Minv):\n",
    "    new_img = np.copy(original_img)\n",
    "    if left_fit is None or right_fit is None:\n",
    "        return original_img\n",
    "\n",
    "    # create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_img).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    ploty = np.linspace(0, binary_img.shape[0] - 1, num=binary_img.shape[0])\n",
    "    left_fitx = left_fit[0] * ploty ** 2 + left_fit[1] * ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0] * ploty ** 2 + right_fit[1] * ploty + right_fit[2]\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(\n",
    "        color_warp, Minv, (binary_img.shape[1], binary_img.shape[0])\n",
    "    )\n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(new_img, 1, newwarp, 0.5, 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"def draw_data(original_img, curve_radius, distance_from_center):\\n    output_img = np.copy(original_img)\\n    cv2.putText(\\n        output_img,\\n        \\\"Curve radius: \\\" + \\\"{:04.2f}\\\".format(curve_radius) + \\\"m\\\",\\n        (40, 80),\\n        cv2.FONT_HERSHEY_DUPLEX,\\n        1,\\n        (152, 251, 152),\\n        2,\\n        cv2.LINE_AA,\\n    )\\n\\n    direction = \\\"\\\"\\n    if distance_from_center > 0:\\n        direction = \\\"right\\\"\\n    elif distance_from_center < 0:\\n        direction = \\\"left\\\"\\n    cv2.putText(\\n        output_img,\\n        \\\"{:04.3f}\\\".format(abs(distance_from_center)) + \\\"m \\\" + direction + \\\" of center\\\",\\n        (40, 120),\\n        cv2.FONT_HERSHEY_DUPLEX,\\n        1,\\n        (152, 251, 152),\\n        2,\\n        cv2.LINE_AA,\\n    )\\n\\n    return output_img\";\n",
       "                var nbb_formatted_code = \"def draw_data(original_img, curve_radius, distance_from_center):\\n    output_img = np.copy(original_img)\\n    cv2.putText(\\n        output_img,\\n        \\\"Curve radius: \\\" + \\\"{:04.2f}\\\".format(curve_radius) + \\\"m\\\",\\n        (40, 80),\\n        cv2.FONT_HERSHEY_DUPLEX,\\n        1,\\n        (152, 251, 152),\\n        2,\\n        cv2.LINE_AA,\\n    )\\n\\n    direction = \\\"\\\"\\n    if distance_from_center > 0:\\n        direction = \\\"right\\\"\\n    elif distance_from_center < 0:\\n        direction = \\\"left\\\"\\n    cv2.putText(\\n        output_img,\\n        \\\"{:04.3f}\\\".format(abs(distance_from_center)) + \\\"m \\\" + direction + \\\" of center\\\",\\n        (40, 120),\\n        cv2.FONT_HERSHEY_DUPLEX,\\n        1,\\n        (152, 251, 152),\\n        2,\\n        cv2.LINE_AA,\\n    )\\n\\n    return output_img\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_data(original_img, curve_radius, distance_from_center):\n",
    "    output_img = np.copy(original_img)\n",
    "    cv2.putText(\n",
    "        output_img,\n",
    "        \"Curve radius: \" + \"{:04.2f}\".format(curve_radius) + \"m\",\n",
    "        (40, 80),\n",
    "        cv2.FONT_HERSHEY_DUPLEX,\n",
    "        1,\n",
    "        (152, 251, 152),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    direction = \"\"\n",
    "    if distance_from_center > 0:\n",
    "        direction = \"right\"\n",
    "    elif distance_from_center < 0:\n",
    "        direction = \"left\"\n",
    "    cv2.putText(\n",
    "        output_img,\n",
    "        \"{:04.3f}\".format(abs(distance_from_center)) + \"m \" + direction + \" of center\",\n",
    "        (40, 120),\n",
    "        cv2.FONT_HERSHEY_DUPLEX,\n",
    "        1,\n",
    "        (152, 251, 152),\n",
    "        2,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a class for storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"class Line:\\n    def __init__(self):\\n        # was the line detected in the last iteration?\\n        self.detected = False\\n\\n        # x values of the last n fits of the line\\n        self.recent_xfitted = []\\n\\n        # average x values of the fitted line over the last n iterations\\n        self.bestx = None\\n\\n        # polynomial coefficients averaged over the last n iterations\\n        self.best_fit = None\\n\\n        # polynomial coefficients for the most recent fit\\n        self.current_fit = []\\n\\n        # radius of curvature of the line in some units\\n        self.radius_of_curvature = None\\n\\n        # distance in meters of vehicle center from the line\\n        self.line_base_pos = None\\n\\n        # difference in fit coefficients between last and new fits\\n        self.diffs = np.array([0, 0, 0], dtype=\\\"float\\\")\\n\\n        # number of detected pixels\\n        self.px_count = None\\n\\n    def add_fit(self, new_fit, inds):\\n        # add a found fit to the line, up to n\\n        if new_fit is not None:\\n            if self.best_fit is not None:\\n                # if we have a best fit, compare it with a new fit\\n                self.diffs = abs(new_fit - self.best_fit)\\n            if (\\n                self.diffs[0] > 0.001 or self.diffs[1] > 1.0 or self.diffs[2] > 100.0\\n            ) and len(self.current_fit) > 0:\\n                # dismiss a bad fit, unless there are no fits in the current_fit queue\\n                self.detected = False\\n            else:\\n                self.detected = True\\n                self.px_count = np.count_nonzero(inds)\\n                self.current_fit.append(new_fit)\\n                if len(self.current_fit) > 5:\\n                    # throw out old fits, keep newest 5\\n                    self.current_fit = self.current_fit[len(self.current_fit) - 5 :]\\n                self.best_fit = np.average(self.current_fit, axis=0)\\n        # remove one fit from the history, if not found\\n        else:\\n            self.detected = False\\n            if len(self.current_fit) > 0:\\n                # removeoldest fit\\n                self.current_fit = self.current_fit[: len(self.current_fit) - 1]\\n            if len(self.current_fit) > 0:\\n                # if there are still any fits in the queue, best_fit is their average\\n                self.best_fit = np.average(self.current_fit, axis=0)\";\n",
       "                var nbb_formatted_code = \"class Line:\\n    def __init__(self):\\n        # was the line detected in the last iteration?\\n        self.detected = False\\n\\n        # x values of the last n fits of the line\\n        self.recent_xfitted = []\\n\\n        # average x values of the fitted line over the last n iterations\\n        self.bestx = None\\n\\n        # polynomial coefficients averaged over the last n iterations\\n        self.best_fit = None\\n\\n        # polynomial coefficients for the most recent fit\\n        self.current_fit = []\\n\\n        # radius of curvature of the line in some units\\n        self.radius_of_curvature = None\\n\\n        # distance in meters of vehicle center from the line\\n        self.line_base_pos = None\\n\\n        # difference in fit coefficients between last and new fits\\n        self.diffs = np.array([0, 0, 0], dtype=\\\"float\\\")\\n\\n        # number of detected pixels\\n        self.px_count = None\\n\\n    def add_fit(self, new_fit, inds):\\n        # add a found fit to the line, up to n\\n        if new_fit is not None:\\n            if self.best_fit is not None:\\n                # if we have a best fit, compare it with a new fit\\n                self.diffs = abs(new_fit - self.best_fit)\\n            if (\\n                self.diffs[0] > 0.001 or self.diffs[1] > 1.0 or self.diffs[2] > 100.0\\n            ) and len(self.current_fit) > 0:\\n                # dismiss a bad fit, unless there are no fits in the current_fit queue\\n                self.detected = False\\n            else:\\n                self.detected = True\\n                self.px_count = np.count_nonzero(inds)\\n                self.current_fit.append(new_fit)\\n                if len(self.current_fit) > 5:\\n                    # throw out old fits, keep newest 5\\n                    self.current_fit = self.current_fit[len(self.current_fit) - 5 :]\\n                self.best_fit = np.average(self.current_fit, axis=0)\\n        # remove one fit from the history, if not found\\n        else:\\n            self.detected = False\\n            if len(self.current_fit) > 0:\\n                # removeoldest fit\\n                self.current_fit = self.current_fit[: len(self.current_fit) - 1]\\n            if len(self.current_fit) > 0:\\n                # if there are still any fits in the queue, best_fit is their average\\n                self.best_fit = np.average(self.current_fit, axis=0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Line:\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False\n",
    "\n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = []\n",
    "\n",
    "        # average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None\n",
    "\n",
    "        # polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None\n",
    "\n",
    "        # polynomial coefficients for the most recent fit\n",
    "        self.current_fit = []\n",
    "\n",
    "        # radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None\n",
    "\n",
    "        # distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None\n",
    "\n",
    "        # difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0, 0, 0], dtype=\"float\")\n",
    "\n",
    "        # number of detected pixels\n",
    "        self.px_count = None\n",
    "\n",
    "    def add_fit(self, new_fit, inds):\n",
    "        # add a found fit to the line, up to n\n",
    "        if new_fit is not None:\n",
    "            if self.best_fit is not None:\n",
    "                # if we have a best fit, compare it with a new fit\n",
    "                self.diffs = abs(new_fit - self.best_fit)\n",
    "            if (\n",
    "                self.diffs[0] > 0.001 or self.diffs[1] > 1.0 or self.diffs[2] > 100.0\n",
    "            ) and len(self.current_fit) > 0:\n",
    "                # dismiss a bad fit, unless there are no fits in the current_fit queue\n",
    "                self.detected = False\n",
    "            else:\n",
    "                self.detected = True\n",
    "                self.px_count = np.count_nonzero(inds)\n",
    "                self.current_fit.append(new_fit)\n",
    "                if len(self.current_fit) > 5:\n",
    "                    # throw out old fits, keep newest 5\n",
    "                    self.current_fit = self.current_fit[len(self.current_fit) - 5 :]\n",
    "                self.best_fit = np.average(self.current_fit, axis=0)\n",
    "        # remove one fit from the history, if not found\n",
    "        else:\n",
    "            self.detected = False\n",
    "            if len(self.current_fit) > 0:\n",
    "                # removeoldest fit\n",
    "                self.current_fit = self.current_fit[: len(self.current_fit) - 1]\n",
    "            if len(self.current_fit) > 0:\n",
    "                # if there are still any fits in the queue, best_fit is their average\n",
    "                self.best_fit = np.average(self.current_fit, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"def lane_finding_pipeline(img_input, left_line, right_line, fname=None):\\n    new_img = np.copy(img_input)\\n\\n    # undistort image\\n    undist_img = cv2.undistort(new_img, mtx, dist, None, mtx)\\n\\n    # perspective transform\\n    warped_img, M_inv = perspective_transform(\\n        undist_img, perspective_src, perspective_dst\\n    )\\n\\n    #  apply color threshold\\n    hls_l_binary = hls_l_threshold(warped_img)\\n    lab_b_binary = lab_b_threshold(warped_img)\\n    color_threshold_img = np.zeros_like(lab_b_binary)\\n    color_threshold_img[(hls_l_binary == 1) | (lab_b_binary == 1)] = 1\\n\\n    # if one or both lane lines were not detected from last frame, use sliding_window_polyfit\\n    # otherwise, use search_around_poly\\n    if not left_line.detected or not right_line.detected:\\n        left_fit, right_fit, left_lane_inds, right_lane_inds, = sliding_window_polyfit(\\n            color_threshold_img\\n        )\\n    else:\\n        left_fit, right_fit, left_lane_inds, right_lane_inds = search_around_poly(\\n            color_threshold_img, left_line.best_fit, right_line.best_fit\\n        )\\n\\n    left_line.add_fit(left_fit, left_lane_inds)\\n    right_line.add_fit(right_fit, right_lane_inds)\\n\\n    # Define conversions in x and y from pixels space to meters\\n    # lane length is 10 ft = 3.048 meters\\n    # lane width is 12 ft = 3.6576 meters\\n    # distance between lanes is 380 pixels\\n    ym_per_pix = 3.048 / 100  # meters per pixel in y dimension\\n    xm_per_pix = 3.6576 / 380  # meters per pixel in x dimension\\n\\n    radius_left, radius_right = calculate_lane_curvature(\\n        color_threshold_img, left_lane_inds, right_lane_inds, ym_per_pix, xm_per_pix\\n    )\\n\\n    distance_from_center = calculate_distance_from_center(\\n        color_threshold_img, left_line.best_fit, right_line.best_fit, xm_per_pix\\n    )\\n\\n    lanes_img = draw_lane(\\n        new_img, color_threshold_img, left_line.best_fit, right_line.best_fit, M_inv\\n    )\\n\\n    result = draw_data(\\n        lanes_img, (radius_left + radius_right) / 2, distance_from_center\\n    )\\n\\n    if fname is not None:\\n        # save images\\n        undist_img = cv2.cvtColor(undist_img, cv2.COLOR_BGR2RGB)\\n        cv2.imwrite(camera_calibration_undistorted_output_dir + fname, undist_img)\\n        warped_img = cv2.cvtColor(warped_img, cv2.COLOR_BGR2RGB)\\n        cv2.imwrite(perspective_transform_output_dir + fname, warped_img)\\n        cv2.imwrite(\\n            color_threshold_output_dir + fname, color_threshold_img * 255\\n        )  # multiply with 255 to see pixels\\n        lanes_img = cv2.cvtColor(lanes_img, cv2.COLOR_BGR2RGB)\\n        cv2.imwrite(lane_lines_img_output_dir + fname, lanes_img)\\n\\n    return result\";\n",
       "                var nbb_formatted_code = \"def lane_finding_pipeline(img_input, left_line, right_line, fname=None):\\n    new_img = np.copy(img_input)\\n\\n    # undistort image\\n    undist_img = cv2.undistort(new_img, mtx, dist, None, mtx)\\n\\n    # perspective transform\\n    warped_img, M_inv = perspective_transform(\\n        undist_img, perspective_src, perspective_dst\\n    )\\n\\n    #  apply color threshold\\n    hls_l_binary = hls_l_threshold(warped_img)\\n    lab_b_binary = lab_b_threshold(warped_img)\\n    color_threshold_img = np.zeros_like(lab_b_binary)\\n    color_threshold_img[(hls_l_binary == 1) | (lab_b_binary == 1)] = 1\\n\\n    # if one or both lane lines were not detected from last frame, use sliding_window_polyfit\\n    # otherwise, use search_around_poly\\n    if not left_line.detected or not right_line.detected:\\n        left_fit, right_fit, left_lane_inds, right_lane_inds, = sliding_window_polyfit(\\n            color_threshold_img\\n        )\\n    else:\\n        left_fit, right_fit, left_lane_inds, right_lane_inds = search_around_poly(\\n            color_threshold_img, left_line.best_fit, right_line.best_fit\\n        )\\n\\n    left_line.add_fit(left_fit, left_lane_inds)\\n    right_line.add_fit(right_fit, right_lane_inds)\\n\\n    # Define conversions in x and y from pixels space to meters\\n    # lane length is 10 ft = 3.048 meters\\n    # lane width is 12 ft = 3.6576 meters\\n    # distance between lanes is 380 pixels\\n    ym_per_pix = 3.048 / 100  # meters per pixel in y dimension\\n    xm_per_pix = 3.6576 / 380  # meters per pixel in x dimension\\n\\n    radius_left, radius_right = calculate_lane_curvature(\\n        color_threshold_img, left_lane_inds, right_lane_inds, ym_per_pix, xm_per_pix\\n    )\\n\\n    distance_from_center = calculate_distance_from_center(\\n        color_threshold_img, left_line.best_fit, right_line.best_fit, xm_per_pix\\n    )\\n\\n    lanes_img = draw_lane(\\n        new_img, color_threshold_img, left_line.best_fit, right_line.best_fit, M_inv\\n    )\\n\\n    result = draw_data(\\n        lanes_img, (radius_left + radius_right) / 2, distance_from_center\\n    )\\n\\n    if fname is not None:\\n        # save images\\n        undist_img = cv2.cvtColor(undist_img, cv2.COLOR_BGR2RGB)\\n        cv2.imwrite(camera_calibration_undistorted_output_dir + fname, undist_img)\\n        warped_img = cv2.cvtColor(warped_img, cv2.COLOR_BGR2RGB)\\n        cv2.imwrite(perspective_transform_output_dir + fname, warped_img)\\n        cv2.imwrite(\\n            color_threshold_output_dir + fname, color_threshold_img * 255\\n        )  # multiply with 255 to see pixels\\n        lanes_img = cv2.cvtColor(lanes_img, cv2.COLOR_BGR2RGB)\\n        cv2.imwrite(lane_lines_img_output_dir + fname, lanes_img)\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lane_finding_pipeline(img_input, left_line, right_line, fname=None):\n",
    "    new_img = np.copy(img_input)\n",
    "\n",
    "    # undistort image\n",
    "    undist_img = cv2.undistort(new_img, mtx, dist, None, mtx)\n",
    "\n",
    "    # perspective transform\n",
    "    warped_img, M_inv = perspective_transform(\n",
    "        undist_img, perspective_src, perspective_dst\n",
    "    )\n",
    "\n",
    "    #  apply color threshold\n",
    "    hls_l_binary = hls_l_threshold(warped_img)\n",
    "    lab_b_binary = lab_b_threshold(warped_img)\n",
    "    color_threshold_img = np.zeros_like(lab_b_binary)\n",
    "    color_threshold_img[(hls_l_binary == 1) | (lab_b_binary == 1)] = 1\n",
    "\n",
    "    # if one or both lane lines were not detected from last frame, use sliding_window_polyfit\n",
    "    # otherwise, use search_around_poly\n",
    "    if not left_line.detected or not right_line.detected:\n",
    "        left_fit, right_fit, left_lane_inds, right_lane_inds, = sliding_window_polyfit(\n",
    "            color_threshold_img\n",
    "        )\n",
    "    else:\n",
    "        left_fit, right_fit, left_lane_inds, right_lane_inds = search_around_poly(\n",
    "            color_threshold_img, left_line.best_fit, right_line.best_fit\n",
    "        )\n",
    "\n",
    "    left_line.add_fit(left_fit, left_lane_inds)\n",
    "    right_line.add_fit(right_fit, right_lane_inds)\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    # lane length is 10 ft = 3.048 meters\n",
    "    # lane width is 12 ft = 3.6576 meters\n",
    "    # distance between lanes is 380 pixels\n",
    "    ym_per_pix = 3.048 / 100  # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.6576 / 380  # meters per pixel in x dimension\n",
    "\n",
    "    radius_left, radius_right = calculate_lane_curvature(\n",
    "        color_threshold_img, left_lane_inds, right_lane_inds, ym_per_pix, xm_per_pix\n",
    "    )\n",
    "\n",
    "    distance_from_center = calculate_distance_from_center(\n",
    "        color_threshold_img, left_line.best_fit, right_line.best_fit, xm_per_pix\n",
    "    )\n",
    "\n",
    "    lanes_img = draw_lane(\n",
    "        new_img, color_threshold_img, left_line.best_fit, right_line.best_fit, M_inv\n",
    "    )\n",
    "\n",
    "    result = draw_data(\n",
    "        lanes_img, (radius_left + radius_right) / 2, distance_from_center\n",
    "    )\n",
    "\n",
    "    if fname is not None:\n",
    "        # save images\n",
    "        undist_img = cv2.cvtColor(undist_img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(camera_calibration_undistorted_output_dir + fname, undist_img)\n",
    "        warped_img = cv2.cvtColor(warped_img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(perspective_transform_output_dir + fname, warped_img)\n",
    "        cv2.imwrite(\n",
    "            color_threshold_output_dir + fname, color_threshold_img * 255\n",
    "        )  # multiply with 255 to see pixels\n",
    "        lanes_img = cv2.cvtColor(lanes_img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(lane_lines_img_output_dir + fname, lanes_img)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# left_line = Line()\\n# right_line = Line()\\n\\n# # Apply lane finding pipeline on test images\\n# for fname in os.listdir(test_images_input_dir):\\n#     test_img = cv2.imread(test_images_input_dir + fname)\\n#     test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\\n\\n#     lane_finding_pipeline(test_img, left_line, right_line, fname=fname)\";\n",
       "                var nbb_formatted_code = \"# left_line = Line()\\n# right_line = Line()\\n\\n# # Apply lane finding pipeline on test images\\n# for fname in os.listdir(test_images_input_dir):\\n#     test_img = cv2.imread(test_images_input_dir + fname)\\n#     test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\\n\\n#     lane_finding_pipeline(test_img, left_line, right_line, fname=fname)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# left_line = Line()\n",
    "# right_line = Line()\n",
    "\n",
    "# # Apply lane finding pipeline on test images\n",
    "# for fname in os.listdir(test_images_input_dir):\n",
    "#     test_img = cv2.imread(test_images_input_dir + fname)\n",
    "#     test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     lane_finding_pipeline(test_img, left_line, right_line, fname=fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "t:   0%|          | 0/1260 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./output_videos/project_video_output.mp4.\n",
      "Moviepy - Writing video ./output_videos/project_video_output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./output_videos/project_video_output.mp4\n",
      "CPU times: user 2min 35s, sys: 6.25 s, total: 2min 41s\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"left_line = Line()\\nright_line = Line()\\n\\nproject_video_output = output_videos_dir + \\\"project_video_output.mp4\\\"\\n\\nproject_video_input = VideoFileClip(\\\"./test_videos/project_video.mp4\\\").fl_image(\\n    lambda image: lane_finding_pipeline(image, left_line, right_line)\\n)\\n\\n%time project_video_input.write_videofile(project_video_output, audio=False)\";\n",
       "                var nbb_formatted_code = \"left_line = Line()\\nright_line = Line()\\n\\nproject_video_output = output_videos_dir + \\\"project_video_output.mp4\\\"\\n\\nproject_video_input = VideoFileClip(\\\"./test_videos/project_video.mp4\\\").fl_image(\\n    lambda image: lane_finding_pipeline(image, left_line, right_line)\\n)\\n\\n%time project_video_input.write_videofile(project_video_output, audio=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "project_video_output = output_videos_dir + \"project_video_output.mp4\"\n",
    "\n",
    "project_video_input = VideoFileClip(\"./test_videos/project_video.mp4\").fl_image(\n",
    "    lambda image: lane_finding_pipeline(image, left_line, right_line)\n",
    ")\n",
    "\n",
    "%time project_video_input.write_videofile(project_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 3/485 [00:00<00:28, 17.07it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./output_videos/challenge_video_output.mp4.\n",
      "Moviepy - Writing video ./output_videos/challenge_video_output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./output_videos/challenge_video_output.mp4\n",
      "CPU times: user 56.9 s, sys: 2.82 s, total: 59.8 s\n",
      "Wall time: 36.8 s\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"left_line = Line()\\nright_line = Line()\\n\\nchallenge_video_output = output_videos_dir + \\\"challenge_video_output.mp4\\\"\\n\\nchallenge_video_input = VideoFileClip(\\\"./test_videos/challenge_video.mp4\\\").fl_image(\\n    lambda image: lane_finding_pipeline(image, left_line, right_line)\\n)\\n\\n%time challenge_video_input.write_videofile(challenge_video_output, audio=False)\";\n",
       "                var nbb_formatted_code = \"left_line = Line()\\nright_line = Line()\\n\\nchallenge_video_output = output_videos_dir + \\\"challenge_video_output.mp4\\\"\\n\\nchallenge_video_input = VideoFileClip(\\\"./test_videos/challenge_video.mp4\\\").fl_image(\\n    lambda image: lane_finding_pipeline(image, left_line, right_line)\\n)\\n\\n%time challenge_video_input.write_videofile(challenge_video_output, audio=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_line = Line()\n",
    "right_line = Line()\n",
    "\n",
    "challenge_video_output = output_videos_dir + \"challenge_video_output.mp4\"\n",
    "\n",
    "challenge_video_input = VideoFileClip(\"./test_videos/challenge_video.mp4\").fl_image(\n",
    "    lambda image: lane_finding_pipeline(image, left_line, right_line)\n",
    ")\n",
    "\n",
    "%time challenge_video_input.write_videofile(challenge_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# left_line = Line()\\n# right_line = Line()\\n\\n# harder_challenge_video_output = output_videos_dir + \\\"harder_challenge_video_output.mp4\\\"\\n\\n# harder_challenge_video_input = VideoFileClip(\\n#     \\\"./test_videos/harder_challenge_video.mp4\\\"\\n# ).fl_image(lambda image: lane_finding_pipeline(image, left_line, right_line))\\n\\n# %time harder_challenge_video_input.write_videofile(harder_challenge_video_output, audio=False)\";\n",
       "                var nbb_formatted_code = \"# left_line = Line()\\n# right_line = Line()\\n\\n# harder_challenge_video_output = output_videos_dir + \\\"harder_challenge_video_output.mp4\\\"\\n\\n# harder_challenge_video_input = VideoFileClip(\\n#     \\\"./test_videos/harder_challenge_video.mp4\\\"\\n# ).fl_image(lambda image: lane_finding_pipeline(image, left_line, right_line))\\n\\n# %time harder_challenge_video_input.write_videofile(harder_challenge_video_output, audio=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# left_line = Line()\n",
    "# right_line = Line()\n",
    "\n",
    "# harder_challenge_video_output = output_videos_dir + \"harder_challenge_video_output.mp4\"\n",
    "\n",
    "# harder_challenge_video_input = VideoFileClip(\n",
    "#     \"./test_videos/harder_challenge_video.mp4\"\n",
    "# ).fl_image(lambda image: lane_finding_pipeline(image, left_line, right_line))\n",
    "\n",
    "# %time harder_challenge_video_input.write_videofile(harder_challenge_video_output, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
